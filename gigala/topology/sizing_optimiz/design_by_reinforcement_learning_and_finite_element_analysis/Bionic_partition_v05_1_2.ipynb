{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import gym\n",
    "from gym import spaces\n",
    "import time\n",
    "import random\n",
    "from stable_baselines3 import A2C,TD3\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Element Model of the Space Frame Element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlaneTrussElementLength(x1,y1,z1,x2,y2,z2):\n",
    "    return math.sqrt((x2-x1)*(x2-x1) + (y2-y1)*(y2-y1)+(z2-z1)*(z2-z1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpaceFrameElementStiffness(E,G,A,Iy,Iz,J,x1,y1,z1,x2,y2,z2):\n",
    "    L = PlaneTrussElementLength(x1,y1,z1,x2,y2,z2)\n",
    "    w1 = E*A/L\n",
    "    w2 = 12*E*Iz/(L*L*L)\n",
    "    w3 = 6*E*Iz/(L*L)\n",
    "    w4 = 4*E*Iz/L\n",
    "    w5 = 2*E*Iz/L\n",
    "    w6 = 12*E*Iy/(L*L*L)\n",
    "    w7 = 6*E*Iy/(L*L)\n",
    "    w8 = 4*E*Iy/L\n",
    "    w9 = 2*E*Iy/L\n",
    "    w10 = G*J/L\n",
    "    \n",
    "    kprime = np.array([[w1, 0, 0, 0, 0, 0, -w1, 0, 0, 0, 0, 0],\n",
    "                        [0, w2, 0, 0, 0, w3, 0, -w2, 0, 0, 0, w3], \n",
    "                        [0, 0, w6, 0, -w7, 0, 0, 0, -w6, 0, -w7, 0],\n",
    "                        [0, 0, 0, w10, 0, 0, 0, 0, 0, -w10, 0, 0],\n",
    "                        [0, 0, -w7, 0, w8, 0, 0, 0, w7, 0, w9, 0],\n",
    "                        [0, w3, 0, 0, 0, w4, 0, -w3, 0, 0, 0, w5],\n",
    "                        [-w1, 0, 0, 0, 0, 0, w1, 0, 0, 0, 0, 0],\n",
    "                        [0, -w2, 0, 0, 0, -w3, 0, w2, 0, 0, 0, -w3],\n",
    "                        [0, 0, -w6, 0, w7, 0, 0, 0, w6, 0, w7, 0],\n",
    "                        [0, 0, 0, -w10, 0, 0, 0, 0, 0, w10, 0, 0],\n",
    "                        [0, 0, -w7, 0, w9, 0, 0, 0, w7, 0, w8, 0],\n",
    "                        [0, w3, 0, 0, 0, w5, 0, -w3, 0, 0, 0, w4]])  \n",
    "    \n",
    "    \n",
    "    if x1 == x2 and y1 == y2:\n",
    "        if z2 > z1:\n",
    "            Lambda = np.array([[0, 0, 1], [0, 1, 0], [-1, 0, 0]])\n",
    "        else:\n",
    "            Lambda = np.array([[0, 0, -1], [0, 1, 0], [1, 0, 0]])\n",
    "    else:\n",
    "        CXx = (x2-x1)/L\n",
    "        CYx = (y2-y1)/L\n",
    "        CZx = (z2-z1)/L\n",
    "        D = math.sqrt(CXx*CXx + CYx*CYx)\n",
    "        CXy = -CYx/D\n",
    "        CYy = CXx/D\n",
    "        CZy = 0\n",
    "        CXz = -CXx*CZx/D\n",
    "        CYz = -CYx*CZx/D\n",
    "        CZz = D\n",
    "        Lambda = np.array([[CXx, CYx, CZx], [CXy, CYy, CZy], [CXz, CYz, CZz]])\n",
    "        \n",
    "        \n",
    "    R = np.array([np.concatenate((np.concatenate((Lambda,np.zeros((3,3)),np.zeros((3,3)),np.zeros((3,3))),axis=1),\n",
    "        np.concatenate((np.zeros((3,3)), Lambda, np.zeros((3,3)), np.zeros((3,3))),axis=1) ,\n",
    "        np.concatenate((np.zeros((3,3)), np.zeros((3,3)), Lambda, np.zeros((3,3))),axis=1), \n",
    "        np.concatenate((np.zeros((3,3)), np.zeros((3,3)), np.zeros((3,3)), Lambda),axis=1)))])[0]\n",
    "    return np.dot(np.dot(R.T,kprime),R)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpaceFrameAssemble(K,k,i,j):\n",
    "    K[6*i,6*i] = K[6*i,6*i] + k[0,0]\n",
    "    K[6*i,6*i+1] = K[6*i,6*i+1] + k[0,1]\n",
    "    K[6*i,6*i+2] = K[6*i,6*i+2] + k[0,2]\n",
    "    K[6*i,6*i+3] = K[6*i,6*i+3] + k[0,3]\n",
    "    K[6*i,6*i+4] = K[6*i,6*i+4] + k[0,4]\n",
    "    K[6*i,6*i+5] = K[6*i,6*i+5] + k[0,5]\n",
    "    K[6*i,6*j] = K[6*i,6*j] + k[0,6]\n",
    "    K[6*i,6*j+1] = K[6*i,6*j+1] + k[0,7]\n",
    "    K[6*i,6*j+2] = K[6*i,6*j+2] + k[0,8]\n",
    "    K[6*i,6*j+3] = K[6*i,6*j+3] + k[0,9]\n",
    "    K[6*i,6*j+4] = K[6*i,6*j+4] + k[0,10]\n",
    "    K[6*i,6*j+5] = K[6*i,6*j+5] + k[0,11]\n",
    "    K[6*i+1,6*i] = K[6*i+1,6*i] + k[1,0]\n",
    "    K[6*i+1,6*i+1] = K[6*i+1,6*i+1] + k[1,1]\n",
    "    K[6*i+1,6*i+2] = K[6*i+1,6*i+2] + k[1,2]\n",
    "    K[6*i+1,6*i+3] = K[6*i+1,6*i+3] + k[1,3]\n",
    "    K[6*i+1,6*i+4] = K[6*i+1,6*i+4] + k[1,4]\n",
    "    K[6*i+1,6*i+5] = K[6*i+1,6*i+5] + k[1,5]\n",
    "    K[6*i+1,6*j] = K[6*i+1,6*j] + k[1,6]\n",
    "    K[6*i+1,6*j+1] = K[6*i+1,6*j+1] + k[1,7]\n",
    "    K[6*i+1,6*j+2] = K[6*i+1,6*j+2] + k[1,8]\n",
    "    K[6*i+1,6*j+3] = K[6*i+1,6*j+3] + k[1,9]\n",
    "    K[6*i+1,6*j+4] = K[6*i+1,6*j+4] + k[1,10]\n",
    "    K[6*i+1,6*j+5] = K[6*i+1,6*j+5] + k[1,11]\n",
    "    K[6*i+2,6*i]   = K[6*i+2,6*i] + k[2,0]\n",
    "    K[6*i+2,6*i+1] = K[6*i+2,6*i+1] + k[2,1]\n",
    "    K[6*i+2,6*i+2] = K[6*i+2,6*i+2] + k[2,2]\n",
    "    K[6*i+2,6*i+3] = K[6*i+2,6*i+3] + k[2,3]\n",
    "    K[6*i+2,6*i+4] = K[6*i+2,6*i+4] + k[2,4]\n",
    "    K[6*i+2,6*i+5] = K[6*i+2,6*i+5] + k[2,5]\n",
    "    K[6*i+2,6*j]   = K[6*i+2,6*j] + k[2,6]\n",
    "    K[6*i+2,6*j+1] = K[6*i+2,6*j+1] + k[2,7]\n",
    "    K[6*i+2,6*j+2] = K[6*i+2,6*j+2] + k[2,8]\n",
    "    K[6*i+2,6*j+3] = K[6*i+2,6*j+3] + k[2,9]\n",
    "    K[6*i+2,6*j+4] = K[6*i+2,6*j+4] + k[2,10]\n",
    "    K[6*i+2,6*j+5] = K[6*i+2,6*j+5] + k[2,11]\n",
    "    K[6*i+3,6*i] = K[6*i+3,6*i] + k[3,0]\n",
    "    K[6*i+3,6*i+1] = K[6*i+3,6*i+1] + k[3,1]\n",
    "    K[6*i+3,6*i+2] = K[6*i+3,6*i+2] + k[3,2]\n",
    "    K[6*i+3,6*i+3] = K[6*i+3,6*i+3] + k[3,3]\n",
    "    K[6*i+3,6*i+4] = K[6*i+3,6*i+4] + k[3,4]\n",
    "    K[6*i+3,6*i+5] = K[6*i+3,6*i+5] + k[3,5]\n",
    "    K[6*i+3,6*j] = K[6*i+3,6*j] + k[3,6]\n",
    "    K[6*i+3,6*j+1] = K[6*i+3,6*j+1] + k[3,7]\n",
    "    K[6*i+3,6*j+2] = K[6*i+3,6*j+2] + k[3,8]    \n",
    "    K[6*i+3,6*j+3] = K[6*i+3,6*j+3] + k[3,9]\n",
    "    K[6*i+3,6*j+4] = K[6*i+3,6*j+4] + k[3,10]\n",
    "    K[6*i+3,6*j+5] = K[6*i+3,6*j+5] + k[3,11]\n",
    "    K[6*i+4,6*i] = K[6*i+4,6*i] + k[4,0]\n",
    "    K[6*i+4,6*i+1] = K[6*i+4,6*i+1] + k[4,1]\n",
    "    K[6*i+4,6*i+2] = K[6*i+4,6*i+2] + k[4,2]\n",
    "    K[6*i+4,6*i+3] = K[6*i+4,6*i+3] + k[4,3]\n",
    "    K[6*i+4,6*i+4] = K[6*i+4,6*i+4] + k[4,4]\n",
    "    K[6*i+4,6*i+5] = K[6*i+4,6*i+5] + k[4,5]\n",
    "    K[6*i+4,6*j] = K[6*i+4,6*j] + k[4,6]\n",
    "    K[6*i+4,6*j+1] = K[6*i+4,6*j+1] + k[4,7]\n",
    "    K[6*i+4,6*j+2] = K[6*i+4,6*j+2] + k[4,8]\n",
    "    K[6*i+4,6*j+3] = K[6*i+4,6*j+3] + k[4,9]\n",
    "    K[6*i+4,6*j+4] = K[6*i+4,6*j+4] + k[4,10]\n",
    "    K[6*i+4,6*j+5] = K[6*i+4,6*j+5] + k[4,11]\n",
    "    K[6*i+5,6*i] = K[6*i+5,6*i] + k[5,0]\n",
    "    K[6*i+5,6*i+1] = K[6*i+5,6*i+1] + k[5,1]\n",
    "    K[6*i+5,6*i+2] = K[6*i+5,6*i+2] + k[5,2]\n",
    "    K[6*i+5,6*i+3] = K[6*i+5,6*i+3] + k[5,3]\n",
    "    K[6*i+5,6*i+4] = K[6*i+5,6*i+4] + k[5,4]\n",
    "    K[6*i+5,6*i+5] = K[6*i+5,6*i+5] + k[5,5]\n",
    "    K[6*i+5,6*j] = K[6*i+5,6*j] + k[5,6]\n",
    "    K[6*i+5,6*j+1] = K[6*i+5,6*j+1] + k[5,7]\n",
    "    K[6*i+5,6*j+2] = K[6*i+5,6*j+2] + k[5,8]\n",
    "    K[6*i+5,6*j+3] = K[6*i+5,6*j+3] + k[5,9]\n",
    "    K[6*i+5,6*j+4] = K[6*i+5,6*j+4] + k[5,10]\n",
    "    K[6*i+5,6*j+5] = K[6*i+5,6*j+5] + k[5,11]\n",
    "    K[6*j,6*i] = K[6*j,6*i] + k[6,0]\n",
    "    K[6*j,6*i+1] = K[6*j,6*i+1] + k[6,1]\n",
    "    K[6*j,6*i+2] = K[6*j,6*i+2] + k[6,2]\n",
    "    K[6*j,6*i+3] = K[6*j,6*i+3] + k[6,3]\n",
    "    K[6*j,6*i+4] = K[6*j,6*i+4] + k[6,4]\n",
    "    K[6*j,6*i+5] = K[6*j,6*i+5] + k[6,5]\n",
    "    K[6*j,6*j] = K[6*j,6*j] + k[6,6]\n",
    "    K[6*j,6*j+1] = K[6*j,6*j+1] + k[6,7]\n",
    "    K[6*j,6*j+2] = K[6*j,6*j+2] + k[6,8]\n",
    "    K[6*j,6*j+3] = K[6*j,6*j+3] + k[6,9]\n",
    "    K[6*j,6*j+4] = K[6*j,6*j+4] + k[6,10]\n",
    "    K[6*j,6*j+5] = K[6*j,6*j+5] + k[6,11]\n",
    "    K[6*j+1,6*i] = K[6*j+1,6*i] + k[7,0]\n",
    "    K[6*j+1,6*i+1] = K[6*j+1,6*i+1] + k[7,1]\n",
    "    K[6*j+1,6*i+2] = K[6*j+1,6*i+2] + k[7,2]\n",
    "    K[6*j+1,6*i+3] = K[6*j+1,6*i+3] + k[7,3]\n",
    "    K[6*j+1,6*i+4] = K[6*j+1,6*i+4] + k[7,4]\n",
    "    K[6*j+1,6*i+5] = K[6*j+1,6*i+5] + k[7,5]\n",
    "    K[6*j+1,6*j] = K[6*j+1,6*j] + k[7,6]\n",
    "    K[6*j+1,6*j+1] = K[6*j+1,6*j+1] + k[7,7]\n",
    "    K[6*j+1,6*j+2] = K[6*j+1,6*j+2] + k[7,8]\n",
    "    K[6*j+1,6*j+3] = K[6*j+1,6*j+3] + k[7,9]\n",
    "    K[6*j+1,6*j+4] = K[6*j+1,6*j+4] + k[7,10]\n",
    "    K[6*j+1,6*j+5] = K[6*j+1,6*j+5] + k[7,11]\n",
    "    K[6*j+2,6*i] = K[6*j+2,6*i] + k[8,0]\n",
    "    K[6*j+2,6*i+1] = K[6*j+2,6*i+1] + k[8,1]\n",
    "    K[6*j+2,6*i+2] = K[6*j+2,6*i+2] + k[8,2]\n",
    "    K[6*j+2,6*i+3] = K[6*j+2,6*i+3] + k[8,3]\n",
    "    K[6*j+2,6*i+4] = K[6*j+2,6*i+4] + k[8,4]\n",
    "    K[6*j+2,6*i+5] = K[6*j+2,6*i+5] + k[8,5]\n",
    "    K[6*j+2,6*j] = K[6*j+2,6*j] + k[8,6]\n",
    "    K[6*j+2,6*j+1] = K[6*j+2,6*j+1] + k[8,7]\n",
    "    K[6*j+2,6*j+2] = K[6*j+2,6*j+2] + k[8,8]\n",
    "    K[6*j+2,6*j+3] = K[6*j+2,6*j+3] + k[8,9]\n",
    "    K[6*j+2,6*j+4] = K[6*j+2,6*j+4] + k[8,10]\n",
    "    K[6*j+2,6*j+5] = K[6*j+2,6*j+5] + k[8,11]\n",
    "    K[6*j+3,6*i] = K[6*j+3,6*i] + k[9,0]\n",
    "    K[6*j+3,6*i+1] = K[6*j+3,6*i+1] + k[9,1]\n",
    "    K[6*j+3,6*i+2] = K[6*j+3,6*i+2] + k[9,2]\n",
    "    K[6*j+3,6*i+3] = K[6*j+3,6*i+3] + k[9,3]\n",
    "    K[6*j+3,6*i+4] = K[6*j+3,6*i+4] + k[9,4]\n",
    "    K[6*j+3,6*i+5] = K[6*j+3,6*i+5] + k[9,5]\n",
    "    K[6*j+3,6*j] = K[6*j+3,6*j] + k[9,6]\n",
    "    K[6*j+3,6*j+1] = K[6*j+3,6*j+1] + k[9,7]\n",
    "    K[6*j+3,6*j+2] = K[6*j+3,6*j+2] + k[9,8]\n",
    "    K[6*j+3,6*j+3] = K[6*j+3,6*j+3] + k[9,9]\n",
    "    K[6*j+3,6*j+4] = K[6*j+3,6*j+4] + k[9,10]\n",
    "    K[6*j+3,6*j+5] = K[6*j+3,6*j+5] + k[9,11]\n",
    "    K[6*j+4,6*i] = K[6*j+4,6*i] + k[10,0]\n",
    "    K[6*j+4,6*i+1] = K[6*j+4,6*i+1] + k[10,1]\n",
    "    K[6*j+4,6*i+2] = K[6*j+4,6*i+2] + k[10,2]\n",
    "    K[6*j+4,6*i+3] = K[6*j+4,6*i+3] + k[10,3]\n",
    "    K[6*j+4,6*i+4] = K[6*j+4,6*i+4] + k[10,4]\n",
    "    K[6*j+4,6*i+5] = K[6*j+4,6*i+5] + k[10,5]\n",
    "    K[6*j+4,6*j] = K[6*j+4,6*j] + k[10,6]\n",
    "    K[6*j+4,6*j+1] = K[6*j+4,6*j+1] + k[10,7]\n",
    "    K[6*j+4,6*j+2] = K[6*j+4,6*j+2] + k[10,8]\n",
    "    K[6*j+4,6*j+3] = K[6*j+4,6*j+3] + k[10,9]\n",
    "    K[6*j+4,6*j+4] = K[6*j+4,6*j+4] + k[10,10]\n",
    "    K[6*j+4,6*j+5] = K[6*j+4,6*j+5] + k[10,11]\n",
    "    K[6*j+5,6*i] = K[6*j+5,6*i] + k[11,0]\n",
    "    K[6*j+5,6*i+1] = K[6*j+5,6*i+1] + k[11,1]\n",
    "    K[6*j+5,6*i+2] = K[6*j+5,6*i+2] + k[11,2]\n",
    "    K[6*j+5,6*i+3] = K[6*j+5,6*i+3] + k[11,3]\n",
    "    K[6*j+5,6*i+4] = K[6*j+5,6*i+4] + k[11,4]\n",
    "    K[6*j+5,6*i+5] = K[6*j+5,6*i+5] + k[11,5]\n",
    "    K[6*j+5,6*j] = K[6*j+5,6*j] + k[11,6]\n",
    "    K[6*j+5,6*j+1] = K[6*j+5,6*j+1] + k[11,7]\n",
    "    K[6*j+5,6*j+2] = K[6*j+5,6*j+2] + k[11,8]\n",
    "    K[6*j+5,6*j+3] = K[6*j+5,6*j+3] + k[11,9]\n",
    "    K[6*j+5,6*j+4] = K[6*j+5,6*j+4] + k[11,10]\n",
    "    K[6*j+5,6*j+5] = K[6*j+5,6*j+5] + k[11,11]\n",
    "    \n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FEA_u(coord, elcon, bc_node, bc_val, global_force, \n",
    "          E=210e6, G=84e6, A=2e-2, Iy=10e-5, Iz=20e-5, J=5e-5):\n",
    "    coord=np.array(coord)\n",
    "    elcon=np.array(elcon)\n",
    "    \n",
    "    K=np.zeros(shape=(6*(np.max(elcon)+1),6*(np.max(elcon)+1)))\n",
    "    for el in elcon:\n",
    "        k=SpaceFrameElementStiffness(E,G,A,Iy,Iz,J,\n",
    "                                     coord[el[0]][0],coord[el[0]][1],coord[el[0]][2],\\\n",
    "                                     coord[el[1]][0],coord[el[1]][1],coord[el[1]][2])\n",
    "        K=SpaceFrameAssemble(K,k,el[0],el[1])\n",
    "        \n",
    "    F = np.array(global_force)\n",
    "    \n",
    "    \n",
    "    # https://github.com/CALFEM/calfem-matlab/blob/master/fem/solveq.m\n",
    "    \n",
    "    bc=np.array([bc_node, \n",
    "                bc_val]).T\n",
    "    nd, nd=K.shape\n",
    "    fdof=np.array([i for i in range(nd)]).T\n",
    "    d=np.zeros(shape=(len(fdof),))\n",
    "    Q=np.zeros(shape=(len(fdof),))\n",
    "\n",
    "    pdof=bc[:,0].astype(int)\n",
    "    dp=bc[:,1]\n",
    "    fdof=np.delete(fdof, pdof, 0)\n",
    "    s=np.linalg.lstsq(K[fdof,:][:,fdof], (F[fdof].T-np.dot(K[fdof,:][:,pdof],dp.T)).T, rcond=None)[0] \n",
    "    d[pdof]=dp\n",
    "    d[fdof]=s.reshape(-1,)\n",
    "    \n",
    "#     Q=np.dot(K,d).T-F \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 The Space Frame Element - verification\n",
    "d=FEA_u(np.array([0,0,0,\n",
    "                  3,0,0,\n",
    "                  0,0,-3,\n",
    "                  0,-4,0]).reshape(4,3),\n",
    "        elcon=np.array([[0, 1],\n",
    "                      [0, 2],\n",
    "                      [0, 3]]),\n",
    "        bc_node=list(range(6,24)), \n",
    "        bc_val=[0]*18,\n",
    "        global_force=[-10,0,20,0,0,0,\n",
    "                                0,0,0,0,0,0,\n",
    "                                0,0,0,0,0,0,\n",
    "                                0,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_length(coord,elcon):\n",
    "    coord=np.array(coord)\n",
    "    elcon=np.array(elcon)\n",
    "    t_length=0\n",
    "    for i in range(len(elcon)):\n",
    "        l=PlaneTrussElementLength(coord[elcon[i][0]][0],\\\n",
    "                                    coord[elcon[i][0]][1],\\\n",
    "                                    coord[elcon[i][0]][2],\\\n",
    "                                    coord[elcon[i][1]][0],\\\n",
    "                                    coord[elcon[i][1]][1],\\\n",
    "                                    coord[elcon[i][1]][2])\n",
    "        t_length+=l        \n",
    "    return t_length    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possible_lines_dic(n,m,dx,dy):\n",
    "    A=[(-dx,0),(-dx,dy),(0,dy),(dx,dy),(dx,0),(dx,-dy),(0,-dy),(-dx,-dy)]\n",
    "    dic={}\n",
    "    t=0\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            for item in A:\n",
    "                x,y=j*dx,i*dy\n",
    "                x1,y1=x+item[0],y+item[1]\n",
    "                if (x1>=0 and x1<=(m-1)*dx and \n",
    "                    y1>=0 and y1<=(n-1)*dy and \n",
    "                    (x1,y1,x,y) not in dic):\n",
    "                    dic[(x,y,x1,y1)]=t\n",
    "                    t+=1\n",
    "    return dic                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = len(possible_lines_dic(n=5,m=5,dx=1,dy=1)) + 3 # +2 for x and y +1 for action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,n=5,m=5,dx=1,dy=1, force=-500,\n",
    "                 E=210e6, G=84e6, A=2e-2, Iy=10e-5, Iz=20e-5, J=5e-5, break_flag=False):\n",
    "        # n,m,dx,dy - grid parameters    \n",
    "        self.E=E\n",
    "        self.G=G\n",
    "        self.A=A\n",
    "        self.Iy=Iy\n",
    "        self.Iz=Iz\n",
    "        self.J=J\n",
    "        self.n=n\n",
    "        self.m=m\n",
    "        self.dx=dx\n",
    "        self.dy=dy\n",
    "        self.dic_lines=possible_lines_dic(self.n,self.m,self.dx,self.dy)\n",
    "        self.line_list=len(self.dic_lines)*[0]\n",
    "        self.break_flag=break_flag\n",
    "        self.coord=[[2,2,0]]    \n",
    "        self.elcon=[]  \n",
    "        self.el_dic={(2,2):0}\n",
    "        self.max_el=0\n",
    "        self.force=force\n",
    "        self.bc_node = []\n",
    "        self.bc_val = []\n",
    "        self.global_force = [0, self.force, 0, 0, 0, 0]\n",
    "        self.old_weight=float(\"inf\")\n",
    "        self.old_strength=-float(\"inf\")\n",
    "        self.visit_list = [0,0,0,0] # number of checkpoints is 4\n",
    "    \n",
    "    def reset(self,break_flag,force):\n",
    "        self.dic_lines=possible_lines_dic(self.n, self.m, self.dx, self.dy)\n",
    "        self.line_list=len(self.dic_lines)*[0]\n",
    "        self.break_flag=break_flag\n",
    "        self.coord=[[2,2,0]]    \n",
    "        self.elcon=[]  \n",
    "        self.el_dic={(2,2):0}\n",
    "        self.max_el=0\n",
    "        self.force=force\n",
    "        self.bc_node = []\n",
    "        self.bc_val = []\n",
    "        self.global_force = [0, self.force, 0, 0, 0, 0]\n",
    "        self.visit_list = [0,0,0,0] # number of checkpoints is 4\n",
    "    \n",
    "    def FEA(self):\n",
    "        return FEA_u(self.coord, \n",
    "                     self.elcon, \n",
    "                     self.bc_node, \n",
    "                     self.bc_val, \n",
    "                     self.global_force, )\n",
    "        \n",
    "    def max_u(self, FEA_output_arr):\n",
    "        t=1\n",
    "        A=[]\n",
    "        while t<len(FEA_output_arr):\n",
    "            A.append(FEA_output_arr[t])\n",
    "            t+=6            \n",
    "        return min(A)    \n",
    "            \n",
    "    \n",
    "    def length(self):\n",
    "        return total_length(self.coord,self.elcon)\n",
    "    \n",
    "    \n",
    "    def move_w(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x-self.dx\n",
    "        y_new=y\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "            \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "            \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1\n",
    "            \n",
    "        return x_new, y_new\n",
    "            \n",
    "    def move_nw(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x-self.dx\n",
    "        y_new=y+self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "            \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]]) \n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1    \n",
    "                \n",
    "        return x_new, y_new\n",
    "    \n",
    "    def move_n(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x\n",
    "        y_new=y+self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "            \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                \n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "                  \n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1     \n",
    "        \n",
    "        return x_new, y_new\n",
    "    \n",
    "    \n",
    "    def move_ne(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x+self.dx\n",
    "        y_new=y+self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "                        \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1     \n",
    "                  \n",
    "        return x_new, y_new\n",
    "    \n",
    "    def move_e(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x+self.dx\n",
    "        y_new=y\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "                       \n",
    "                  \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])   \n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1      \n",
    "        return x_new, y_new\n",
    "    \n",
    "    def move_se(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x+self.dx\n",
    "        y_new=y-self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "            \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1      \n",
    "        \n",
    "        return x_new, y_new\n",
    "    \n",
    "    def move_s(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x\n",
    "        y_new=y-self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True \n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "            \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1      \n",
    "        \n",
    "        return x_new, y_new\n",
    "    \n",
    "    def move_sw(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x-self.dx\n",
    "        y_new=y-self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "                        \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "                  \n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1    \n",
    "                                  \n",
    "        return x_new, y_new\n",
    "    \n",
    "        \n",
    "    def action_space(self,action,x0,y0):\n",
    "        if action==0:\n",
    "            return self.move_w(x0,y0)\n",
    "        elif action==1:    \n",
    "            return self.move_nw(x0,y0)\n",
    "        elif action==2:  \n",
    "            return self.move_n(x0,y0)\n",
    "        elif action==3:\n",
    "            return self.move_ne(x0,y0)\n",
    "        elif action==4:\n",
    "            return self.move_e(x0,y0)\n",
    "        elif action==5:\n",
    "            return self.move_se(x0,y0)\n",
    "        elif action==6:\n",
    "            return self.move_s(x0,y0)\n",
    "        elif action==7:\n",
    "            return self.move_sw(x0,y0)\n",
    "                        \n",
    "    \n",
    "    def nn_input(self,x,y,action):  \n",
    "        return self.line_list+[x,y]+[action]        \n",
    "    \n",
    "    def reward_(self,x_new,y_new,n_steps):\n",
    "        reward=2*n_steps\n",
    "        if all([x>=1 for x in self.visit_list]):\n",
    "            reward+=10000\n",
    "            weight=self.length()\n",
    "        \n",
    "            FEA_output_arr=self.FEA()\n",
    "            max_=self.max_u(FEA_output_arr)\n",
    "            strength=max_\n",
    "            if weight<=self.old_weight:\n",
    "                reward+=50000\n",
    "                self.old_weight=weight\n",
    "            if strength>=self.old_strength: \n",
    "                reward+=100000000\n",
    "                self.old_strength=strength        \n",
    "            self.break_flag=True     \n",
    "            return reward \n",
    "        return reward     \n",
    "                                   \n",
    "    def draw(self,color):\n",
    "        c=self.coord\n",
    "        e=self.elcon\n",
    "        c=np.array(c)\n",
    "        e=np.array(e)\n",
    "        coord=c.reshape(np.max(e)+1,3)\n",
    "        fig=plt.figure(figsize=(13,5))\n",
    "        for item in e:\n",
    "            ax = fig.gca(projection='3d') \n",
    "            ax.plot([coord[item[0]][0],coord[item[1]][0]],\\\n",
    "                     [coord[item[0]][1],coord[item[1]][1]],\\\n",
    "                     [coord[item[0]][2],coord[item[1]][2]],\n",
    "                     color=color) \n",
    "        ax.view_init(-90,90)\n",
    "        ax.set_xlim([0, 5])\n",
    "        ax.set_ylim([0, 5])\n",
    "        plt.show()             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DISCRETE_ACTIONS=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BionicEnv(gym.Env):\n",
    "    \n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n",
    "        self.M=Model()\n",
    "        self.M.reset(False,-500)\n",
    "        self.x0=2\n",
    "        self.y0=2\n",
    "        self.obs=self.M.nn_input(self.x0,self.y0, 0)\n",
    "        self.observation_space = spaces.Box(low=np.array([-np.inf for x in range(DIM)]),\n",
    "                                            high=np.array([np.inf for y in range(DIM)]),\n",
    "                                            shape=(DIM,),\n",
    "                                           dtype=np.float64)\n",
    "        self.step_=0\n",
    "        self.needs_reset = True\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        x_new, y_new = self.x0, self.y0 \n",
    "        x_new, y_new = self.M.action_space(action, x_new, y_new)\n",
    "        self.obs=self.M.nn_input(x_new,y_new,action)\n",
    "                \n",
    "        self.step_+=1           \n",
    "        reward=self.M.reward_(x_new,y_new,self.step_)\n",
    "        self.x0,self.y0 = x_new,y_new\n",
    "        \n",
    "        done=False\n",
    "        if self.M.break_flag:\n",
    "            reward-=10000\n",
    "            done=True\n",
    "        \n",
    "        if self.needs_reset:\n",
    "            raise RuntimeError(\"Tried to step environment that needs reset\")\n",
    "            \n",
    "        if done:\n",
    "            self.needs_reset = True\n",
    "      \n",
    "        return np.array(self.obs), reward, done, dict()\n",
    "\n",
    "    def reset(self):\n",
    "        self.M.reset(False,-500)\n",
    "        self.x0=2\n",
    "        self.y0=2\n",
    "        self.obs=self.M.nn_input(self.x0,self.y0,0)\n",
    "        self.step_=0\n",
    "        self.needs_reset = False\n",
    "        return np.array(self.obs)  \n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        self.M.draw('blue')    \n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(\n",
    "                        f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\"\n",
    "                    )\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model to {self.save_path}.zip\")\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log dir\n",
    "log_dir = \"/tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = BionicEnv()\n",
    "# Logs will be saved in log_dir/monitor.csv\n",
    "env = Monitor(env, log_dir)\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "model = TD3(\"MlpPolicy\", env).learn(total_timesteps=1e5, callback=callback)\n",
    "end=time.time()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total time taken: {} min'.format((end - start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design by AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "while i<100:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    if env.M.break_flag:\n",
    "        break\n",
    "    i+=1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.M.length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEA_output_arr=env.M.FEA()\n",
    "env.M.max_u(FEA_output_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: -497.94\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -497.94 - Last mean reward per episode: -473.62\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 3000\n",
      "Best mean reward: -473.62 - Last mean reward per episode: -495.20\n",
      "Num timesteps: 4000\n",
      "Best mean reward: -473.62 - Last mean reward per episode: -481.89\n",
      "Num timesteps: 5000\n",
      "Best mean reward: -473.62 - Last mean reward per episode: -460.96\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 6000\n",
      "Best mean reward: -460.96 - Last mean reward per episode: -439.54\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 7000\n",
      "Best mean reward: -439.54 - Last mean reward per episode: -416.00\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 8000\n",
      "Best mean reward: -416.00 - Last mean reward per episode: -410.54\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 9000\n",
      "Best mean reward: -410.54 - Last mean reward per episode: -406.46\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -406.46 - Last mean reward per episode: -402.11\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 11000\n",
      "Best mean reward: -402.11 - Last mean reward per episode: -393.80\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 12000\n",
      "Best mean reward: -393.80 - Last mean reward per episode: -383.99\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 13000\n",
      "Best mean reward: -383.99 - Last mean reward per episode: -387.77\n",
      "Num timesteps: 14000\n",
      "Best mean reward: -383.99 - Last mean reward per episode: -381.43\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 15000\n",
      "Best mean reward: -381.43 - Last mean reward per episode: -384.51\n",
      "Num timesteps: 16000\n",
      "Best mean reward: -381.43 - Last mean reward per episode: -384.94\n",
      "Num timesteps: 17000\n",
      "Best mean reward: -381.43 - Last mean reward per episode: -383.17\n",
      "Num timesteps: 18000\n",
      "Best mean reward: -381.43 - Last mean reward per episode: -384.34\n",
      "Num timesteps: 19000\n",
      "Best mean reward: -381.43 - Last mean reward per episode: -388.15\n",
      "Num timesteps: 20000\n",
      "Best mean reward: -381.43 - Last mean reward per episode: -384.33\n",
      "Num timesteps: 21000\n",
      "Best mean reward: -381.43 - Last mean reward per episode: -381.36\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 22000\n",
      "Best mean reward: -381.36 - Last mean reward per episode: -377.62\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 23000\n",
      "Best mean reward: -377.62 - Last mean reward per episode: -371.77\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 24000\n",
      "Best mean reward: -371.77 - Last mean reward per episode: -368.88\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 25000\n",
      "Best mean reward: -368.88 - Last mean reward per episode: -363.41\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 26000\n",
      "Best mean reward: -363.41 - Last mean reward per episode: -358.95\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 27000\n",
      "Best mean reward: -358.95 - Last mean reward per episode: -357.92\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 28000\n",
      "Best mean reward: -357.92 - Last mean reward per episode: -352.55\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 29000\n",
      "Best mean reward: -352.55 - Last mean reward per episode: -350.06\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 30000\n",
      "Best mean reward: -350.06 - Last mean reward per episode: -346.49\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 31000\n",
      "Best mean reward: -346.49 - Last mean reward per episode: -344.27\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 32000\n",
      "Best mean reward: -344.27 - Last mean reward per episode: -339.41\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 33000\n",
      "Best mean reward: -339.41 - Last mean reward per episode: -334.79\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 34000\n",
      "Best mean reward: -334.79 - Last mean reward per episode: -333.00\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 35000\n",
      "Best mean reward: -333.00 - Last mean reward per episode: -326.44\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 36000\n",
      "Best mean reward: -326.44 - Last mean reward per episode: -317.95\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 37000\n",
      "Best mean reward: -317.95 - Last mean reward per episode: -315.56\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 38000\n",
      "Best mean reward: -315.56 - Last mean reward per episode: -314.31\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 39000\n",
      "Best mean reward: -314.31 - Last mean reward per episode: -313.54\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 40000\n",
      "Best mean reward: -313.54 - Last mean reward per episode: -309.94\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 41000\n",
      "Best mean reward: -309.94 - Last mean reward per episode: -306.78\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 42000\n",
      "Best mean reward: -306.78 - Last mean reward per episode: -303.27\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 43000\n",
      "Best mean reward: -303.27 - Last mean reward per episode: -298.99\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 44000\n",
      "Best mean reward: -298.99 - Last mean reward per episode: -295.19\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 45000\n",
      "Best mean reward: -295.19 - Last mean reward per episode: -289.03\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 46000\n",
      "Best mean reward: -289.03 - Last mean reward per episode: -284.79\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 47000\n",
      "Best mean reward: -284.79 - Last mean reward per episode: -279.48\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 48000\n",
      "Best mean reward: -279.48 - Last mean reward per episode: -276.05\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 49000\n",
      "Best mean reward: -276.05 - Last mean reward per episode: -271.88\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 50000\n",
      "Best mean reward: -271.88 - Last mean reward per episode: -268.43\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 51000\n",
      "Best mean reward: -268.43 - Last mean reward per episode: -260.27\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 52000\n",
      "Best mean reward: -260.27 - Last mean reward per episode: -259.64\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 53000\n",
      "Best mean reward: -259.64 - Last mean reward per episode: -254.30\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 54000\n",
      "Best mean reward: -254.30 - Last mean reward per episode: -249.02\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 55000\n",
      "Best mean reward: -249.02 - Last mean reward per episode: -246.28\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 56000\n",
      "Best mean reward: -246.28 - Last mean reward per episode: -242.53\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 57000\n",
      "Best mean reward: -242.53 - Last mean reward per episode: -238.69\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 58000\n",
      "Best mean reward: -238.69 - Last mean reward per episode: -236.01\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 59000\n",
      "Best mean reward: -236.01 - Last mean reward per episode: -233.89\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 60000\n",
      "Best mean reward: -233.89 - Last mean reward per episode: -230.39\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 61000\n",
      "Best mean reward: -230.39 - Last mean reward per episode: -227.23\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 62000\n",
      "Best mean reward: -227.23 - Last mean reward per episode: -224.21\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 63000\n",
      "Best mean reward: -224.21 - Last mean reward per episode: -222.22\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 64000\n",
      "Best mean reward: -222.22 - Last mean reward per episode: -220.09\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 65000\n",
      "Best mean reward: -220.09 - Last mean reward per episode: -214.69\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 66000\n",
      "Best mean reward: -214.69 - Last mean reward per episode: -213.70\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 67000\n",
      "Best mean reward: -213.70 - Last mean reward per episode: -211.60\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 68000\n",
      "Best mean reward: -211.60 - Last mean reward per episode: -210.58\n",
      "Saving new best model to tmp/best_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 69000\n",
      "Best mean reward: -210.58 - Last mean reward per episode: -207.48\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 70000\n",
      "Best mean reward: -207.48 - Last mean reward per episode: -206.19\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 71000\n",
      "Best mean reward: -206.19 - Last mean reward per episode: -206.29\n",
      "Num timesteps: 72000\n",
      "Best mean reward: -206.19 - Last mean reward per episode: -204.09\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 73000\n",
      "Best mean reward: -204.09 - Last mean reward per episode: -201.35\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 74000\n",
      "Best mean reward: -201.35 - Last mean reward per episode: -200.38\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 75000\n",
      "Best mean reward: -200.38 - Last mean reward per episode: -199.77\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 76000\n",
      "Best mean reward: -199.77 - Last mean reward per episode: -199.05\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 77000\n",
      "Best mean reward: -199.05 - Last mean reward per episode: -200.00\n",
      "Num timesteps: 78000\n",
      "Best mean reward: -199.05 - Last mean reward per episode: -199.25\n",
      "Num timesteps: 79000\n",
      "Best mean reward: -199.05 - Last mean reward per episode: -197.58\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 80000\n",
      "Best mean reward: -197.58 - Last mean reward per episode: -198.13\n",
      "Num timesteps: 81000\n",
      "Best mean reward: -197.58 - Last mean reward per episode: -195.35\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 82000\n",
      "Best mean reward: -195.35 - Last mean reward per episode: -194.27\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 83000\n",
      "Best mean reward: -194.27 - Last mean reward per episode: -190.71\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 84000\n",
      "Best mean reward: -190.71 - Last mean reward per episode: -185.94\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 85000\n",
      "Best mean reward: -185.94 - Last mean reward per episode: -186.05\n",
      "Num timesteps: 86000\n",
      "Best mean reward: -185.94 - Last mean reward per episode: -184.32\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 87000\n",
      "Best mean reward: -184.32 - Last mean reward per episode: -181.60\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 88000\n",
      "Best mean reward: -181.60 - Last mean reward per episode: -176.72\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 89000\n",
      "Best mean reward: -176.72 - Last mean reward per episode: -173.19\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 90000\n",
      "Best mean reward: -173.19 - Last mean reward per episode: -167.14\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 91000\n",
      "Best mean reward: -167.14 - Last mean reward per episode: -162.48\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 92000\n",
      "Best mean reward: -162.48 - Last mean reward per episode: -159.44\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 93000\n",
      "Best mean reward: -159.44 - Last mean reward per episode: -156.62\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 94000\n",
      "Best mean reward: -156.62 - Last mean reward per episode: -150.57\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 95000\n",
      "Best mean reward: -150.57 - Last mean reward per episode: -150.81\n",
      "Num timesteps: 96000\n",
      "Best mean reward: -150.57 - Last mean reward per episode: -143.91\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 97000\n",
      "Best mean reward: -143.91 - Last mean reward per episode: -143.62\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 98000\n",
      "Best mean reward: -143.62 - Last mean reward per episode: -140.31\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 99000\n",
      "Best mean reward: -140.31 - Last mean reward per episode: -138.03\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 100000\n",
      "Best mean reward: -138.03 - Last mean reward per episode: -137.43\n",
      "Saving new best model to tmp/best_model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAACICAYAAADqIJGqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAkaUlEQVR4nO3debzcdX3v8dd75sxZc7JDWRMCN9FS1AARuLcVW4xClQpt7YVCBWutXeQ297q0eG2tSvW2Wq1QrWLBKioqqFWKLCWtFsWyJBglQAkRCAGiJGQ5yVln+dw/ft85mZzMnPnNvpzP8/H4PWZ++3d+v/nN7zPf7SczwznnnHOumyRanQDnnHPOuXrzAMc555xzXccDHOecc851HQ9wnHPOOdd1PMBxzjnnXNfxAMc555xzXccDHOecazBJn5P0V61Oh3NziQc4zs1xkg4UDDlJ4wXjl0p6n6S0pP1h2CLpE5KOLtjGyZI2SNoThvWSTp5ln9+V9JbmfMLSPPBwrnt5gOPcHGdm8/ID8DTwawXTvhQW+6qZDQOLgV8HjgI2FgQ5zwFvCPOXArcAX2nqB6mQpGSr01COpJ5Wp8G5TuUBjnMuNjNLm9nDwEXATuAdYfpeM3vKoq7RBWSB/1bp9kNu0RcLxk+QZPkbfcj5uUrSPSE36V8lLS1Y/mZJP5W0T9Ldkn6hYN7nJH1K0m2SRoFfKZOWqyVtlzQiaaOkV8xI502SbgjpeFjSmoL5p0p6MMz7KtA/Y9vnS9okaa+kH0h6acG8pyT9maQfA6Me5DhXHQ9wnHMVM7Ms8C3gFYXTJe0FJoC/Bz7UoN1fAvwucCTQC7yzYN7twMow70HgS0XW/SAwDHy/zH4eAFYT5UrdCNwsqTBQeT1RLtVCohyrTwBI6gW+CXwhrHsz8Jv5lSSdCnwW+ANgCXAtcIukvoJt/zbwOmChmWXKpNM5V4QHOM65aj1HdAOfZmYLgQXAFcAPG7TffzKzLWY2DtxEFITk9/9ZM9tvZpPA+4CXSVpQsO63zOweM8uZ2cRsOzGzL5rZC2aWMbOPAn3AiwoW+b6Z3RaCvS8ALwvTzwJSwMdDjtfXiIKlvLcC15rZfWaWNbPPA5NhvbxrzGx7+IzOuSp4gOOcq9axwO6ZE81sFPg0cIOkIxuw358WvB8D5kFUp0bSX0v6iaQR4KmwzNKC5bfH3Ymkd0p6NBR37SUK3Aq3NTMd/aE46RjgWTv0ScbbCt4vB94Riqf2hm0fH9arOJ3OueI8wHHOVUxSAvg14HslFkkAg0RBUCVGw3p5R1Ww7iXABcBaomDkhDBdBcsYMYT6Nn8K/E9gUciZ2jdjW6XsAI6VVLjssoL324EPmtnCgmHQzL5caTqdc6V5gOOci01Sj6SfB75MFHx8LEx/dahYm5Q0P0zfAzw6y+Z6JPUXDClgE3C2pGWhaOndFSRvmKio5wWiICluHaDkjHT0hm1liCpS90h6LzA/5vb+M6z7J5JSkn4DOKNg/j8CfyjpTEWGJL1O0nDM7TvnYvAAxzkXx0WSDhDlYtxCFEScbmbPhfkLiYKefcBPgJOA88rUc/kUMF4w/JOZ3QV8FfgxsBG4tYI03kBUFPQs8Ahwb8z1rpyRjn8H7gTuALaEbU4Qs9jIzKaA3wDeRFSEdxHwjYL5G4DfJ6qUvAfYGpZ1ztWRDi0mds4555zrfJ6D45xzzrmuU1GAIykRytedc84559pW2QBH0o2S5ksaAjYDj0h6V+OT5pxzzjlXnTg5OCeb2QhwIVEvoSuANzYyUc4555xztYjzjJNUaL55IfAJM0tL6oiayUuXLrUTTjih1clwzjnnXINs3Lhxl5kdMXN6nADnWqIeQX8E3C1pOTBS3+Q1xgknnMCGDRtanQznnHPONYikbcWmlw1wzOwa4JqCSdskzfoUXuecc865VioZ4Eh6e5l1P1bntDjnnHPOFbVx2x6uXr+FdWtXcfryRWWXny0HJ99t+IuAlxP1XgrR82furymVzjnnnHMVuHr9Fu5+fBcAN/zemWWXLxngmNn7ASTdDZxmZvvD+PuAb9chrc4555xzsaxbu+qQ13LiNBP/OWCqYHwqTGsJSedJekzSVklXtiodzjnnnGue05cv4obfOzNW8RTEa0V1A3C/pH8O4xcCn6sqdTWSlAQ+CbwaeAZ4QNItZvZIK9LjnHPOufY0a4AjSUQBzu3AK8Lk3zWzHzY6YSWcAWw1sydC+r4CXED05GDnnHPOOaBMEZVFjxq/zcweNLOrw9Cq4AbgWGB7wfgzYdo0SW+VtEHShp07dzY1cc455zrLxm17uOz6+9i4bU+rk9Jy3XYs4tTBeVDSyxuekjoxs8+Y2RozW3PEEYd1bOica0Pd9sPqOke+Zc7V67e0Oikt123HIk4dnDOBS0NPgaOAiDJ3XtrQlBX3LHB8wfhxYZpzroPFbf5ZaT8YzpVTacucbtZtxyJODs65wEnAOUR94JwfXlvhAWClpBWSeoGLOdg/z5zj/3pdt1i3dhVnr1xa9oe12/5hutartGVOKa36PS6335nzZ1u+2LGo5XNVu269jmXZAMfMtpnZNmAcsIKh6cwsA1wB3Ak8CtxkZg+3Ii3twH/sXbeIe5OJGwjVg/+BcJWI+3tc7+9Vsf0W7mPm/HLLV7r9StNWymxpLrdez+JjVxabVzbAkfR6SY8DTwL/QfTgzdvL7rVBzOw2M1tlZieZ2QdblY5mK/aFivtjPxd/qKv5zHPxODVavY9pvf5tz1QsnfX+A9Hu369W/9tuB7V8llblQhbbb+E+Zs4vt3yl25/NeacczaLBFOedcnTZzzFbmsutl+gdmF9sXpwiqquAs4AtZrYCeBVwb4z1XB0V+0LF/bHvlJyeev5QVvOZq/23EWf6XNWM7149jnmxdNaSW9SMgKnW9MxUbfpq+YcfV7Out1pyYWrJhazlcxTbb+E+Zs4vt3yl25/NHZt3sGcszR2bdxwyvdwf9kr+yKxbu4rc1PhIsXlxKhmnzewFSQlJCTP7jqSPx1jP1VEtlb/aoeJYnMqhlT5nZLZ9VPOZZ1tnZvpLpbWen6EbKtHW8t2LeyxqPeal0pn/ka1GsTQ16zosdtziHKNq01dsvXqck0KNut5mKvZZqj2epRT7XtX7c1T63W3U8qW+U8U+b7XX2+nLF5HZ/ezjRWea2awDsB6YB/w98GXgauAH5dZrh+H000831x7eeN29tvzPbrU3XnfvIdM3PLXb3njdvbbhqd2HvK/nPuph5rZLpbWdP0Oj1frZZ4p7LOq930oV2381aSq3TtxtFjtuzT5G9d5fo663OJpxPOv1HeoU9f5swAYrFr8Um3jIAjBEVJTVA1wO/AmwpNx67TDUI8Bp5Jesm7/AM5X6rJXc0MsFQ404nvltfunebU05V538nah3cNaOx6JYmur1ucttJ+6Nth2PWyXaLf21pKeWdTvtz04rz1upACdOEdXFwN1m9jjw+YrzjzpctVmHM7M1653N2WlKZT9Wki1eeLyAumVxxt1nLduOW9xSzWeotlir2Hpxt1VsubjnspHHotEaWfRUbjtxi4LqedzqWWQa9/vWrGLeZnwPa/ks7VC1oBLFPuu+8TT3P7mbyUwWs9AE2wwL7bCNg+9zBgcm0oxMZBgZTzMykWY8nWMinWUinWUynWMykyVrRi4HObMwlE5TnABnGXCtpBXABuBu4Htmtqmqo9Bhqv2SzTzZjfxh7OR6G5X8eBQ7XvW4+Gc7fvU6R40MZqvddrH14m6rlhtrJwf29a6rU6jcdorNb/RNsJ7nKu73rdbPVMt3uN5q+SytCPAn0lleGJ1ibDLD2FSWsaks4+kM+ycy7Dowxc79k9FwYJI9o1PkzEhISDA6mWGwN8kjO0Z42fv/lbGpDOlsdT3KDPYmGe7vYSCVpD+VpC+VpL8nwcLBXnoSQhIJQTIhEhL/VmI7MouXAEkDwO8D7wSONbNkVSlvojVr1tiGDRtasu84OTjVbmumy66/j7sf38XZK5d23A0jjkqOXTXHuRnHr5FBaLvk4DQ6va52+WN/3ilHc8fmHQ09z3G21YjvQjO+w60SN82ZbI7RqSyjkxnGpjKMTGTYfWCK3aNT7BqNgpO9Y2n2jKXZNx6933Vgkj1j6Vn3n0qKI+b1ccRwH4uGeklKGFFuCsBAKslQXw9DvdHrvP4e1ixfzMLBFAIUgqH8e8i/ByGG+pLMH0iRSsZp4H2QpI1mtuaw6eUCHEl/DvwiUUXjHwLfJ8rB2THrim2g2QFOPS6YYtsodwPuxAu1EpUEINUEK/U6fs36AXfdr1Hfm/z1sWgwxZ6xdNf+KWpnuZwxlc0dMk0CM5jM5JjK5JjKRq+TmSyjk1nGpjKMTmb5u7se47GfHWDZ4gFeuepIdo9NsXdsiv0TGQ5MZNg/Gb2Op7OzpqE/lWDRYC8LBlIsGuxl4WCKxUO9HDW/nyOG+xjq62GwN8lAb5LB3h7m9SVZOq+PBQOp6cCknZQKcOIUUf0GkAG+TdTR33+a2WSd09cVSmV5VvJjVU2WbTvWVainSrJ5q8kSrtfxq6XIp5xWBkqdFKR1SlrLpbNRxSf566IwB6fTmRmTmaiuxthUlslMjuH+HhYOpOiJmRMwlckxPpVlLJ0hZ5AQ00UvSYmpbC4qrpnKMh72M5XJkc3lSGeNbM5IZ3NMZHIcmMhwYDI9HXCMjEc5JXvGopySvWNTs9YbiWP77nFu/fFz08HJkqFeli8ZYl5fknl9PVHuSRgG+3oY7uth8VAvS+b1smSoj4Heti+AqYuyAY6ZnSZpPlEuzquBz0h63sx+qeGpa0PV1Neo5MeqkWX8naqSz9/KYzVbHaGZ34lKb8StrLfSSXVmOiWt5dLZqLo1hdfHJWcuK7t8PnjI5Ix0Jkc6l2MyHd3sD4Tij9HJqBJoOhstl8lGN/1ckdIBs6ieRz4gmchkmUiH4GIqqvcxkY6CiEwuChwyIYDI5KKWMVEl06hyaiZnTIQKrMUkE6InIVLJBAmFZwwVVHbNmk0HKPWUTGg6wJg/kGLRYIqfP2o+CwdTLBxMMdjbM51rkydBbzJBX0+C3vyQTDLUl5zOURnq7WHBQIr5AymSifbLSWk3cYqoTgFeAbwSWANsJyqiem/jk1ebYkVUtf7Di1sEUrgfoCP+VbrmqbQozXNw4umUtNYrndmcMZ7OMpnOMpXNhZYmuUOCiPzr3vE0z+4Z5/n9E+zcP8nIRIZsLjcdQGRzRjqXYyKdYzKdZSIUlzRCQtAfKpD29yQYyBeHpHqi96kkPckoOEkmEvQkRCIhkokoZ+XgwPS6A6lo6O1JsH8iw96xNFPZLJmsHRJw5et7KFRSTSUVrdsbBRFJabp1TjY0N04lE1GRTSoqshnoTdLXkzgsjX09CYb7U/SnEm1ZlNOtaqmDcyvwvTA8YGaz10JqI8UCnForlMb9Yer2ir+uNvW+EdezUrtrjGzOeG7vOHvHQhPYqex0XYupTI7JbI6J0HJlLJ1hbDKqJDoacknyOSb51i2jkxkmKwxAUklx5HA/S4f7oiKchKJcjuTBm3R/KkFfT5K+/GtPglQyygV5bu8E//HY87x+9TGccuyCg3U1UklSyfwNP1o+kRDi0CcziyiwybeEca4eqq6DY2bnhxZUyzopuCml1qzfWruorhe/gXW2uN+jah9X0ClFNY1Qj2sjmzP2jadDK5R8vYsM46FoZmQiw/6JNCPj0ev+icLX6H02NKFNSCQSMDqZZffoVKz99/YkovoToVhiqC9qNnvU/H4G+6Jp+df+VCJqSttzMCDp7Tk4Lf86fyDFEfP6SNRQtHHZ9fex5fkD3P/kbq44p+gDnJ1rG2UDHEm/Bvwt0AuskLQa+ICZvb7BaWuIZtXRaPR+5vINrBLtEAjWkoa453lmQN1pnYTV08fveozvbX2Bn+6bYKA3yfkvPYZjFw0wPpVlIDRf3TeeZveBSXaPTvHC6BQvHJhi91jUjHb36BR7xqZK1uso1JMQw/1RPYvh/h6G+1IsXzLIcH+KVDIq6sjmovoeCE5btogjh/uYP5BiIBXlkvQmD9a5yBezxK0c22xz+XvlOk+cIqqNwDnAd83s1DDtITN7SRPSV5NW9oPTaM28cbdDkFCtmUWFrfgstRRXdvKxr4dczqZbouwLvZuOjGc4MJlhdDJ6PTCZCQFLFJw8smOEA5OZWNuXYNFgL4uHomFJweuiod6QixLlpPSnkgz2JpnX3xMFNf0p+nri17WY6+fSuUappZl42sz2zbiI61vlvEHGprJcdv19XfmD0szWQp2cWzTzH2ezPkutTzbP64YWdGY2HYTkh71jaV4YnQpBySQvjE6xbzwdBSwTBwOXA5OZsjkpqaSY35+abgb72pccxctPWMyuA1Pc9tBzXHLGck5dvpD+niTj6SwPbtvDNx58hreds5JXrjqiaa1ROvk66nQeXM5NcQKchyVdAiQlrSR62OYPGpus+vjZyERDf1AKewW96YGnQeIvzj+57S+gSi/2Ts6WnhkgNOuzzLyZdcsNbSqTmw5S9ofnxhTWQdk9mp5upbNz/yS7QjHQbK1w5/f3sCR0IpavZzIv9II6r+9gs9j5/anpZYbDvHn9PfT1lO7T449++aTDpv2/2x5l49N7+dw9T3LOi4+sx2GJpZOvo07nweXcFKeIahB4D/AaokrwdwBXdUJnfye/9FRbs+7TDYvaZ/YKCnREqylv4dUY7dg1gOWbu4beU0cnDwYkhbklUWud0GonVKIdKSgSygc15XpI7U0mOGK4j6XDfdNdui8Z6g1BysFgZeFAL0vnRcVAlXbLXq1KH1Pguofn4HS3WlpRjREFOO8JG3oR8Ami51K1tcHe5PQNvBFf8MJeQfM5OJ3w76we/yT9B+Nw9ci1mcxk2bl/kuf3T/L8SPRQu50jEzy/P8oJyYQOz/aOTbHthVEWD/WFiqw23dX7ZCbq+ySTzVXcY2oyIYZ6o+fBLAi5JsuXDLIgjC8YSLFg8OC8KDfl0FyVdm3+6//i546Zv0/dUNTrKlcywJH0UqLWU8cA3wQ+SRTYnAl8tBmJq6dqftzK3cRL9Qra7jf/elzsfrM4XLHA0czYM5bmnq272D+RYSoEH1OZHGPpKJj52cgEz49M8rP9E+wt8rC7hGDJvCgnJJVMkEiIp3aNMjKRIZlI8IqVS6frkeSbBPf2JEglomWT0nSHZPPyAUlfFJDku3TP92dSSaXZTuNFRHOH/z45mD0H5x+BTwH/CfwqsAn4PHCpmU00Pmn1Vc2PW7VB0Vs+/8B0kVW3XlyFuVfdWpG7UsUCx1Ovuqto0AJRbsmRw30cOb+f5UsGOWPF4jDex5HD0UPvjhzuY/FQ72HNhts9iG5H/i9+7vBg1sEsdXAkbTKz1QXjT5jZic1KWD3U2ky83E1ktid/LxpM8a5zX9z1Zf2NrM/TDTfxa//jJxhw1olLOGp+P709B5814725Oudc7aqpg9Mv6VSiisUAk4XjZvZg/ZPZeJXcNMv94yv35O9mZJO2Ogho5D+lbshm/oNXHt6KxznnXOPNFuDsAD5WMP7TgnEj6vyv7c1sOTEykWHT9r1A7TfNwpv7zH5P8vssXK4RWh0EVJLtP5eap7ebVgfCzjnXbCUDHDP7lWYmpFHyAcBDz+5jz1ia1cct4OyVSyu+aZZ7mGG+qCavWUFHJwUBlQZjXmeiflodCDvnXLPF6eivoxVWhq2lPkzhDWLd2lWHVSQuFmjMzNnJ77ee/6Y7KQjopGCs2/ixd87NNWU7+utk9XwWVWFQkg924lQkLlYJtxM62mt0kYYXmTjnnKuHUpWM2/ORtS20cdseLrv+PjZu23PI9HxOyenLF7Fu7SrOXrmU6y5/OXds3sHdj+/iLZ9/4LB1gOllZ+bsVFNM1gilPm8+iLt6/ZaG7LfR23fOOTe3xXlUg4BLgRPN7AOSlgFHmdn9zUhgLSrNwSnswyZu7ko167STUrlJs+Ww1CP3xXNwnHPO1UMtTxP/ByBH1GrqA8B+4OvAy+uawibauG0PV/3Lw4c9HPPq9VvYM5amJ6HpFlDlnL58Eddd/vJDnkHUSUrVzZitbk89Kqx2Ut0h55xznSdOEdWZZvY2YALAzPYAvbXsVNL7JD0raVMYXlsw792Stkp6TNK5BdPPC9O2Srqylv1fvX4Lm57Zx6btew8pIlm3dhWLBlNkcsYdm3cctl6c4qtWK5XGUqpJezsVsTnnnHPFxAlw0pKSRH3fIOkIohydWv2dma0Ow21h2ycDFwO/AJwH/IOkZNj/J4keGXEy8Nth2Vhm3vTXrV3F6uMWsPr4hYfcpPO5McVu3vmiqNnqjVQaXDRCM+q2tFNA55xzzhUTJ8C5Bvhn4EhJHwS+D3yoQem5APiKmU2a2ZPAVuCMMGw1syfMbAr4Slg2lmI3/fkDKf7i/ChGKgxKSt2888VXiwZTJXMu2qHirOeuOOecczHq4JjZlyRtBF5F9JiGC83s0Trs+wpJlwEbgHeEoq9jgXsLlnkmTAPYPmN60Qockt4KvBVg2bLoCd8z65kU1iGBwzvlm9n7cb534vw2SuVctENfI163xTnnnJslwJG0uGD0eeDLhfPMbPdsG5a0HjiqyKz3ED2l/CqiYq+rgI8Cb46f7NLM7DPAZyBqRQWH3/RLdcqXN7P3Y4iCn3KBw1wKLuZyK6i5/Nmdc65TzJaDs5EoABGwDNgT3i8EngZWzLZhM1sbJwGS/hG4NYw+CxxfMPu4MI1ZplcsH4iUulEV6/24UzTr5juXu/6fy5/dOec6xWzPoloB0wHIPxdUBP5V4MJadirpaDPLN1P6dWBzeH8LcKOkjwHHACuB+4kCq5WSVhAFNhcDl9SSBih9oyrMibnkzGW17qapqr35+oMw45vLn9055zpFnH5wzjKz38+PmNntkj5c434/LGk1UQ7RU8AfhG0/LOkm4BEgA7zNzLIAkq4A7gSSwGfN7OEa01DyaeCdXOxQ7c3XH4QZ31z+7M451yniBDjPSfpz4Ith/FLguVp2amZvnGXeB4EPFpl+G3BbpfuKG7hcdesjbNq+l5GJDN982y9Wupu2Ue3N13MlnHPOdZM4Ac5vA39J1FQc4O4wrSPMljNxSGuq/CMrijy6oltyd2bjuRLOOee6SZxm4ruBdZKGo1E70Phk1c9sORPFmo8XW84rlTrnnHOdJc7DNl8C3ADkm43vAi43s82l12oPlT5ss5Qb73uaj9z5X7zr3Bd3XKVj55xzrpuVethmnJ6MrwXebmbLzWw58A5CPzNzxR2bd7BnLF30+VTOOeecaz9x6uAMmdl38iNm9l1JQw1MU9vxCrjOOedcZ4kT4Dwh6S+AL4Tx3wGeaFyS2o9XwHXOOec6S5wiqjcDRwDfCMNS6vRYhXbVDk8Fd84551z14rSi2gP8CYCkJFGR1UijE9ZK3mqqNnOhWb1zzrn2VjYHR9KNkuaHejcPAY9Ielfjk9Y8M3Ns1q1dxdkrl3qdmyrlA8Sr129pdVKcc87NUXHq4JxsZiOSLgVuB64kehDnRxqasiaamWPjdW5q45WynXPOtVqcACclKUX0gM1PmFla0uyd53QYvyHXlweIzjnnWi1OgHMt0QMxfwTcLWk50FV1cPyG7JxzznWXOJWMrwGuKZi0TdKvNC5JzjnnnHO1KRngSPodM/uipLeXWORjDUqTc84551xNZmtFle+teLjE0BG8TxvnnHNu7imZg2Nm14bX9zcvOfXnfdo455xzc0+cfnBOlPQvknZKel7StySd2IzE1YP3aeOcc87NPXEe1XAjcBNwNHAMcDPw5UYmqp5OX76IdWtXcfX6LV5M5Zxzzs0RcQKcQTP7gpllwvBFoL/RCasn71nXOeecm1vi9INzu6Qrga8ABlwE3CZpMYCZ7W5g+urCO/Jzzjnn5haZzd4psaQnZ5ltZta29XHWrFljGzZsaHUynHPOOdcgkjaa2ZqZ0+N09LeiMUlyzjnnnGuMkjk4kv7UzD4c3v+Wmd1cMO9DZvZ/m5TGqknaDzzW6nS4wywFdrU6Ee4Qfk7aj5+T9uTnpf0sN7MjZk6cLcB50MxOm/m+2Hi7krShWLaVay0/L+3Hz0n78XPSnvy8dI7ZWlGpxPti484555xzbWO2AMdKvC827pxzzjnXNmarZPwySSNEuTUD4T1hvFP6wflMqxPgivLz0n78nLQfPyftyc9LhyjbTNw555xzrtPE6cnYOeecc66jeIDjnHPOua7TtQGOpPMkPSZpa3jUhKsjScdL+o6kRyQ9LGldmL5Y0l2SHg+vi8J0SbomnI8fSyrsduDysPzjki4vmH66pIfCOtdI8tZ7MUhKSvqhpFvD+ApJ94Xj+FVJvWF6XxjfGuafULCNd4fpj0k6t2C6X1cVkrRQ0tck/ZekRyX9d79OWk/S/wm/XZslfVlSv18rXcbMum4AksBPgBOBXuBHwMmtTlc3DURPlz8tvB8GtgAnAx8GrgzTrwT+Jrx/LXA7USX1s4D7wvTFwBPhdVF4vyjMuz8sq7Dur7b6c3fCALwduBG4NYzfBFwc3n8a+KPw/o+BT4f3FwNfDe9PDtdMH7AiXEtJv66qPh+fB94S3vcCC/06afk5ORZ4EhgI4zcBb/JrpbuGbs3BOQPYamZPmNkU0YNCL2hxmrqKme0wswfD+/3Ao0Q/GhcQ/aATXi8M7y8AbrDIvcBCSUcD5wJ3mdluM9sD3AWcF+bNN7N7LfoluaFgW64ESccBrwOuC+MCzgG+FhaZeU7y5+prwKvC8hcAXzGzSTN7EthKdE35dVUhSQuAs4HrAcxsysz24tdJO+ghaiHcAwwCO/Brpat0a4BzLLC9YPyZMM01QMiuPRW4D/g5M9sRZv0U+LnwvtQ5mW36M0Wmu9l9HPhTIBfGlwB7zSwTxguP4/SxD/P3heUrPVeutBXATuCfQrHhdZKG8OukpczsWeBvgaeJApt9wEb8Wukq3RrguCaRNA/4OvC/zWykcF74R+n9EDSJpPOB581sY6vT4qb1AKcBnzKzU4FRoiKpaX6dNF+o83QBUQB6DDAEnNfSRLm669YA51ng+ILx48I0V0eSUkTBzZfM7Bth8s9Ctjnh9fkwvdQ5mW36cUWmu9J+EXi9pKeIssTPAa4mKubId+pZeBynj32YvwB4gcrPlSvtGeAZM7svjH+NKODx66S11gJPmtlOM0sD3yC6fvxa6SLdGuA8AKwMNeJ7iSqF3dLiNHWVUP58PfComX2sYNYtQL6Fx+XAtwqmXxZaiZwF7AtZ9HcCr5G0KPyreg1wZ5g3IumssK/LCrblijCzd5vZcWZ2AtF3/t/N7FLgO8AbwmIzz0n+XL0hLG9h+sWh5cgKYCVRRVa/ripkZj8Ftkt6UZj0KuAR/DpptaeBsyQNhuOWPy9+rXSTVtdybtRA1BphC1FN9ve0Oj3dNgC/RJSt/mNgUxheS1Qu/W/A48B6YHFYXsAnw/l4CFhTsK03E1XO2wr8bsH0NcDmsM4nCD1v+xDr/PwyB1tRnUj0o7sVuBnoC9P7w/jWMP/EgvXfE477YxS0yvHrqqpzsRrYEK6VbxK1gvLrpPXn5f3Af4Vj9wWillB+rXTR4I9qcM4551zX6dYiKuecc87NYR7gOOecc67reIDjnHPOua7jAY5zzjnnuo4HOM4555zrOh7gOOeaJjxZ+4/D+2Mkfa3cOjXsa7Wk1zZq+8659uYBjnOumRYSPZkZM3vOzN4w++I1WU3UF4lzbg7yAMc510x/DZwkaZOkmyVtBpD0JknflHSXpKckXSHp7eEBlfdKWhyWO0nSHZI2SvqepBeH6b8labOkH0m6O/Qe+wHgorCviyQNSfqspPvDdi8o2Pe3JH1X0uOS/jJMH5L07bDNzZIuaskRc85Vpaf8Is45VzdXAqeY2erwFPpbC+adQvRU+n6iHmP/zMxOlfR3RI8g+DjwGeAPzexxSWcC/0D0zK33Auea2bOSFprZlKT3EvUEfAWApA8RdbH/ZkkLgfslrQ/7PiPsfwx4QNK3geXAc2b2urD+ggYdE+dcA3iA45xrF98xs/3Afkn7gH8J0x8CXhqeXP8/gJujxwcBUff6APcAn5N0E9GDE4t5DdHDSN8ZxvuBZeH9XWb2AoCkbxA9iuQ24KOS/obosRffq8eHdM41hwc4zrl2MVnwPlcwniP6rUoAe81s9cwVzewPQ47O64CNkk4vsn0Bv2lmjx0yMVpv5jNrzMy2SDqNqB7PX0n6NzP7QBWfyznXAl4HxznXTPuB4WpWNLMR4ElJvwXRE+0lvSy8P8nM7jOz9wI7geOL7OtO4H+Fp0cj6dSCea+WtFjSAHAhcI+kY4AxM/si8BHgtGrS7ZxrDQ9wnHNNE4qB7gmViz9SxSYuBX5P0o+Ah4ELwvSPSHoobPcHwI+A7wAn5ysZA1cBKeDHkh4O43n3A18neuL3181sA/ASono6m4C/BP6qivQ651rEnybunJvTJL2JgsrIzrnu4Dk4zjnnnOs6noPjnHPOua7jOTjOOeec6zoe4DjnnHOu63iA45xzzrmu4wGOc84557qOBzjOOeec6zr/H3VSof30r7cAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages, 2 for debug messages\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose >= 1:\n",
    "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose >= 1:\n",
    "                    print(f\"Saving new best model to {self.save_path}\")\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# Add some action noise for exploration\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "# Because we use parameter noise, we should use a MlpPolicy with layer normalization\n",
    "model = TD3(\"MlpPolicy\", env, action_noise=action_noise, verbose=0)\n",
    "# Create the callback: check every 1000 steps\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n",
    "# Train the agent\n",
    "timesteps = 1e5\n",
    "model.learn(total_timesteps=int(timesteps), callback=callback)\n",
    "\n",
    "plot_results([log_dir], timesteps, results_plotter.X_TIMESTEPS, \"TD3 LunarLander\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common import results_plotter\n",
    "\n",
    "# Helper from the library\n",
    "results_plotter.plot_results(\n",
    "    [log_dir], 1e5, results_plotter.X_TIMESTEPS, \"A2C BionicEnv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(values, window):\n",
    "    \"\"\"\n",
    "    Smooth values by doing a moving average\n",
    "    :param values: (numpy array)\n",
    "    :param window: (int)\n",
    "    :return: (numpy array)\n",
    "    \"\"\"\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, \"valid\")\n",
    "\n",
    "\n",
    "def plot_results(log_folder, title=\"Learning Curve\"):\n",
    "    \"\"\"\n",
    "    plot the results\n",
    "\n",
    "    :param log_folder: (str) the save location of the results to plot\n",
    "    :param title: (str) the title of the task to plot\n",
    "    \"\"\"\n",
    "    x, y = ts2xy(load_results(log_folder), \"timesteps\")\n",
    "    y = moving_average(y, window=50)\n",
    "    # Truncate x\n",
    "    x = x[len(x) - len(y) :]\n",
    "\n",
    "    fig = plt.figure(title)\n",
    "    plt.plot(x, y)\n",
    "    plt.ylim(0,2e3)\n",
    "    plt.xlabel(\"Number of Timesteps\")\n",
    "    plt.ylabel(\"Rewards\")\n",
    "    plt.title(title + \" Smoothed\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
