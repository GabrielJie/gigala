{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import gym\n",
    "from gym import spaces\n",
    "import time\n",
    "import random\n",
    "from stable_baselines3 import A2C\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Element Model of the Space Frame Element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlaneTrussElementLength(x1,y1,z1,x2,y2,z2):\n",
    "    return math.sqrt((x2-x1)*(x2-x1) + (y2-y1)*(y2-y1)+(z2-z1)*(z2-z1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpaceFrameElementStiffness(E,G,A,Iy,Iz,J,x1,y1,z1,x2,y2,z2):\n",
    "    L = PlaneTrussElementLength(x1,y1,z1,x2,y2,z2)\n",
    "    w1 = E*A/L\n",
    "    w2 = 12*E*Iz/(L*L*L)\n",
    "    w3 = 6*E*Iz/(L*L)\n",
    "    w4 = 4*E*Iz/L\n",
    "    w5 = 2*E*Iz/L\n",
    "    w6 = 12*E*Iy/(L*L*L)\n",
    "    w7 = 6*E*Iy/(L*L)\n",
    "    w8 = 4*E*Iy/L\n",
    "    w9 = 2*E*Iy/L\n",
    "    w10 = G*J/L\n",
    "    \n",
    "    kprime = np.array([[w1, 0, 0, 0, 0, 0, -w1, 0, 0, 0, 0, 0],\n",
    "                        [0, w2, 0, 0, 0, w3, 0, -w2, 0, 0, 0, w3], \n",
    "                        [0, 0, w6, 0, -w7, 0, 0, 0, -w6, 0, -w7, 0],\n",
    "                        [0, 0, 0, w10, 0, 0, 0, 0, 0, -w10, 0, 0],\n",
    "                        [0, 0, -w7, 0, w8, 0, 0, 0, w7, 0, w9, 0],\n",
    "                        [0, w3, 0, 0, 0, w4, 0, -w3, 0, 0, 0, w5],\n",
    "                        [-w1, 0, 0, 0, 0, 0, w1, 0, 0, 0, 0, 0],\n",
    "                        [0, -w2, 0, 0, 0, -w3, 0, w2, 0, 0, 0, -w3],\n",
    "                        [0, 0, -w6, 0, w7, 0, 0, 0, w6, 0, w7, 0],\n",
    "                        [0, 0, 0, -w10, 0, 0, 0, 0, 0, w10, 0, 0],\n",
    "                        [0, 0, -w7, 0, w9, 0, 0, 0, w7, 0, w8, 0],\n",
    "                        [0, w3, 0, 0, 0, w5, 0, -w3, 0, 0, 0, w4]])  \n",
    "    \n",
    "    \n",
    "    if x1 == x2 and y1 == y2:\n",
    "        if z2 > z1:\n",
    "            Lambda = np.array([[0, 0, 1], [0, 1, 0], [-1, 0, 0]])\n",
    "        else:\n",
    "            Lambda = np.array([[0, 0, -1], [0, 1, 0], [1, 0, 0]])\n",
    "    else:\n",
    "        CXx = (x2-x1)/L\n",
    "        CYx = (y2-y1)/L\n",
    "        CZx = (z2-z1)/L\n",
    "        D = math.sqrt(CXx*CXx + CYx*CYx)\n",
    "        CXy = -CYx/D\n",
    "        CYy = CXx/D\n",
    "        CZy = 0\n",
    "        CXz = -CXx*CZx/D\n",
    "        CYz = -CYx*CZx/D\n",
    "        CZz = D\n",
    "        Lambda = np.array([[CXx, CYx, CZx], [CXy, CYy, CZy], [CXz, CYz, CZz]])\n",
    "        \n",
    "        \n",
    "    R = np.array([np.concatenate((np.concatenate((Lambda,np.zeros((3,3)),np.zeros((3,3)),np.zeros((3,3))),axis=1),\n",
    "        np.concatenate((np.zeros((3,3)), Lambda, np.zeros((3,3)), np.zeros((3,3))),axis=1) ,\n",
    "        np.concatenate((np.zeros((3,3)), np.zeros((3,3)), Lambda, np.zeros((3,3))),axis=1), \n",
    "        np.concatenate((np.zeros((3,3)), np.zeros((3,3)), np.zeros((3,3)), Lambda),axis=1)))])[0]\n",
    "    return np.dot(np.dot(R.T,kprime),R)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpaceFrameAssemble(K,k,i,j):\n",
    "    K[6*i,6*i] = K[6*i,6*i] + k[0,0]\n",
    "    K[6*i,6*i+1] = K[6*i,6*i+1] + k[0,1]\n",
    "    K[6*i,6*i+2] = K[6*i,6*i+2] + k[0,2]\n",
    "    K[6*i,6*i+3] = K[6*i,6*i+3] + k[0,3]\n",
    "    K[6*i,6*i+4] = K[6*i,6*i+4] + k[0,4]\n",
    "    K[6*i,6*i+5] = K[6*i,6*i+5] + k[0,5]\n",
    "    K[6*i,6*j] = K[6*i,6*j] + k[0,6]\n",
    "    K[6*i,6*j+1] = K[6*i,6*j+1] + k[0,7]\n",
    "    K[6*i,6*j+2] = K[6*i,6*j+2] + k[0,8]\n",
    "    K[6*i,6*j+3] = K[6*i,6*j+3] + k[0,9]\n",
    "    K[6*i,6*j+4] = K[6*i,6*j+4] + k[0,10]\n",
    "    K[6*i,6*j+5] = K[6*i,6*j+5] + k[0,11]\n",
    "    K[6*i+1,6*i] = K[6*i+1,6*i] + k[1,0]\n",
    "    K[6*i+1,6*i+1] = K[6*i+1,6*i+1] + k[1,1]\n",
    "    K[6*i+1,6*i+2] = K[6*i+1,6*i+2] + k[1,2]\n",
    "    K[6*i+1,6*i+3] = K[6*i+1,6*i+3] + k[1,3]\n",
    "    K[6*i+1,6*i+4] = K[6*i+1,6*i+4] + k[1,4]\n",
    "    K[6*i+1,6*i+5] = K[6*i+1,6*i+5] + k[1,5]\n",
    "    K[6*i+1,6*j] = K[6*i+1,6*j] + k[1,6]\n",
    "    K[6*i+1,6*j+1] = K[6*i+1,6*j+1] + k[1,7]\n",
    "    K[6*i+1,6*j+2] = K[6*i+1,6*j+2] + k[1,8]\n",
    "    K[6*i+1,6*j+3] = K[6*i+1,6*j+3] + k[1,9]\n",
    "    K[6*i+1,6*j+4] = K[6*i+1,6*j+4] + k[1,10]\n",
    "    K[6*i+1,6*j+5] = K[6*i+1,6*j+5] + k[1,11]\n",
    "    K[6*i+2,6*i]   = K[6*i+2,6*i] + k[2,0]\n",
    "    K[6*i+2,6*i+1] = K[6*i+2,6*i+1] + k[2,1]\n",
    "    K[6*i+2,6*i+2] = K[6*i+2,6*i+2] + k[2,2]\n",
    "    K[6*i+2,6*i+3] = K[6*i+2,6*i+3] + k[2,3]\n",
    "    K[6*i+2,6*i+4] = K[6*i+2,6*i+4] + k[2,4]\n",
    "    K[6*i+2,6*i+5] = K[6*i+2,6*i+5] + k[2,5]\n",
    "    K[6*i+2,6*j]   = K[6*i+2,6*j] + k[2,6]\n",
    "    K[6*i+2,6*j+1] = K[6*i+2,6*j+1] + k[2,7]\n",
    "    K[6*i+2,6*j+2] = K[6*i+2,6*j+2] + k[2,8]\n",
    "    K[6*i+2,6*j+3] = K[6*i+2,6*j+3] + k[2,9]\n",
    "    K[6*i+2,6*j+4] = K[6*i+2,6*j+4] + k[2,10]\n",
    "    K[6*i+2,6*j+5] = K[6*i+2,6*j+5] + k[2,11]\n",
    "    K[6*i+3,6*i] = K[6*i+3,6*i] + k[3,0]\n",
    "    K[6*i+3,6*i+1] = K[6*i+3,6*i+1] + k[3,1]\n",
    "    K[6*i+3,6*i+2] = K[6*i+3,6*i+2] + k[3,2]\n",
    "    K[6*i+3,6*i+3] = K[6*i+3,6*i+3] + k[3,3]\n",
    "    K[6*i+3,6*i+4] = K[6*i+3,6*i+4] + k[3,4]\n",
    "    K[6*i+3,6*i+5] = K[6*i+3,6*i+5] + k[3,5]\n",
    "    K[6*i+3,6*j] = K[6*i+3,6*j] + k[3,6]\n",
    "    K[6*i+3,6*j+1] = K[6*i+3,6*j+1] + k[3,7]\n",
    "    K[6*i+3,6*j+2] = K[6*i+3,6*j+2] + k[3,8]    \n",
    "    K[6*i+3,6*j+3] = K[6*i+3,6*j+3] + k[3,9]\n",
    "    K[6*i+3,6*j+4] = K[6*i+3,6*j+4] + k[3,10]\n",
    "    K[6*i+3,6*j+5] = K[6*i+3,6*j+5] + k[3,11]\n",
    "    K[6*i+4,6*i] = K[6*i+4,6*i] + k[4,0]\n",
    "    K[6*i+4,6*i+1] = K[6*i+4,6*i+1] + k[4,1]\n",
    "    K[6*i+4,6*i+2] = K[6*i+4,6*i+2] + k[4,2]\n",
    "    K[6*i+4,6*i+3] = K[6*i+4,6*i+3] + k[4,3]\n",
    "    K[6*i+4,6*i+4] = K[6*i+4,6*i+4] + k[4,4]\n",
    "    K[6*i+4,6*i+5] = K[6*i+4,6*i+5] + k[4,5]\n",
    "    K[6*i+4,6*j] = K[6*i+4,6*j] + k[4,6]\n",
    "    K[6*i+4,6*j+1] = K[6*i+4,6*j+1] + k[4,7]\n",
    "    K[6*i+4,6*j+2] = K[6*i+4,6*j+2] + k[4,8]\n",
    "    K[6*i+4,6*j+3] = K[6*i+4,6*j+3] + k[4,9]\n",
    "    K[6*i+4,6*j+4] = K[6*i+4,6*j+4] + k[4,10]\n",
    "    K[6*i+4,6*j+5] = K[6*i+4,6*j+5] + k[4,11]\n",
    "    K[6*i+5,6*i] = K[6*i+5,6*i] + k[5,0]\n",
    "    K[6*i+5,6*i+1] = K[6*i+5,6*i+1] + k[5,1]\n",
    "    K[6*i+5,6*i+2] = K[6*i+5,6*i+2] + k[5,2]\n",
    "    K[6*i+5,6*i+3] = K[6*i+5,6*i+3] + k[5,3]\n",
    "    K[6*i+5,6*i+4] = K[6*i+5,6*i+4] + k[5,4]\n",
    "    K[6*i+5,6*i+5] = K[6*i+5,6*i+5] + k[5,5]\n",
    "    K[6*i+5,6*j] = K[6*i+5,6*j] + k[5,6]\n",
    "    K[6*i+5,6*j+1] = K[6*i+5,6*j+1] + k[5,7]\n",
    "    K[6*i+5,6*j+2] = K[6*i+5,6*j+2] + k[5,8]\n",
    "    K[6*i+5,6*j+3] = K[6*i+5,6*j+3] + k[5,9]\n",
    "    K[6*i+5,6*j+4] = K[6*i+5,6*j+4] + k[5,10]\n",
    "    K[6*i+5,6*j+5] = K[6*i+5,6*j+5] + k[5,11]\n",
    "    K[6*j,6*i] = K[6*j,6*i] + k[6,0]\n",
    "    K[6*j,6*i+1] = K[6*j,6*i+1] + k[6,1]\n",
    "    K[6*j,6*i+2] = K[6*j,6*i+2] + k[6,2]\n",
    "    K[6*j,6*i+3] = K[6*j,6*i+3] + k[6,3]\n",
    "    K[6*j,6*i+4] = K[6*j,6*i+4] + k[6,4]\n",
    "    K[6*j,6*i+5] = K[6*j,6*i+5] + k[6,5]\n",
    "    K[6*j,6*j] = K[6*j,6*j] + k[6,6]\n",
    "    K[6*j,6*j+1] = K[6*j,6*j+1] + k[6,7]\n",
    "    K[6*j,6*j+2] = K[6*j,6*j+2] + k[6,8]\n",
    "    K[6*j,6*j+3] = K[6*j,6*j+3] + k[6,9]\n",
    "    K[6*j,6*j+4] = K[6*j,6*j+4] + k[6,10]\n",
    "    K[6*j,6*j+5] = K[6*j,6*j+5] + k[6,11]\n",
    "    K[6*j+1,6*i] = K[6*j+1,6*i] + k[7,0]\n",
    "    K[6*j+1,6*i+1] = K[6*j+1,6*i+1] + k[7,1]\n",
    "    K[6*j+1,6*i+2] = K[6*j+1,6*i+2] + k[7,2]\n",
    "    K[6*j+1,6*i+3] = K[6*j+1,6*i+3] + k[7,3]\n",
    "    K[6*j+1,6*i+4] = K[6*j+1,6*i+4] + k[7,4]\n",
    "    K[6*j+1,6*i+5] = K[6*j+1,6*i+5] + k[7,5]\n",
    "    K[6*j+1,6*j] = K[6*j+1,6*j] + k[7,6]\n",
    "    K[6*j+1,6*j+1] = K[6*j+1,6*j+1] + k[7,7]\n",
    "    K[6*j+1,6*j+2] = K[6*j+1,6*j+2] + k[7,8]\n",
    "    K[6*j+1,6*j+3] = K[6*j+1,6*j+3] + k[7,9]\n",
    "    K[6*j+1,6*j+4] = K[6*j+1,6*j+4] + k[7,10]\n",
    "    K[6*j+1,6*j+5] = K[6*j+1,6*j+5] + k[7,11]\n",
    "    K[6*j+2,6*i] = K[6*j+2,6*i] + k[8,0]\n",
    "    K[6*j+2,6*i+1] = K[6*j+2,6*i+1] + k[8,1]\n",
    "    K[6*j+2,6*i+2] = K[6*j+2,6*i+2] + k[8,2]\n",
    "    K[6*j+2,6*i+3] = K[6*j+2,6*i+3] + k[8,3]\n",
    "    K[6*j+2,6*i+4] = K[6*j+2,6*i+4] + k[8,4]\n",
    "    K[6*j+2,6*i+5] = K[6*j+2,6*i+5] + k[8,5]\n",
    "    K[6*j+2,6*j] = K[6*j+2,6*j] + k[8,6]\n",
    "    K[6*j+2,6*j+1] = K[6*j+2,6*j+1] + k[8,7]\n",
    "    K[6*j+2,6*j+2] = K[6*j+2,6*j+2] + k[8,8]\n",
    "    K[6*j+2,6*j+3] = K[6*j+2,6*j+3] + k[8,9]\n",
    "    K[6*j+2,6*j+4] = K[6*j+2,6*j+4] + k[8,10]\n",
    "    K[6*j+2,6*j+5] = K[6*j+2,6*j+5] + k[8,11]\n",
    "    K[6*j+3,6*i] = K[6*j+3,6*i] + k[9,0]\n",
    "    K[6*j+3,6*i+1] = K[6*j+3,6*i+1] + k[9,1]\n",
    "    K[6*j+3,6*i+2] = K[6*j+3,6*i+2] + k[9,2]\n",
    "    K[6*j+3,6*i+3] = K[6*j+3,6*i+3] + k[9,3]\n",
    "    K[6*j+3,6*i+4] = K[6*j+3,6*i+4] + k[9,4]\n",
    "    K[6*j+3,6*i+5] = K[6*j+3,6*i+5] + k[9,5]\n",
    "    K[6*j+3,6*j] = K[6*j+3,6*j] + k[9,6]\n",
    "    K[6*j+3,6*j+1] = K[6*j+3,6*j+1] + k[9,7]\n",
    "    K[6*j+3,6*j+2] = K[6*j+3,6*j+2] + k[9,8]\n",
    "    K[6*j+3,6*j+3] = K[6*j+3,6*j+3] + k[9,9]\n",
    "    K[6*j+3,6*j+4] = K[6*j+3,6*j+4] + k[9,10]\n",
    "    K[6*j+3,6*j+5] = K[6*j+3,6*j+5] + k[9,11]\n",
    "    K[6*j+4,6*i] = K[6*j+4,6*i] + k[10,0]\n",
    "    K[6*j+4,6*i+1] = K[6*j+4,6*i+1] + k[10,1]\n",
    "    K[6*j+4,6*i+2] = K[6*j+4,6*i+2] + k[10,2]\n",
    "    K[6*j+4,6*i+3] = K[6*j+4,6*i+3] + k[10,3]\n",
    "    K[6*j+4,6*i+4] = K[6*j+4,6*i+4] + k[10,4]\n",
    "    K[6*j+4,6*i+5] = K[6*j+4,6*i+5] + k[10,5]\n",
    "    K[6*j+4,6*j] = K[6*j+4,6*j] + k[10,6]\n",
    "    K[6*j+4,6*j+1] = K[6*j+4,6*j+1] + k[10,7]\n",
    "    K[6*j+4,6*j+2] = K[6*j+4,6*j+2] + k[10,8]\n",
    "    K[6*j+4,6*j+3] = K[6*j+4,6*j+3] + k[10,9]\n",
    "    K[6*j+4,6*j+4] = K[6*j+4,6*j+4] + k[10,10]\n",
    "    K[6*j+4,6*j+5] = K[6*j+4,6*j+5] + k[10,11]\n",
    "    K[6*j+5,6*i] = K[6*j+5,6*i] + k[11,0]\n",
    "    K[6*j+5,6*i+1] = K[6*j+5,6*i+1] + k[11,1]\n",
    "    K[6*j+5,6*i+2] = K[6*j+5,6*i+2] + k[11,2]\n",
    "    K[6*j+5,6*i+3] = K[6*j+5,6*i+3] + k[11,3]\n",
    "    K[6*j+5,6*i+4] = K[6*j+5,6*i+4] + k[11,4]\n",
    "    K[6*j+5,6*i+5] = K[6*j+5,6*i+5] + k[11,5]\n",
    "    K[6*j+5,6*j] = K[6*j+5,6*j] + k[11,6]\n",
    "    K[6*j+5,6*j+1] = K[6*j+5,6*j+1] + k[11,7]\n",
    "    K[6*j+5,6*j+2] = K[6*j+5,6*j+2] + k[11,8]\n",
    "    K[6*j+5,6*j+3] = K[6*j+5,6*j+3] + k[11,9]\n",
    "    K[6*j+5,6*j+4] = K[6*j+5,6*j+4] + k[11,10]\n",
    "    K[6*j+5,6*j+5] = K[6*j+5,6*j+5] + k[11,11]\n",
    "    \n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FEA_u(coord, elcon, bc_node, bc_val, global_force, \n",
    "          E=210e6, G=84e6, A=2e-2, Iy=10e-5, Iz=20e-5, J=5e-5):\n",
    "    coord=np.array(coord)\n",
    "    elcon=np.array(elcon)\n",
    "    \n",
    "    K=np.zeros(shape=(6*(np.max(elcon)+1),6*(np.max(elcon)+1)))\n",
    "    for el in elcon:\n",
    "        k=SpaceFrameElementStiffness(E,G,A,Iy,Iz,J,\n",
    "                                     coord[el[0]][0],coord[el[0]][1],coord[el[0]][2],\\\n",
    "                                     coord[el[1]][0],coord[el[1]][1],coord[el[1]][2])\n",
    "        K=SpaceFrameAssemble(K,k,el[0],el[1])\n",
    "        \n",
    "    F = np.array(global_force)\n",
    "    \n",
    "    \n",
    "    # https://github.com/CALFEM/calfem-matlab/blob/master/fem/solveq.m\n",
    "    \n",
    "    bc=np.array([bc_node, \n",
    "                bc_val]).T\n",
    "    nd, nd=K.shape\n",
    "    fdof=np.array([i for i in range(nd)]).T\n",
    "    d=np.zeros(shape=(len(fdof),))\n",
    "    Q=np.zeros(shape=(len(fdof),))\n",
    "\n",
    "    pdof=bc[:,0].astype(int)\n",
    "    dp=bc[:,1]\n",
    "    fdof=np.delete(fdof, pdof, 0)\n",
    "    s=np.linalg.lstsq(K[fdof,:][:,fdof], (F[fdof].T-np.dot(K[fdof,:][:,pdof],dp.T)).T, rcond=None)[0] \n",
    "    d[pdof]=dp\n",
    "    d[fdof]=s.reshape(-1,)\n",
    "    \n",
    "#     Q=np.dot(K,d).T-F \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 The Space Frame Element - verification\n",
    "d=FEA_u(np.array([0,0,0,\n",
    "                  3,0,0,\n",
    "                  0,0,-3,\n",
    "                  0,-4,0]).reshape(4,3),\n",
    "        elcon=np.array([[0, 1],\n",
    "                      [0, 2],\n",
    "                      [0, 3]]),\n",
    "        bc_node=list(range(6,24)), \n",
    "        bc_val=[0]*18,\n",
    "        global_force=[-10,0,20,0,0,0,\n",
    "                                0,0,0,0,0,0,\n",
    "                                0,0,0,0,0,0,\n",
    "                                0,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.05147750e-06, -6.65367100e-08,  1.41769582e-05,  1.44778793e-06,\n",
       "        1.74858422e-06,  1.13605431e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_length(coord,elcon):\n",
    "    coord=np.array(coord)\n",
    "    elcon=np.array(elcon)\n",
    "    t_length=0\n",
    "    for i in range(len(elcon)):\n",
    "        l=PlaneTrussElementLength(coord[elcon[i][0]][0],\\\n",
    "                                    coord[elcon[i][0]][1],\\\n",
    "                                    coord[elcon[i][0]][2],\\\n",
    "                                    coord[elcon[i][1]][0],\\\n",
    "                                    coord[elcon[i][1]][1],\\\n",
    "                                    coord[elcon[i][1]][2])\n",
    "        t_length+=l        \n",
    "    return t_length    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possible_lines_dic(n,m,dx,dy):\n",
    "    A=[(-dx,0),(-dx,dy),(0,dy),(dx,dy),(dx,0),(dx,-dy),(0,-dy),(-dx,-dy)]\n",
    "    dic={}\n",
    "    t=0\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            for item in A:\n",
    "                x,y=j*dx,i*dy\n",
    "                x1,y1=x+item[0],y+item[1]\n",
    "                if (x1>=0 and x1<=(m-1)*dx and \n",
    "                    y1>=0 and y1<=(n-1)*dy and \n",
    "                    (x1,y1,x,y) not in dic):\n",
    "                    dic[(x,y,x1,y1)]=t\n",
    "                    t+=1\n",
    "    return dic                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = len(possible_lines_dic(n=5,m=5,dx=1,dy=1)) + 3 # +2 for x and y +1 for action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,n=5,m=5,dx=1,dy=1, force=-500,\n",
    "                 E=210e6, G=84e6, A=2e-2, Iy=10e-5, Iz=20e-5, J=5e-5, break_flag=False):\n",
    "        # n,m,dx,dy - grid parameters    \n",
    "        self.E=E\n",
    "        self.G=G\n",
    "        self.A=A\n",
    "        self.Iy=Iy\n",
    "        self.Iz=Iz\n",
    "        self.J=J\n",
    "        self.n=n\n",
    "        self.m=m\n",
    "        self.dx=dx\n",
    "        self.dy=dy\n",
    "        self.dic_lines=possible_lines_dic(self.n,self.m,self.dx,self.dy)\n",
    "        self.line_list=len(self.dic_lines)*[0]\n",
    "        self.break_flag=break_flag\n",
    "        self.coord=[[2,2,0]]    \n",
    "        self.elcon=[]  \n",
    "        self.el_dic={(2,2):0}\n",
    "        self.max_el=0\n",
    "        self.force=force\n",
    "        self.bc_node = []\n",
    "        self.bc_val = []\n",
    "        self.global_force = [0, self.force, 0, 0, 0, 0]\n",
    "        self.old_weight=float(\"inf\")\n",
    "        self.old_strength=-float(\"inf\")\n",
    "        self.visit_list = [0,0,0,0] # number of checkpoints is 4\n",
    "    \n",
    "    def reset(self,break_flag,force):\n",
    "        self.dic_lines=possible_lines_dic(self.n, self.m, self.dx, self.dy)\n",
    "        self.line_list=len(self.dic_lines)*[0]\n",
    "        self.break_flag=break_flag\n",
    "        self.coord=[[2,2,0]]    \n",
    "        self.elcon=[]  \n",
    "        self.el_dic={(2,2):0}\n",
    "        self.max_el=0\n",
    "        self.force=force\n",
    "        self.bc_node = []\n",
    "        self.bc_val = []\n",
    "        self.global_force = [0, self.force, 0, 0, 0, 0]\n",
    "        self.visit_list = [0,0,0,0] # number of checkpoints is 4\n",
    "    \n",
    "    def FEA(self):\n",
    "        return FEA_u(self.coord, \n",
    "                     self.elcon, \n",
    "                     self.bc_node, \n",
    "                     self.bc_val, \n",
    "                     self.global_force, )\n",
    "        \n",
    "    def max_u(self, FEA_output_arr):\n",
    "        t=1\n",
    "        A=[]\n",
    "        while t<len(FEA_output_arr):\n",
    "            A.append(FEA_output_arr[t])\n",
    "            t+=6            \n",
    "        return min(A)    \n",
    "            \n",
    "    \n",
    "    def length(self):\n",
    "        return total_length(self.coord,self.elcon)\n",
    "    \n",
    "    \n",
    "    def move_w(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x-self.dx\n",
    "        y_new=y\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "            \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "            \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1\n",
    "            \n",
    "        return x_new, y_new\n",
    "            \n",
    "    def move_nw(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x-self.dx\n",
    "        y_new=y+self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "            \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]]) \n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1    \n",
    "                \n",
    "        return x_new, y_new\n",
    "    \n",
    "    def move_n(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x\n",
    "        y_new=y+self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "            \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                \n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "                  \n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1     \n",
    "        \n",
    "        return x_new, y_new\n",
    "    \n",
    "    \n",
    "    def move_ne(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x+self.dx\n",
    "        y_new=y+self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "                        \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1     \n",
    "                  \n",
    "        return x_new, y_new\n",
    "    \n",
    "    def move_e(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x+self.dx\n",
    "        y_new=y\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "                       \n",
    "                  \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])   \n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1      \n",
    "        return x_new, y_new\n",
    "    \n",
    "    def move_se(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x+self.dx\n",
    "        y_new=y-self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "            \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1      \n",
    "        \n",
    "        return x_new, y_new\n",
    "    \n",
    "    def move_s(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x\n",
    "        y_new=y-self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True \n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "            \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1      \n",
    "        \n",
    "        return x_new, y_new\n",
    "    \n",
    "    def move_sw(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x-self.dx\n",
    "        y_new=y-self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "                        \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "                  \n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1    \n",
    "                                  \n",
    "        return x_new, y_new\n",
    "    \n",
    "        \n",
    "    def action_space(self,action,x0,y0):\n",
    "        if action==0:\n",
    "            return self.move_w(x0,y0)\n",
    "        elif action==1:    \n",
    "            return self.move_nw(x0,y0)\n",
    "        elif action==2:  \n",
    "            return self.move_n(x0,y0)\n",
    "        elif action==3:\n",
    "            return self.move_ne(x0,y0)\n",
    "        elif action==4:\n",
    "            return self.move_e(x0,y0)\n",
    "        elif action==5:\n",
    "            return self.move_se(x0,y0)\n",
    "        elif action==6:\n",
    "            return self.move_s(x0,y0)\n",
    "        elif action==7:\n",
    "            return self.move_sw(x0,y0)\n",
    "                        \n",
    "    \n",
    "    def nn_input(self,x,y,action):  \n",
    "        return self.line_list+[x,y]+[action]        \n",
    "    \n",
    "    def reward_(self,x_new,y_new,n_steps):\n",
    "        reward=2*n_steps\n",
    "        if all([x>=1 for x in self.visit_list]):\n",
    "            reward+=10000\n",
    "            weight=self.length()\n",
    "        \n",
    "            FEA_output_arr=self.FEA()\n",
    "            max_=self.max_u(FEA_output_arr)\n",
    "            strength=max_\n",
    "            if weight<=self.old_weight:\n",
    "                reward+=50000\n",
    "                self.old_weight=weight\n",
    "            if strength>=self.old_strength: \n",
    "                reward+=100000000\n",
    "                self.old_strength=strength        \n",
    "            self.break_flag=True     \n",
    "            return reward \n",
    "        return reward     \n",
    "                                   \n",
    "    def draw(self,color):\n",
    "        c=self.coord\n",
    "        e=self.elcon\n",
    "        c=np.array(c)\n",
    "        e=np.array(e)\n",
    "        coord=c.reshape(np.max(e)+1,3)\n",
    "        fig=plt.figure(figsize=(13,5))\n",
    "        for item in e:\n",
    "            ax = fig.gca(projection='3d') \n",
    "            ax.plot([coord[item[0]][0],coord[item[1]][0]],\\\n",
    "                     [coord[item[0]][1],coord[item[1]][1]],\\\n",
    "                     [coord[item[0]][2],coord[item[1]][2]],\n",
    "                     color=color) \n",
    "        ax.view_init(-90,90)\n",
    "        ax.set_xlim([0, 5])\n",
    "        ax.set_ylim([0, 5])\n",
    "        plt.show()             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DISCRETE_ACTIONS=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BionicEnv(gym.Env):\n",
    "    \n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n",
    "        self.M=Model()\n",
    "        self.M.reset(False,-500)\n",
    "        self.x0=2\n",
    "        self.y0=2\n",
    "        self.obs=self.M.nn_input(self.x0,self.y0, 0)\n",
    "        self.observation_space = spaces.Box(low=np.array([-np.inf for x in range(DIM)]),\n",
    "                                            high=np.array([np.inf for y in range(DIM)]),\n",
    "                                            shape=(DIM,),\n",
    "                                           dtype=np.float64)\n",
    "        self.step_=0\n",
    "        self.needs_reset = True\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        x_new, y_new = self.x0, self.y0 \n",
    "        x_new, y_new = self.M.action_space(action, x_new, y_new)\n",
    "        self.obs=self.M.nn_input(x_new,y_new,action)\n",
    "                \n",
    "        self.step_+=1           \n",
    "        reward=self.M.reward_(x_new,y_new,self.step_)\n",
    "        self.x0,self.y0 = x_new,y_new\n",
    "        \n",
    "        done=False\n",
    "        if self.M.break_flag:\n",
    "            reward-=10000\n",
    "            done=True\n",
    "        \n",
    "        if self.needs_reset:\n",
    "            raise RuntimeError(\"Tried to step environment that needs reset\")\n",
    "            \n",
    "        if done:\n",
    "            self.needs_reset = True\n",
    "      \n",
    "        return np.array(self.obs), reward, done, dict()\n",
    "\n",
    "    def reset(self):\n",
    "        self.M.reset(False,-500)\n",
    "        self.x0=2\n",
    "        self.y0=2\n",
    "        self.obs=self.M.nn_input(self.x0,self.y0,0)\n",
    "        self.step_=0\n",
    "        self.needs_reset = False\n",
    "        return np.array(self.obs)  \n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        self.M.draw('blue')    \n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(\n",
    "                        f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\"\n",
    "                    )\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model to {self.save_path}.zip\")\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log dir\n",
    "log_dir = \"/tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = BionicEnv()\n",
    "# Logs will be saved in log_dir/monitor.csv\n",
    "env = Monitor(env, log_dir)\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: -9219.24\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -9219.24 - Last mean reward per episode: -9779.66\n",
      "Num timesteps: 3000\n",
      "Best mean reward: -9219.24 - Last mean reward per episode: -9104.96\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 4000\n",
      "Best mean reward: -9104.96 - Last mean reward per episode: 990945.10\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 5000\n",
      "Best mean reward: 990945.10 - Last mean reward per episode: 991532.54\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 6000\n",
      "Best mean reward: 991532.54 - Last mean reward per episode: -8615.28\n",
      "Num timesteps: 7000\n",
      "Best mean reward: 991532.54 - Last mean reward per episode: -7843.10\n",
      "Num timesteps: 8000\n",
      "Best mean reward: 991532.54 - Last mean reward per episode: -8165.62\n",
      "Num timesteps: 9000\n",
      "Best mean reward: 991532.54 - Last mean reward per episode: -8179.70\n",
      "Num timesteps: 10000\n",
      "Best mean reward: 991532.54 - Last mean reward per episode: -6854.82\n",
      "Num timesteps: 11000\n",
      "Best mean reward: 991532.54 - Last mean reward per episode: -7646.40\n",
      "Num timesteps: 12000\n",
      "Best mean reward: 991532.54 - Last mean reward per episode: 993443.68\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 13000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: 993288.76\n",
      "Num timesteps: 14000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8319.98\n",
      "Num timesteps: 15000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -7728.06\n",
      "Num timesteps: 16000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -6626.86\n",
      "Num timesteps: 17000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -7983.06\n",
      "Num timesteps: 18000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8812.76\n",
      "Num timesteps: 19000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8790.14\n",
      "Num timesteps: 20000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8397.18\n",
      "Num timesteps: 21000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8175.96\n",
      "Num timesteps: 22000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8253.02\n",
      "Num timesteps: 23000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8251.20\n",
      "Num timesteps: 24000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9049.60\n",
      "Num timesteps: 25000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8881.10\n",
      "Num timesteps: 26000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8865.56\n",
      "Num timesteps: 27000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8819.28\n",
      "Num timesteps: 28000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8741.30\n",
      "Num timesteps: 29000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8655.06\n",
      "Num timesteps: 30000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8949.80\n",
      "Num timesteps: 31000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8920.80\n",
      "Num timesteps: 32000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8984.40\n",
      "Num timesteps: 33000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8933.32\n",
      "Num timesteps: 34000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9089.98\n",
      "Num timesteps: 35000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9164.20\n",
      "Num timesteps: 36000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9135.70\n",
      "Num timesteps: 37000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9093.96\n",
      "Num timesteps: 38000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9151.06\n",
      "Num timesteps: 39000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8808.74\n",
      "Num timesteps: 40000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8594.00\n",
      "Num timesteps: 41000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8137.18\n",
      "Num timesteps: 42000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -7803.00\n",
      "Num timesteps: 43000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8140.96\n",
      "Num timesteps: 44000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8186.72\n",
      "Num timesteps: 45000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -7787.64\n",
      "Num timesteps: 46000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8387.84\n",
      "Num timesteps: 47000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9094.54\n",
      "Num timesteps: 48000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9242.76\n",
      "Num timesteps: 49000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9130.10\n",
      "Num timesteps: 50000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9144.22\n",
      "Num timesteps: 51000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9122.88\n",
      "Num timesteps: 52000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9271.56\n",
      "Num timesteps: 53000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9289.66\n",
      "Num timesteps: 54000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9139.64\n",
      "Num timesteps: 55000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9032.14\n",
      "Num timesteps: 56000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8119.74\n",
      "Num timesteps: 57000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -7918.88\n",
      "Num timesteps: 58000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -7228.70\n",
      "Num timesteps: 59000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -7816.02\n",
      "Num timesteps: 60000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -7730.80\n",
      "Num timesteps: 61000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8340.00\n",
      "Num timesteps: 62000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8326.78\n",
      "Num timesteps: 63000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8552.40\n",
      "Num timesteps: 64000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8502.52\n",
      "Num timesteps: 65000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8995.00\n",
      "Num timesteps: 66000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9162.34\n",
      "Num timesteps: 67000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9230.28\n",
      "Num timesteps: 68000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9125.96\n",
      "Num timesteps: 69000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9047.08\n",
      "Num timesteps: 70000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8993.80\n",
      "Num timesteps: 71000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9203.30\n",
      "Num timesteps: 72000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8886.88\n",
      "Num timesteps: 73000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8778.02\n",
      "Num timesteps: 74000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8269.52\n",
      "Num timesteps: 75000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9082.08\n",
      "Num timesteps: 76000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9335.74\n",
      "Num timesteps: 77000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9283.46\n",
      "Num timesteps: 78000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8945.88\n",
      "Num timesteps: 79000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8921.78\n",
      "Num timesteps: 80000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8760.56\n",
      "Num timesteps: 81000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8679.92\n",
      "Num timesteps: 82000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8581.88\n",
      "Num timesteps: 83000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8583.28\n",
      "Num timesteps: 84000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8701.52\n",
      "Num timesteps: 85000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8699.60\n",
      "Num timesteps: 86000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8716.20\n",
      "Num timesteps: 87000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8647.08\n",
      "Num timesteps: 88000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8404.00\n",
      "Num timesteps: 89000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8122.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 90000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -7956.08\n",
      "Num timesteps: 91000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -7913.24\n",
      "Num timesteps: 92000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8141.84\n",
      "Num timesteps: 93000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8273.84\n",
      "Num timesteps: 94000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8282.90\n",
      "Num timesteps: 95000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8281.28\n",
      "Num timesteps: 96000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -7783.70\n",
      "Num timesteps: 97000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -7721.54\n",
      "Num timesteps: 98000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -7401.22\n",
      "Num timesteps: 99000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -7055.20\n",
      "Num timesteps: 100000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -7107.00\n",
      "Num timesteps: 101000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -7591.20\n",
      "Num timesteps: 102000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8650.98\n",
      "Num timesteps: 103000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9238.74\n",
      "Num timesteps: 104000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9257.56\n",
      "Num timesteps: 105000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9152.40\n",
      "Num timesteps: 106000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8829.64\n",
      "Num timesteps: 107000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8690.62\n",
      "Num timesteps: 108000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8828.36\n",
      "Num timesteps: 109000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9283.42\n",
      "Num timesteps: 110000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9251.34\n",
      "Num timesteps: 111000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9073.12\n",
      "Num timesteps: 112000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8846.78\n",
      "Num timesteps: 113000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8652.10\n",
      "Num timesteps: 114000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9395.56\n",
      "Num timesteps: 115000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9749.90\n",
      "Num timesteps: 116000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9730.38\n",
      "Num timesteps: 117000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9647.44\n",
      "Num timesteps: 118000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9330.96\n",
      "Num timesteps: 119000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9022.02\n",
      "Num timesteps: 120000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8658.90\n",
      "Num timesteps: 121000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8486.72\n",
      "Num timesteps: 122000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8373.12\n",
      "Num timesteps: 123000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8189.00\n",
      "Num timesteps: 124000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8708.76\n",
      "Num timesteps: 125000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8836.22\n",
      "Num timesteps: 126000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8738.44\n",
      "Num timesteps: 127000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8503.34\n",
      "Num timesteps: 128000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8432.30\n",
      "Num timesteps: 129000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8670.60\n",
      "Num timesteps: 130000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8853.28\n",
      "Num timesteps: 131000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9093.38\n",
      "Num timesteps: 132000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9534.88\n",
      "Num timesteps: 133000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9599.08\n",
      "Num timesteps: 134000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9448.42\n",
      "Num timesteps: 135000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9547.72\n",
      "Num timesteps: 136000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9645.34\n",
      "Num timesteps: 137000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9366.30\n",
      "Num timesteps: 138000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9095.80\n",
      "Num timesteps: 139000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8768.32\n",
      "Num timesteps: 140000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8827.14\n",
      "Num timesteps: 141000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8729.98\n",
      "Num timesteps: 142000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8837.72\n",
      "Num timesteps: 143000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8691.16\n",
      "Num timesteps: 144000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8751.92\n",
      "Num timesteps: 145000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8762.02\n",
      "Num timesteps: 146000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8798.82\n",
      "Num timesteps: 147000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8640.70\n",
      "Num timesteps: 148000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8696.88\n",
      "Num timesteps: 149000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8787.92\n",
      "Num timesteps: 150000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9061.02\n",
      "Num timesteps: 151000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8999.84\n",
      "Num timesteps: 152000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9019.44\n",
      "Num timesteps: 153000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9117.70\n",
      "Num timesteps: 154000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9288.88\n",
      "Num timesteps: 155000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9412.80\n",
      "Num timesteps: 156000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9213.04\n",
      "Num timesteps: 157000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9065.06\n",
      "Num timesteps: 158000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8900.20\n",
      "Num timesteps: 159000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9078.74\n",
      "Num timesteps: 160000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9227.98\n",
      "Num timesteps: 161000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9355.90\n",
      "Num timesteps: 162000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9316.44\n",
      "Num timesteps: 163000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9190.36\n",
      "Num timesteps: 164000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9134.14\n",
      "Num timesteps: 165000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9174.26\n",
      "Num timesteps: 166000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9241.18\n",
      "Num timesteps: 167000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9284.60\n",
      "Num timesteps: 168000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9351.66\n",
      "Num timesteps: 169000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9236.34\n",
      "Num timesteps: 170000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8804.94\n",
      "Num timesteps: 171000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8617.58\n",
      "Num timesteps: 172000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8678.78\n",
      "Num timesteps: 173000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8906.10\n",
      "Num timesteps: 174000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9018.72\n",
      "Num timesteps: 175000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8863.72\n",
      "Num timesteps: 176000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8926.60\n",
      "Num timesteps: 177000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9009.12\n",
      "Num timesteps: 178000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8865.68\n",
      "Num timesteps: 179000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8784.98\n",
      "Num timesteps: 180000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8544.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 181000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8465.76\n",
      "Num timesteps: 182000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8480.32\n",
      "Num timesteps: 183000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8552.28\n",
      "Num timesteps: 184000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8508.56\n",
      "Num timesteps: 185000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8054.06\n",
      "Num timesteps: 186000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -7867.28\n",
      "Num timesteps: 187000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -7518.62\n",
      "Num timesteps: 188000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -7503.36\n",
      "Num timesteps: 189000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8571.34\n",
      "Num timesteps: 190000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9085.14\n",
      "Num timesteps: 191000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9021.76\n",
      "Num timesteps: 192000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8856.94\n",
      "Num timesteps: 193000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9085.40\n",
      "Num timesteps: 194000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9181.86\n",
      "Num timesteps: 195000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9300.10\n",
      "Num timesteps: 196000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9529.34\n",
      "Num timesteps: 197000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9464.04\n",
      "Num timesteps: 198000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9169.76\n",
      "Num timesteps: 199000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8827.90\n",
      "Num timesteps: 200000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8654.42\n",
      "Num timesteps: 201000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8722.04\n",
      "Num timesteps: 202000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9139.36\n",
      "Num timesteps: 203000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9000.52\n",
      "Num timesteps: 204000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8829.08\n",
      "Num timesteps: 205000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8836.46\n",
      "Num timesteps: 206000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8691.22\n",
      "Num timesteps: 207000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8562.58\n",
      "Num timesteps: 208000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8496.30\n",
      "Num timesteps: 209000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9115.14\n",
      "Num timesteps: 210000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9923.56\n",
      "Num timesteps: 211000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9404.84\n",
      "Num timesteps: 212000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8986.18\n",
      "Num timesteps: 213000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8566.94\n",
      "Num timesteps: 214000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8148.22\n",
      "Num timesteps: 215000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8108.00\n",
      "Num timesteps: 216000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8395.02\n",
      "Num timesteps: 217000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8625.08\n",
      "Num timesteps: 218000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9210.24\n",
      "Num timesteps: 219000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9382.14\n",
      "Num timesteps: 220000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9210.80\n",
      "Num timesteps: 221000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8952.22\n",
      "Num timesteps: 222000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8732.44\n",
      "Num timesteps: 223000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8621.90\n",
      "Num timesteps: 224000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -8927.30\n",
      "Num timesteps: 225000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9267.48\n",
      "Num timesteps: 226000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9395.26\n",
      "Num timesteps: 227000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9360.54\n",
      "Num timesteps: 228000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9359.64\n",
      "Num timesteps: 229000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9301.44\n",
      "Num timesteps: 230000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9181.50\n",
      "Num timesteps: 231000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9096.52\n",
      "Num timesteps: 232000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9079.74\n",
      "Num timesteps: 233000\n",
      "Best mean reward: 993443.68 - Last mean reward per episode: -9172.84\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model = A2C(\"MlpPolicy\", env).learn(total_timesteps=4_000_000,callback=callback)\n",
    "end=time.time()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total time taken: {} min'.format((end - start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design by AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "while i<100:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    if env.M.break_flag:\n",
    "        break\n",
    "    i+=1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.M.length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEA_output_arr=env.M.FEA()\n",
    "env.M.max_u(FEA_output_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common import results_plotter\n",
    "\n",
    "# Helper from the library\n",
    "results_plotter.plot_results(\n",
    "    [log_dir], 1e5, results_plotter.X_TIMESTEPS, \"TD3 LunarLander\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(values, window):\n",
    "    \"\"\"\n",
    "    Smooth values by doing a moving average\n",
    "    :param values: (numpy array)\n",
    "    :param window: (int)\n",
    "    :return: (numpy array)\n",
    "    \"\"\"\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, \"valid\")\n",
    "\n",
    "\n",
    "def plot_results(log_folder, title=\"Learning Curve\"):\n",
    "    \"\"\"\n",
    "    plot the results\n",
    "\n",
    "    :param log_folder: (str) the save location of the results to plot\n",
    "    :param title: (str) the title of the task to plot\n",
    "    \"\"\"\n",
    "    x, y = ts2xy(load_results(log_folder), \"timesteps\")\n",
    "    y = moving_average(y, window=50)\n",
    "    # Truncate x\n",
    "    x = x[len(x) - len(y) :]\n",
    "\n",
    "    fig = plt.figure(title)\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(\"Number of Timesteps\")\n",
    "    plt.ylabel(\"Rewards\")\n",
    "    plt.title(title + \" Smoothed\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(log_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
