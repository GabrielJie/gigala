{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import gym\n",
    "from gym import spaces\n",
    "import time\n",
    "import random\n",
    "from stable_baselines3 import A2C, SAC,PPO\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from stable_baselines3.common import results_plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Element Model of the Space Frame Element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlaneTrussElementLength(x1,y1,z1,x2,y2,z2):\n",
    "    return math.sqrt((x2-x1)*(x2-x1) + (y2-y1)*(y2-y1)+(z2-z1)*(z2-z1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpaceFrameElementStiffness(E,G,A,Iy,Iz,J,x1,y1,z1,x2,y2,z2):\n",
    "    L = PlaneTrussElementLength(x1,y1,z1,x2,y2,z2)\n",
    "    w1 = E*A/L\n",
    "    w2 = 12*E*Iz/(L*L*L)\n",
    "    w3 = 6*E*Iz/(L*L)\n",
    "    w4 = 4*E*Iz/L\n",
    "    w5 = 2*E*Iz/L\n",
    "    w6 = 12*E*Iy/(L*L*L)\n",
    "    w7 = 6*E*Iy/(L*L)\n",
    "    w8 = 4*E*Iy/L\n",
    "    w9 = 2*E*Iy/L\n",
    "    w10 = G*J/L\n",
    "    \n",
    "    kprime = np.array([[w1, 0, 0, 0, 0, 0, -w1, 0, 0, 0, 0, 0],\n",
    "                        [0, w2, 0, 0, 0, w3, 0, -w2, 0, 0, 0, w3], \n",
    "                        [0, 0, w6, 0, -w7, 0, 0, 0, -w6, 0, -w7, 0],\n",
    "                        [0, 0, 0, w10, 0, 0, 0, 0, 0, -w10, 0, 0],\n",
    "                        [0, 0, -w7, 0, w8, 0, 0, 0, w7, 0, w9, 0],\n",
    "                        [0, w3, 0, 0, 0, w4, 0, -w3, 0, 0, 0, w5],\n",
    "                        [-w1, 0, 0, 0, 0, 0, w1, 0, 0, 0, 0, 0],\n",
    "                        [0, -w2, 0, 0, 0, -w3, 0, w2, 0, 0, 0, -w3],\n",
    "                        [0, 0, -w6, 0, w7, 0, 0, 0, w6, 0, w7, 0],\n",
    "                        [0, 0, 0, -w10, 0, 0, 0, 0, 0, w10, 0, 0],\n",
    "                        [0, 0, -w7, 0, w9, 0, 0, 0, w7, 0, w8, 0],\n",
    "                        [0, w3, 0, 0, 0, w5, 0, -w3, 0, 0, 0, w4]])  \n",
    "    \n",
    "    \n",
    "    if x1 == x2 and y1 == y2:\n",
    "        if z2 > z1:\n",
    "            Lambda = np.array([[0, 0, 1], [0, 1, 0], [-1, 0, 0]])\n",
    "        else:\n",
    "            Lambda = np.array([[0, 0, -1], [0, 1, 0], [1, 0, 0]])\n",
    "    else:\n",
    "        CXx = (x2-x1)/L\n",
    "        CYx = (y2-y1)/L\n",
    "        CZx = (z2-z1)/L\n",
    "        D = math.sqrt(CXx*CXx + CYx*CYx)\n",
    "        CXy = -CYx/D\n",
    "        CYy = CXx/D\n",
    "        CZy = 0\n",
    "        CXz = -CXx*CZx/D\n",
    "        CYz = -CYx*CZx/D\n",
    "        CZz = D\n",
    "        Lambda = np.array([[CXx, CYx, CZx], [CXy, CYy, CZy], [CXz, CYz, CZz]])\n",
    "        \n",
    "        \n",
    "    R = np.array([np.concatenate((np.concatenate((Lambda,np.zeros((3,3)),np.zeros((3,3)),np.zeros((3,3))),axis=1),\n",
    "        np.concatenate((np.zeros((3,3)), Lambda, np.zeros((3,3)), np.zeros((3,3))),axis=1) ,\n",
    "        np.concatenate((np.zeros((3,3)), np.zeros((3,3)), Lambda, np.zeros((3,3))),axis=1), \n",
    "        np.concatenate((np.zeros((3,3)), np.zeros((3,3)), np.zeros((3,3)), Lambda),axis=1)))])[0]\n",
    "    return np.dot(np.dot(R.T,kprime),R)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpaceFrameAssemble(K,k,i,j):\n",
    "    K[6*i,6*i] = K[6*i,6*i] + k[0,0]\n",
    "    K[6*i,6*i+1] = K[6*i,6*i+1] + k[0,1]\n",
    "    K[6*i,6*i+2] = K[6*i,6*i+2] + k[0,2]\n",
    "    K[6*i,6*i+3] = K[6*i,6*i+3] + k[0,3]\n",
    "    K[6*i,6*i+4] = K[6*i,6*i+4] + k[0,4]\n",
    "    K[6*i,6*i+5] = K[6*i,6*i+5] + k[0,5]\n",
    "    K[6*i,6*j] = K[6*i,6*j] + k[0,6]\n",
    "    K[6*i,6*j+1] = K[6*i,6*j+1] + k[0,7]\n",
    "    K[6*i,6*j+2] = K[6*i,6*j+2] + k[0,8]\n",
    "    K[6*i,6*j+3] = K[6*i,6*j+3] + k[0,9]\n",
    "    K[6*i,6*j+4] = K[6*i,6*j+4] + k[0,10]\n",
    "    K[6*i,6*j+5] = K[6*i,6*j+5] + k[0,11]\n",
    "    K[6*i+1,6*i] = K[6*i+1,6*i] + k[1,0]\n",
    "    K[6*i+1,6*i+1] = K[6*i+1,6*i+1] + k[1,1]\n",
    "    K[6*i+1,6*i+2] = K[6*i+1,6*i+2] + k[1,2]\n",
    "    K[6*i+1,6*i+3] = K[6*i+1,6*i+3] + k[1,3]\n",
    "    K[6*i+1,6*i+4] = K[6*i+1,6*i+4] + k[1,4]\n",
    "    K[6*i+1,6*i+5] = K[6*i+1,6*i+5] + k[1,5]\n",
    "    K[6*i+1,6*j] = K[6*i+1,6*j] + k[1,6]\n",
    "    K[6*i+1,6*j+1] = K[6*i+1,6*j+1] + k[1,7]\n",
    "    K[6*i+1,6*j+2] = K[6*i+1,6*j+2] + k[1,8]\n",
    "    K[6*i+1,6*j+3] = K[6*i+1,6*j+3] + k[1,9]\n",
    "    K[6*i+1,6*j+4] = K[6*i+1,6*j+4] + k[1,10]\n",
    "    K[6*i+1,6*j+5] = K[6*i+1,6*j+5] + k[1,11]\n",
    "    K[6*i+2,6*i]   = K[6*i+2,6*i] + k[2,0]\n",
    "    K[6*i+2,6*i+1] = K[6*i+2,6*i+1] + k[2,1]\n",
    "    K[6*i+2,6*i+2] = K[6*i+2,6*i+2] + k[2,2]\n",
    "    K[6*i+2,6*i+3] = K[6*i+2,6*i+3] + k[2,3]\n",
    "    K[6*i+2,6*i+4] = K[6*i+2,6*i+4] + k[2,4]\n",
    "    K[6*i+2,6*i+5] = K[6*i+2,6*i+5] + k[2,5]\n",
    "    K[6*i+2,6*j]   = K[6*i+2,6*j] + k[2,6]\n",
    "    K[6*i+2,6*j+1] = K[6*i+2,6*j+1] + k[2,7]\n",
    "    K[6*i+2,6*j+2] = K[6*i+2,6*j+2] + k[2,8]\n",
    "    K[6*i+2,6*j+3] = K[6*i+2,6*j+3] + k[2,9]\n",
    "    K[6*i+2,6*j+4] = K[6*i+2,6*j+4] + k[2,10]\n",
    "    K[6*i+2,6*j+5] = K[6*i+2,6*j+5] + k[2,11]\n",
    "    K[6*i+3,6*i] = K[6*i+3,6*i] + k[3,0]\n",
    "    K[6*i+3,6*i+1] = K[6*i+3,6*i+1] + k[3,1]\n",
    "    K[6*i+3,6*i+2] = K[6*i+3,6*i+2] + k[3,2]\n",
    "    K[6*i+3,6*i+3] = K[6*i+3,6*i+3] + k[3,3]\n",
    "    K[6*i+3,6*i+4] = K[6*i+3,6*i+4] + k[3,4]\n",
    "    K[6*i+3,6*i+5] = K[6*i+3,6*i+5] + k[3,5]\n",
    "    K[6*i+3,6*j] = K[6*i+3,6*j] + k[3,6]\n",
    "    K[6*i+3,6*j+1] = K[6*i+3,6*j+1] + k[3,7]\n",
    "    K[6*i+3,6*j+2] = K[6*i+3,6*j+2] + k[3,8]    \n",
    "    K[6*i+3,6*j+3] = K[6*i+3,6*j+3] + k[3,9]\n",
    "    K[6*i+3,6*j+4] = K[6*i+3,6*j+4] + k[3,10]\n",
    "    K[6*i+3,6*j+5] = K[6*i+3,6*j+5] + k[3,11]\n",
    "    K[6*i+4,6*i] = K[6*i+4,6*i] + k[4,0]\n",
    "    K[6*i+4,6*i+1] = K[6*i+4,6*i+1] + k[4,1]\n",
    "    K[6*i+4,6*i+2] = K[6*i+4,6*i+2] + k[4,2]\n",
    "    K[6*i+4,6*i+3] = K[6*i+4,6*i+3] + k[4,3]\n",
    "    K[6*i+4,6*i+4] = K[6*i+4,6*i+4] + k[4,4]\n",
    "    K[6*i+4,6*i+5] = K[6*i+4,6*i+5] + k[4,5]\n",
    "    K[6*i+4,6*j] = K[6*i+4,6*j] + k[4,6]\n",
    "    K[6*i+4,6*j+1] = K[6*i+4,6*j+1] + k[4,7]\n",
    "    K[6*i+4,6*j+2] = K[6*i+4,6*j+2] + k[4,8]\n",
    "    K[6*i+4,6*j+3] = K[6*i+4,6*j+3] + k[4,9]\n",
    "    K[6*i+4,6*j+4] = K[6*i+4,6*j+4] + k[4,10]\n",
    "    K[6*i+4,6*j+5] = K[6*i+4,6*j+5] + k[4,11]\n",
    "    K[6*i+5,6*i] = K[6*i+5,6*i] + k[5,0]\n",
    "    K[6*i+5,6*i+1] = K[6*i+5,6*i+1] + k[5,1]\n",
    "    K[6*i+5,6*i+2] = K[6*i+5,6*i+2] + k[5,2]\n",
    "    K[6*i+5,6*i+3] = K[6*i+5,6*i+3] + k[5,3]\n",
    "    K[6*i+5,6*i+4] = K[6*i+5,6*i+4] + k[5,4]\n",
    "    K[6*i+5,6*i+5] = K[6*i+5,6*i+5] + k[5,5]\n",
    "    K[6*i+5,6*j] = K[6*i+5,6*j] + k[5,6]\n",
    "    K[6*i+5,6*j+1] = K[6*i+5,6*j+1] + k[5,7]\n",
    "    K[6*i+5,6*j+2] = K[6*i+5,6*j+2] + k[5,8]\n",
    "    K[6*i+5,6*j+3] = K[6*i+5,6*j+3] + k[5,9]\n",
    "    K[6*i+5,6*j+4] = K[6*i+5,6*j+4] + k[5,10]\n",
    "    K[6*i+5,6*j+5] = K[6*i+5,6*j+5] + k[5,11]\n",
    "    K[6*j,6*i] = K[6*j,6*i] + k[6,0]\n",
    "    K[6*j,6*i+1] = K[6*j,6*i+1] + k[6,1]\n",
    "    K[6*j,6*i+2] = K[6*j,6*i+2] + k[6,2]\n",
    "    K[6*j,6*i+3] = K[6*j,6*i+3] + k[6,3]\n",
    "    K[6*j,6*i+4] = K[6*j,6*i+4] + k[6,4]\n",
    "    K[6*j,6*i+5] = K[6*j,6*i+5] + k[6,5]\n",
    "    K[6*j,6*j] = K[6*j,6*j] + k[6,6]\n",
    "    K[6*j,6*j+1] = K[6*j,6*j+1] + k[6,7]\n",
    "    K[6*j,6*j+2] = K[6*j,6*j+2] + k[6,8]\n",
    "    K[6*j,6*j+3] = K[6*j,6*j+3] + k[6,9]\n",
    "    K[6*j,6*j+4] = K[6*j,6*j+4] + k[6,10]\n",
    "    K[6*j,6*j+5] = K[6*j,6*j+5] + k[6,11]\n",
    "    K[6*j+1,6*i] = K[6*j+1,6*i] + k[7,0]\n",
    "    K[6*j+1,6*i+1] = K[6*j+1,6*i+1] + k[7,1]\n",
    "    K[6*j+1,6*i+2] = K[6*j+1,6*i+2] + k[7,2]\n",
    "    K[6*j+1,6*i+3] = K[6*j+1,6*i+3] + k[7,3]\n",
    "    K[6*j+1,6*i+4] = K[6*j+1,6*i+4] + k[7,4]\n",
    "    K[6*j+1,6*i+5] = K[6*j+1,6*i+5] + k[7,5]\n",
    "    K[6*j+1,6*j] = K[6*j+1,6*j] + k[7,6]\n",
    "    K[6*j+1,6*j+1] = K[6*j+1,6*j+1] + k[7,7]\n",
    "    K[6*j+1,6*j+2] = K[6*j+1,6*j+2] + k[7,8]\n",
    "    K[6*j+1,6*j+3] = K[6*j+1,6*j+3] + k[7,9]\n",
    "    K[6*j+1,6*j+4] = K[6*j+1,6*j+4] + k[7,10]\n",
    "    K[6*j+1,6*j+5] = K[6*j+1,6*j+5] + k[7,11]\n",
    "    K[6*j+2,6*i] = K[6*j+2,6*i] + k[8,0]\n",
    "    K[6*j+2,6*i+1] = K[6*j+2,6*i+1] + k[8,1]\n",
    "    K[6*j+2,6*i+2] = K[6*j+2,6*i+2] + k[8,2]\n",
    "    K[6*j+2,6*i+3] = K[6*j+2,6*i+3] + k[8,3]\n",
    "    K[6*j+2,6*i+4] = K[6*j+2,6*i+4] + k[8,4]\n",
    "    K[6*j+2,6*i+5] = K[6*j+2,6*i+5] + k[8,5]\n",
    "    K[6*j+2,6*j] = K[6*j+2,6*j] + k[8,6]\n",
    "    K[6*j+2,6*j+1] = K[6*j+2,6*j+1] + k[8,7]\n",
    "    K[6*j+2,6*j+2] = K[6*j+2,6*j+2] + k[8,8]\n",
    "    K[6*j+2,6*j+3] = K[6*j+2,6*j+3] + k[8,9]\n",
    "    K[6*j+2,6*j+4] = K[6*j+2,6*j+4] + k[8,10]\n",
    "    K[6*j+2,6*j+5] = K[6*j+2,6*j+5] + k[8,11]\n",
    "    K[6*j+3,6*i] = K[6*j+3,6*i] + k[9,0]\n",
    "    K[6*j+3,6*i+1] = K[6*j+3,6*i+1] + k[9,1]\n",
    "    K[6*j+3,6*i+2] = K[6*j+3,6*i+2] + k[9,2]\n",
    "    K[6*j+3,6*i+3] = K[6*j+3,6*i+3] + k[9,3]\n",
    "    K[6*j+3,6*i+4] = K[6*j+3,6*i+4] + k[9,4]\n",
    "    K[6*j+3,6*i+5] = K[6*j+3,6*i+5] + k[9,5]\n",
    "    K[6*j+3,6*j] = K[6*j+3,6*j] + k[9,6]\n",
    "    K[6*j+3,6*j+1] = K[6*j+3,6*j+1] + k[9,7]\n",
    "    K[6*j+3,6*j+2] = K[6*j+3,6*j+2] + k[9,8]\n",
    "    K[6*j+3,6*j+3] = K[6*j+3,6*j+3] + k[9,9]\n",
    "    K[6*j+3,6*j+4] = K[6*j+3,6*j+4] + k[9,10]\n",
    "    K[6*j+3,6*j+5] = K[6*j+3,6*j+5] + k[9,11]\n",
    "    K[6*j+4,6*i] = K[6*j+4,6*i] + k[10,0]\n",
    "    K[6*j+4,6*i+1] = K[6*j+4,6*i+1] + k[10,1]\n",
    "    K[6*j+4,6*i+2] = K[6*j+4,6*i+2] + k[10,2]\n",
    "    K[6*j+4,6*i+3] = K[6*j+4,6*i+3] + k[10,3]\n",
    "    K[6*j+4,6*i+4] = K[6*j+4,6*i+4] + k[10,4]\n",
    "    K[6*j+4,6*i+5] = K[6*j+4,6*i+5] + k[10,5]\n",
    "    K[6*j+4,6*j] = K[6*j+4,6*j] + k[10,6]\n",
    "    K[6*j+4,6*j+1] = K[6*j+4,6*j+1] + k[10,7]\n",
    "    K[6*j+4,6*j+2] = K[6*j+4,6*j+2] + k[10,8]\n",
    "    K[6*j+4,6*j+3] = K[6*j+4,6*j+3] + k[10,9]\n",
    "    K[6*j+4,6*j+4] = K[6*j+4,6*j+4] + k[10,10]\n",
    "    K[6*j+4,6*j+5] = K[6*j+4,6*j+5] + k[10,11]\n",
    "    K[6*j+5,6*i] = K[6*j+5,6*i] + k[11,0]\n",
    "    K[6*j+5,6*i+1] = K[6*j+5,6*i+1] + k[11,1]\n",
    "    K[6*j+5,6*i+2] = K[6*j+5,6*i+2] + k[11,2]\n",
    "    K[6*j+5,6*i+3] = K[6*j+5,6*i+3] + k[11,3]\n",
    "    K[6*j+5,6*i+4] = K[6*j+5,6*i+4] + k[11,4]\n",
    "    K[6*j+5,6*i+5] = K[6*j+5,6*i+5] + k[11,5]\n",
    "    K[6*j+5,6*j] = K[6*j+5,6*j] + k[11,6]\n",
    "    K[6*j+5,6*j+1] = K[6*j+5,6*j+1] + k[11,7]\n",
    "    K[6*j+5,6*j+2] = K[6*j+5,6*j+2] + k[11,8]\n",
    "    K[6*j+5,6*j+3] = K[6*j+5,6*j+3] + k[11,9]\n",
    "    K[6*j+5,6*j+4] = K[6*j+5,6*j+4] + k[11,10]\n",
    "    K[6*j+5,6*j+5] = K[6*j+5,6*j+5] + k[11,11]\n",
    "    \n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FEA_u(coord, elcon, bc_node, bc_val, global_force, \n",
    "          E=210e6, G=84e6, A=2e-2, Iy=10e-5, Iz=20e-5, J=5e-5):\n",
    "    coord=np.array(coord)\n",
    "    elcon=np.array(elcon)\n",
    "    \n",
    "    K=np.zeros(shape=(6*(np.max(elcon)+1),6*(np.max(elcon)+1)))\n",
    "    for el in elcon:\n",
    "        k=SpaceFrameElementStiffness(E,G,A,Iy,Iz,J,\n",
    "                                     coord[el[0]][0],coord[el[0]][1],coord[el[0]][2],\\\n",
    "                                     coord[el[1]][0],coord[el[1]][1],coord[el[1]][2])\n",
    "        K=SpaceFrameAssemble(K,k,el[0],el[1])\n",
    "        \n",
    "    F = np.array(global_force)\n",
    "    \n",
    "    \n",
    "    # https://github.com/CALFEM/calfem-matlab/blob/master/fem/solveq.m\n",
    "    \n",
    "    bc=np.array([bc_node, \n",
    "                bc_val]).T\n",
    "    nd, nd=K.shape\n",
    "    fdof=np.array([i for i in range(nd)]).T\n",
    "    d=np.zeros(shape=(len(fdof),))\n",
    "    Q=np.zeros(shape=(len(fdof),))\n",
    "\n",
    "    pdof=bc[:,0].astype(int)\n",
    "    dp=bc[:,1]\n",
    "    fdof=np.delete(fdof, pdof, 0)\n",
    "    s=np.linalg.lstsq(K[fdof,:][:,fdof], (F[fdof].T-np.dot(K[fdof,:][:,pdof],dp.T)).T, rcond=None)[0] \n",
    "    d[pdof]=dp\n",
    "    d[fdof]=s.reshape(-1,)\n",
    "    \n",
    "#     Q=np.dot(K,d).T-F \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 The Space Frame Element - verification\n",
    "d=FEA_u(np.array([0,0,0,\n",
    "                  3,0,0,\n",
    "                  0,0,-3,\n",
    "                  0,-4,0]).reshape(4,3),\n",
    "        elcon=np.array([[0, 1],\n",
    "                      [0, 2],\n",
    "                      [0, 3]]),\n",
    "        bc_node=list(range(6,24)), \n",
    "        bc_val=[0]*18,\n",
    "        global_force=[-10,0,20,0,0,0,\n",
    "                                0,0,0,0,0,0,\n",
    "                                0,0,0,0,0,0,\n",
    "                                0,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.05147750e-06, -6.65367100e-08,  1.41769582e-05,  1.44778793e-06,\n",
       "        1.74858422e-06,  1.13605431e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_length(coord,elcon):\n",
    "    coord=np.array(coord)\n",
    "    elcon=np.array(elcon)\n",
    "    t_length=0\n",
    "    for i in range(len(elcon)):\n",
    "        l=PlaneTrussElementLength(coord[elcon[i][0]][0],\\\n",
    "                                    coord[elcon[i][0]][1],\\\n",
    "                                    coord[elcon[i][0]][2],\\\n",
    "                                    coord[elcon[i][1]][0],\\\n",
    "                                    coord[elcon[i][1]][1],\\\n",
    "                                    coord[elcon[i][1]][2])\n",
    "        t_length+=l        \n",
    "    return t_length    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possible_lines_dic(n,m,dx,dy):\n",
    "    A=[(-dx,0),(-dx,dy),(0,dy),(dx,dy),(dx,0),(dx,-dy),(0,-dy),(-dx,-dy)]\n",
    "    dic={}\n",
    "    t=0\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            for item in A:\n",
    "                x,y=j*dx,i*dy\n",
    "                x1,y1=x+item[0],y+item[1]\n",
    "                if (x1>=0 and x1<=(m-1)*dx and \n",
    "                    y1>=0 and y1<=(n-1)*dy and \n",
    "                    (x1,y1,x,y) not in dic):\n",
    "                    dic[(x,y,x1,y1)]=t\n",
    "                    t+=1\n",
    "    return dic                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = len(possible_lines_dic(n=5,m=5,dx=1,dy=1)) + 3 # +2 for x and y +1 for action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,n=5,m=5,dx=1,dy=1, force=-500,\n",
    "                 E=210e6, G=84e6, A=2e-2, Iy=10e-5, Iz=20e-5, J=5e-5, break_flag=False):\n",
    "        # n,m,dx,dy - grid parameters    \n",
    "        self.E=E\n",
    "        self.G=G\n",
    "        self.A=A\n",
    "        self.Iy=Iy\n",
    "        self.Iz=Iz\n",
    "        self.J=J\n",
    "        self.n=n\n",
    "        self.m=m\n",
    "        self.dx=dx\n",
    "        self.dy=dy\n",
    "        self.dic_lines=possible_lines_dic(self.n,self.m,self.dx,self.dy)\n",
    "        self.line_list=len(self.dic_lines)*[0]\n",
    "        self.break_flag=break_flag\n",
    "        self.coord=[[2,2,0]]    \n",
    "        self.elcon=[]  \n",
    "        self.el_dic={(2,2):0}\n",
    "        self.max_el=0\n",
    "        self.force=force\n",
    "        self.bc_node = []\n",
    "        self.bc_val = []\n",
    "        self.global_force = [0, self.force, 0, 0, 0, 0]\n",
    "        self.old_weight=float(\"inf\")\n",
    "        self.old_strength=-float(\"inf\")\n",
    "        self.visit_list = [0,0,0,0] # number of checkpoints is 4\n",
    "    \n",
    "    def reset(self,break_flag,force):\n",
    "        self.dic_lines=possible_lines_dic(self.n, self.m, self.dx, self.dy)\n",
    "        self.line_list=len(self.dic_lines)*[0]\n",
    "        self.break_flag=break_flag\n",
    "        self.coord=[[2,2,0]]    \n",
    "        self.elcon=[]  \n",
    "        self.el_dic={(2,2):0}\n",
    "        self.max_el=0\n",
    "        self.force=force\n",
    "        self.bc_node = []\n",
    "        self.bc_val = []\n",
    "        self.global_force = [0, self.force, 0, 0, 0, 0]\n",
    "        self.visit_list = [0,0,0,0] # number of checkpoints is 4\n",
    "    \n",
    "    def FEA(self):\n",
    "        return FEA_u(self.coord, \n",
    "                     self.elcon, \n",
    "                     self.bc_node, \n",
    "                     self.bc_val, \n",
    "                     self.global_force, )\n",
    "        \n",
    "    def max_u(self, FEA_output_arr):\n",
    "        t=1\n",
    "        A=[]\n",
    "        while t<len(FEA_output_arr):\n",
    "            A.append(FEA_output_arr[t])\n",
    "            t+=6            \n",
    "        return min(A)    \n",
    "            \n",
    "    \n",
    "    def length(self):\n",
    "        return total_length(self.coord,self.elcon)\n",
    "    \n",
    "    \n",
    "    def move_w(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x-self.dx\n",
    "        y_new=y\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "            \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "            \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1\n",
    "            \n",
    "        return x_new, y_new\n",
    "            \n",
    "    def move_nw(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x-self.dx\n",
    "        y_new=y+self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "            \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]]) \n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1    \n",
    "                \n",
    "        return x_new, y_new\n",
    "    \n",
    "    def move_n(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x\n",
    "        y_new=y+self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "            \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                \n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "                  \n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1     \n",
    "        \n",
    "        return x_new, y_new\n",
    "    \n",
    "    \n",
    "    def move_ne(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x+self.dx\n",
    "        y_new=y+self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "                        \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1     \n",
    "                  \n",
    "        return x_new, y_new\n",
    "    \n",
    "    def move_e(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x+self.dx\n",
    "        y_new=y\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "                       \n",
    "                  \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])   \n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1      \n",
    "        return x_new, y_new\n",
    "    \n",
    "    def move_se(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x+self.dx\n",
    "        y_new=y-self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "            \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1      \n",
    "        \n",
    "        return x_new, y_new\n",
    "    \n",
    "    def move_s(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x\n",
    "        y_new=y-self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True \n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "            \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1      \n",
    "        \n",
    "        return x_new, y_new\n",
    "    \n",
    "    def move_sw(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x-self.dx\n",
    "        y_new=y-self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "                        \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "                  \n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1    \n",
    "                                  \n",
    "        return x_new, y_new\n",
    "    \n",
    "        \n",
    "    def action_space(self,action,x0,y0):\n",
    "        if action==0:\n",
    "            return self.move_w(x0,y0)\n",
    "        elif action==1:    \n",
    "            return self.move_nw(x0,y0)\n",
    "        elif action==2:  \n",
    "            return self.move_n(x0,y0)\n",
    "        elif action==3:\n",
    "            return self.move_ne(x0,y0)\n",
    "        elif action==4:\n",
    "            return self.move_e(x0,y0)\n",
    "        elif action==5:\n",
    "            return self.move_se(x0,y0)\n",
    "        elif action==6:\n",
    "            return self.move_s(x0,y0)\n",
    "        elif action==7:\n",
    "            return self.move_sw(x0,y0)\n",
    "                        \n",
    "    \n",
    "    def nn_input(self,x,y,action):  \n",
    "        return self.line_list+[x,y]+[action]        \n",
    "    \n",
    "    def reward_(self,x_new,y_new,n_steps):\n",
    "        reward=2*n_steps\n",
    "        if all([x>=1 for x in self.visit_list]):\n",
    "            reward+=100\n",
    "            weight=self.length()\n",
    "        \n",
    "            FEA_output_arr=self.FEA()\n",
    "            max_=self.max_u(FEA_output_arr)\n",
    "            strength=max_\n",
    "            if weight<=self.old_weight:\n",
    "                reward+=500\n",
    "                self.old_weight=weight\n",
    "            if strength>=self.old_strength: \n",
    "                reward+=1000\n",
    "                self.old_strength=strength        \n",
    "            self.break_flag=True     \n",
    "            return reward \n",
    "        return reward     \n",
    "                                   \n",
    "    def draw(self,color):\n",
    "        c=self.coord\n",
    "        e=self.elcon\n",
    "        c=np.array(c)\n",
    "        e=np.array(e)\n",
    "        coord=c.reshape(np.max(e)+1,3)\n",
    "        fig=plt.figure(figsize=(13,5))\n",
    "        for item in e:\n",
    "            ax = fig.gca(projection='3d') \n",
    "            ax.plot([coord[item[0]][0],coord[item[1]][0]],\\\n",
    "                     [coord[item[0]][1],coord[item[1]][1]],\\\n",
    "                     [coord[item[0]][2],coord[item[1]][2]],\n",
    "                     color=color) \n",
    "        ax.view_init(-90,90)\n",
    "        ax.set_xlim([0, 5])\n",
    "        ax.set_ylim([0, 5])\n",
    "        plt.show()             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DISCRETE_ACTIONS=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BionicEnv(gym.Env):\n",
    "    \n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n",
    "        self.M=Model()\n",
    "        self.M.reset(False,-500)\n",
    "        self.x0=2\n",
    "        self.y0=2\n",
    "        self.obs=self.M.nn_input(self.x0,self.y0, 0)\n",
    "        self.observation_space = spaces.Box(low=np.array([-1e10 for x in range(DIM)]),\n",
    "                                            high=np.array([1e10 for y in range(DIM)]),\n",
    "                                            shape=(DIM,),\n",
    "                                           dtype=np.int64)\n",
    "        self.step_=0\n",
    "        self.needs_reset = True\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        x_new, y_new = self.x0, self.y0 \n",
    "        x_new, y_new = self.M.action_space(action, x_new, y_new)\n",
    "        self.obs=self.M.nn_input(x_new,y_new,action)\n",
    "                \n",
    "        self.step_+=1           \n",
    "        reward=self.M.reward_(x_new,y_new,self.step_)\n",
    "        self.x0,self.y0 = x_new,y_new\n",
    "        \n",
    "        done=False\n",
    "        if self.M.break_flag:\n",
    "            reward-=100\n",
    "            done=True\n",
    "        \n",
    "        if self.needs_reset:\n",
    "            raise RuntimeError(\"Tried to step environment that needs reset\")\n",
    "            \n",
    "        if done:\n",
    "            self.needs_reset = True\n",
    "      \n",
    "        return np.array(self.obs), reward, done, dict()\n",
    "\n",
    "    def reset(self):\n",
    "        self.M.reset(False,-500)\n",
    "        self.x0=2\n",
    "        self.y0=2\n",
    "        self.obs=self.M.nn_input(self.x0,self.y0,0)\n",
    "        self.step_=0\n",
    "        self.needs_reset = False\n",
    "        return np.array(self.obs)  \n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        self.M.draw('blue')    \n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(\n",
    "                        f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\"\n",
    "                    )\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model to {self.save_path}.zip\")\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log dir\n",
    "log_dir = \"/tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = BionicEnv()\n",
    "# Logs will be saved in log_dir/monitor.csv\n",
    "env = Monitor(env, log_dir)\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=5e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: -32.22\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -32.22 - Last mean reward per episode: -32.48\n",
      "Num timesteps: 3000\n",
      "Best mean reward: -32.22 - Last mean reward per episode: 16.66\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 4000\n",
      "Best mean reward: 16.66 - Last mean reward per episode: -9.54\n",
      "Num timesteps: 5000\n",
      "Best mean reward: 16.66 - Last mean reward per episode: -7.30\n",
      "Num timesteps: 6000\n",
      "Best mean reward: 16.66 - Last mean reward per episode: 32.16\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 7000\n",
      "Best mean reward: 32.16 - Last mean reward per episode: 55.44\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 8000\n",
      "Best mean reward: 55.44 - Last mean reward per episode: 62.32\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 9000\n",
      "Best mean reward: 62.32 - Last mean reward per episode: 80.98\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 10000\n",
      "Best mean reward: 80.98 - Last mean reward per episode: 80.16\n",
      "Num timesteps: 11000\n",
      "Best mean reward: 80.98 - Last mean reward per episode: 108.06\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 12000\n",
      "Best mean reward: 108.06 - Last mean reward per episode: 123.78\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 13000\n",
      "Best mean reward: 123.78 - Last mean reward per episode: 132.82\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 14000\n",
      "Best mean reward: 132.82 - Last mean reward per episode: 110.08\n",
      "Num timesteps: 15000\n",
      "Best mean reward: 132.82 - Last mean reward per episode: 127.92\n",
      "Num timesteps: 16000\n",
      "Best mean reward: 132.82 - Last mean reward per episode: 109.10\n",
      "Num timesteps: 17000\n",
      "Best mean reward: 132.82 - Last mean reward per episode: 128.82\n",
      "Num timesteps: 18000\n",
      "Best mean reward: 132.82 - Last mean reward per episode: 163.06\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 19000\n",
      "Best mean reward: 163.06 - Last mean reward per episode: 179.38\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 20000\n",
      "Best mean reward: 179.38 - Last mean reward per episode: 205.02\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 21000\n",
      "Best mean reward: 205.02 - Last mean reward per episode: 185.20\n",
      "Num timesteps: 22000\n",
      "Best mean reward: 205.02 - Last mean reward per episode: 198.56\n",
      "Num timesteps: 23000\n",
      "Best mean reward: 205.02 - Last mean reward per episode: 164.20\n",
      "Num timesteps: 24000\n",
      "Best mean reward: 205.02 - Last mean reward per episode: 147.22\n",
      "Num timesteps: 25000\n",
      "Best mean reward: 205.02 - Last mean reward per episode: 186.84\n",
      "Num timesteps: 26000\n",
      "Best mean reward: 205.02 - Last mean reward per episode: 176.64\n",
      "Num timesteps: 27000\n",
      "Best mean reward: 205.02 - Last mean reward per episode: 171.38\n",
      "Num timesteps: 28000\n",
      "Best mean reward: 205.02 - Last mean reward per episode: 214.62\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 29000\n",
      "Best mean reward: 214.62 - Last mean reward per episode: 193.86\n",
      "Num timesteps: 30000\n",
      "Best mean reward: 214.62 - Last mean reward per episode: 167.90\n",
      "Num timesteps: 31000\n",
      "Best mean reward: 214.62 - Last mean reward per episode: 216.00\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 32000\n",
      "Best mean reward: 216.00 - Last mean reward per episode: 258.02\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 33000\n",
      "Best mean reward: 258.02 - Last mean reward per episode: 234.08\n",
      "Num timesteps: 34000\n",
      "Best mean reward: 258.02 - Last mean reward per episode: 231.84\n",
      "Num timesteps: 35000\n",
      "Best mean reward: 258.02 - Last mean reward per episode: 237.48\n",
      "Num timesteps: 36000\n",
      "Best mean reward: 258.02 - Last mean reward per episode: 261.94\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 37000\n",
      "Best mean reward: 261.94 - Last mean reward per episode: 247.52\n",
      "Num timesteps: 38000\n",
      "Best mean reward: 261.94 - Last mean reward per episode: 229.06\n",
      "Num timesteps: 39000\n",
      "Best mean reward: 261.94 - Last mean reward per episode: 226.42\n",
      "Num timesteps: 40000\n",
      "Best mean reward: 261.94 - Last mean reward per episode: 234.26\n",
      "Num timesteps: 41000\n",
      "Best mean reward: 261.94 - Last mean reward per episode: 254.48\n",
      "Num timesteps: 42000\n",
      "Best mean reward: 261.94 - Last mean reward per episode: 288.92\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 43000\n",
      "Best mean reward: 288.92 - Last mean reward per episode: 261.40\n",
      "Num timesteps: 44000\n",
      "Best mean reward: 288.92 - Last mean reward per episode: 247.24\n",
      "Num timesteps: 45000\n",
      "Best mean reward: 288.92 - Last mean reward per episode: 251.04\n",
      "Num timesteps: 46000\n",
      "Best mean reward: 288.92 - Last mean reward per episode: 266.96\n",
      "Num timesteps: 47000\n",
      "Best mean reward: 288.92 - Last mean reward per episode: 240.90\n",
      "Num timesteps: 48000\n",
      "Best mean reward: 288.92 - Last mean reward per episode: 255.32\n",
      "Num timesteps: 49000\n",
      "Best mean reward: 288.92 - Last mean reward per episode: 278.56\n",
      "Num timesteps: 50000\n",
      "Best mean reward: 288.92 - Last mean reward per episode: 291.52\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 51000\n",
      "Best mean reward: 291.52 - Last mean reward per episode: 289.46\n",
      "Num timesteps: 52000\n",
      "Best mean reward: 291.52 - Last mean reward per episode: 299.22\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 53000\n",
      "Best mean reward: 299.22 - Last mean reward per episode: 343.14\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 54000\n",
      "Best mean reward: 343.14 - Last mean reward per episode: 290.02\n",
      "Num timesteps: 55000\n",
      "Best mean reward: 343.14 - Last mean reward per episode: 269.22\n",
      "Num timesteps: 56000\n",
      "Best mean reward: 343.14 - Last mean reward per episode: 288.02\n",
      "Num timesteps: 57000\n",
      "Best mean reward: 343.14 - Last mean reward per episode: 277.70\n",
      "Num timesteps: 58000\n",
      "Best mean reward: 343.14 - Last mean reward per episode: 273.00\n",
      "Num timesteps: 59000\n",
      "Best mean reward: 343.14 - Last mean reward per episode: 294.12\n",
      "Num timesteps: 60000\n",
      "Best mean reward: 343.14 - Last mean reward per episode: 289.88\n",
      "Num timesteps: 61000\n",
      "Best mean reward: 343.14 - Last mean reward per episode: 283.42\n",
      "Num timesteps: 62000\n",
      "Best mean reward: 343.14 - Last mean reward per episode: 278.28\n",
      "Num timesteps: 63000\n",
      "Best mean reward: 343.14 - Last mean reward per episode: 264.00\n",
      "Num timesteps: 64000\n",
      "Best mean reward: 343.14 - Last mean reward per episode: 254.28\n",
      "Num timesteps: 65000\n",
      "Best mean reward: 343.14 - Last mean reward per episode: 312.72\n",
      "Num timesteps: 66000\n",
      "Best mean reward: 343.14 - Last mean reward per episode: 351.12\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 67000\n",
      "Best mean reward: 351.12 - Last mean reward per episode: 297.02\n",
      "Num timesteps: 68000\n",
      "Best mean reward: 351.12 - Last mean reward per episode: 340.04\n",
      "Num timesteps: 69000\n",
      "Best mean reward: 351.12 - Last mean reward per episode: 342.60\n",
      "Num timesteps: 70000\n",
      "Best mean reward: 351.12 - Last mean reward per episode: 320.32\n",
      "Num timesteps: 71000\n",
      "Best mean reward: 351.12 - Last mean reward per episode: 340.22\n",
      "Num timesteps: 72000\n",
      "Best mean reward: 351.12 - Last mean reward per episode: 360.48\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 73000\n",
      "Best mean reward: 360.48 - Last mean reward per episode: 315.30\n",
      "Num timesteps: 74000\n",
      "Best mean reward: 360.48 - Last mean reward per episode: 284.98\n",
      "Num timesteps: 75000\n",
      "Best mean reward: 360.48 - Last mean reward per episode: 352.48\n",
      "Num timesteps: 76000\n",
      "Best mean reward: 360.48 - Last mean reward per episode: 341.68\n",
      "Num timesteps: 77000\n",
      "Best mean reward: 360.48 - Last mean reward per episode: 310.62\n",
      "Num timesteps: 78000\n",
      "Best mean reward: 360.48 - Last mean reward per episode: 355.98\n",
      "Num timesteps: 79000\n",
      "Best mean reward: 360.48 - Last mean reward per episode: 369.92\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 80000\n",
      "Best mean reward: 369.92 - Last mean reward per episode: 369.84\n",
      "Num timesteps: 81000\n",
      "Best mean reward: 369.92 - Last mean reward per episode: 390.18\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 82000\n",
      "Best mean reward: 390.18 - Last mean reward per episode: 387.26\n",
      "Num timesteps: 83000\n",
      "Best mean reward: 390.18 - Last mean reward per episode: 327.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 84000\n",
      "Best mean reward: 390.18 - Last mean reward per episode: 333.50\n",
      "Num timesteps: 85000\n",
      "Best mean reward: 390.18 - Last mean reward per episode: 348.92\n",
      "Num timesteps: 86000\n",
      "Best mean reward: 390.18 - Last mean reward per episode: 349.32\n",
      "Num timesteps: 87000\n",
      "Best mean reward: 390.18 - Last mean reward per episode: 396.96\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 88000\n",
      "Best mean reward: 396.96 - Last mean reward per episode: 391.62\n",
      "Num timesteps: 89000\n",
      "Best mean reward: 396.96 - Last mean reward per episode: 351.20\n",
      "Num timesteps: 90000\n",
      "Best mean reward: 396.96 - Last mean reward per episode: 373.62\n",
      "Num timesteps: 91000\n",
      "Best mean reward: 396.96 - Last mean reward per episode: 385.54\n",
      "Num timesteps: 92000\n",
      "Best mean reward: 396.96 - Last mean reward per episode: 376.58\n",
      "Num timesteps: 93000\n",
      "Best mean reward: 396.96 - Last mean reward per episode: 427.58\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 94000\n",
      "Best mean reward: 427.58 - Last mean reward per episode: 401.54\n",
      "Num timesteps: 95000\n",
      "Best mean reward: 427.58 - Last mean reward per episode: 401.44\n",
      "Num timesteps: 96000\n",
      "Best mean reward: 427.58 - Last mean reward per episode: 396.66\n",
      "Num timesteps: 97000\n",
      "Best mean reward: 427.58 - Last mean reward per episode: 393.76\n",
      "Num timesteps: 98000\n",
      "Best mean reward: 427.58 - Last mean reward per episode: 418.12\n",
      "Num timesteps: 99000\n",
      "Best mean reward: 427.58 - Last mean reward per episode: 429.30\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 100000\n",
      "Best mean reward: 429.30 - Last mean reward per episode: 421.44\n",
      "Num timesteps: 101000\n",
      "Best mean reward: 429.30 - Last mean reward per episode: 361.24\n",
      "Num timesteps: 102000\n",
      "Best mean reward: 429.30 - Last mean reward per episode: 394.66\n",
      "Num timesteps: 103000\n",
      "Best mean reward: 429.30 - Last mean reward per episode: 525.10\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 104000\n",
      "Best mean reward: 525.10 - Last mean reward per episode: 553.76\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 105000\n",
      "Best mean reward: 553.76 - Last mean reward per episode: 523.00\n",
      "Num timesteps: 106000\n",
      "Best mean reward: 553.76 - Last mean reward per episode: 509.66\n",
      "Num timesteps: 107000\n",
      "Best mean reward: 553.76 - Last mean reward per episode: 554.74\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 108000\n",
      "Best mean reward: 554.74 - Last mean reward per episode: 564.82\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 109000\n",
      "Best mean reward: 564.82 - Last mean reward per episode: 524.72\n",
      "Num timesteps: 110000\n",
      "Best mean reward: 564.82 - Last mean reward per episode: 621.08\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 111000\n",
      "Best mean reward: 621.08 - Last mean reward per episode: 643.58\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 112000\n",
      "Best mean reward: 643.58 - Last mean reward per episode: 638.56\n",
      "Num timesteps: 113000\n",
      "Best mean reward: 643.58 - Last mean reward per episode: 619.84\n",
      "Num timesteps: 114000\n",
      "Best mean reward: 643.58 - Last mean reward per episode: 672.92\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 115000\n",
      "Best mean reward: 672.92 - Last mean reward per episode: 723.48\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 116000\n",
      "Best mean reward: 723.48 - Last mean reward per episode: 637.86\n",
      "Num timesteps: 117000\n",
      "Best mean reward: 723.48 - Last mean reward per episode: 700.84\n",
      "Num timesteps: 118000\n",
      "Best mean reward: 723.48 - Last mean reward per episode: 756.92\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 119000\n",
      "Best mean reward: 756.92 - Last mean reward per episode: 716.96\n",
      "Num timesteps: 120000\n",
      "Best mean reward: 756.92 - Last mean reward per episode: 703.88\n",
      "Num timesteps: 121000\n",
      "Best mean reward: 756.92 - Last mean reward per episode: 719.68\n",
      "Num timesteps: 122000\n",
      "Best mean reward: 756.92 - Last mean reward per episode: 738.36\n",
      "Num timesteps: 123000\n",
      "Best mean reward: 756.92 - Last mean reward per episode: 765.68\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 124000\n",
      "Best mean reward: 765.68 - Last mean reward per episode: 736.34\n",
      "Num timesteps: 125000\n",
      "Best mean reward: 765.68 - Last mean reward per episode: 818.26\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 126000\n",
      "Best mean reward: 818.26 - Last mean reward per episode: 816.16\n",
      "Num timesteps: 127000\n",
      "Best mean reward: 818.26 - Last mean reward per episode: 754.74\n",
      "Num timesteps: 128000\n",
      "Best mean reward: 818.26 - Last mean reward per episode: 855.46\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 129000\n",
      "Best mean reward: 855.46 - Last mean reward per episode: 804.74\n",
      "Num timesteps: 130000\n",
      "Best mean reward: 855.46 - Last mean reward per episode: 842.40\n",
      "Num timesteps: 131000\n",
      "Best mean reward: 855.46 - Last mean reward per episode: 772.98\n",
      "Num timesteps: 132000\n",
      "Best mean reward: 855.46 - Last mean reward per episode: 831.86\n",
      "Num timesteps: 133000\n",
      "Best mean reward: 855.46 - Last mean reward per episode: 899.46\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 134000\n",
      "Best mean reward: 899.46 - Last mean reward per episode: 922.78\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 135000\n",
      "Best mean reward: 922.78 - Last mean reward per episode: 907.30\n",
      "Num timesteps: 136000\n",
      "Best mean reward: 922.78 - Last mean reward per episode: 940.96\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 137000\n",
      "Best mean reward: 940.96 - Last mean reward per episode: 906.32\n",
      "Num timesteps: 138000\n",
      "Best mean reward: 940.96 - Last mean reward per episode: 908.12\n",
      "Num timesteps: 139000\n",
      "Best mean reward: 940.96 - Last mean reward per episode: 942.98\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 140000\n",
      "Best mean reward: 942.98 - Last mean reward per episode: 945.78\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 141000\n",
      "Best mean reward: 945.78 - Last mean reward per episode: 936.84\n",
      "Num timesteps: 142000\n",
      "Best mean reward: 945.78 - Last mean reward per episode: 878.34\n",
      "Num timesteps: 143000\n",
      "Best mean reward: 945.78 - Last mean reward per episode: 966.96\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 144000\n",
      "Best mean reward: 966.96 - Last mean reward per episode: 969.04\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 145000\n",
      "Best mean reward: 969.04 - Last mean reward per episode: 978.62\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 146000\n",
      "Best mean reward: 978.62 - Last mean reward per episode: 963.60\n",
      "Num timesteps: 147000\n",
      "Best mean reward: 978.62 - Last mean reward per episode: 973.80\n",
      "Num timesteps: 148000\n",
      "Best mean reward: 978.62 - Last mean reward per episode: 986.94\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 149000\n",
      "Best mean reward: 986.94 - Last mean reward per episode: 985.68\n",
      "Num timesteps: 150000\n",
      "Best mean reward: 986.94 - Last mean reward per episode: 964.16\n",
      "Num timesteps: 151000\n",
      "Best mean reward: 986.94 - Last mean reward per episode: 931.74\n",
      "Num timesteps: 152000\n",
      "Best mean reward: 986.94 - Last mean reward per episode: 1006.98\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 153000\n",
      "Best mean reward: 1006.98 - Last mean reward per episode: 952.56\n",
      "Num timesteps: 154000\n",
      "Best mean reward: 1006.98 - Last mean reward per episode: 950.64\n",
      "Num timesteps: 155000\n",
      "Best mean reward: 1006.98 - Last mean reward per episode: 1000.56\n",
      "Num timesteps: 156000\n",
      "Best mean reward: 1006.98 - Last mean reward per episode: 1012.62\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 157000\n",
      "Best mean reward: 1012.62 - Last mean reward per episode: 985.82\n",
      "Num timesteps: 158000\n",
      "Best mean reward: 1012.62 - Last mean reward per episode: 1004.32\n",
      "Num timesteps: 159000\n",
      "Best mean reward: 1012.62 - Last mean reward per episode: 1016.36\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 160000\n",
      "Best mean reward: 1016.36 - Last mean reward per episode: 995.68\n",
      "Num timesteps: 161000\n",
      "Best mean reward: 1016.36 - Last mean reward per episode: 992.64\n",
      "Num timesteps: 162000\n",
      "Best mean reward: 1016.36 - Last mean reward per episode: 1017.50\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 163000\n",
      "Best mean reward: 1017.50 - Last mean reward per episode: 1032.14\n",
      "Saving new best model to /tmp/gym/best_model.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 164000\n",
      "Best mean reward: 1032.14 - Last mean reward per episode: 995.54\n",
      "Num timesteps: 165000\n",
      "Best mean reward: 1032.14 - Last mean reward per episode: 1002.20\n",
      "Num timesteps: 166000\n",
      "Best mean reward: 1032.14 - Last mean reward per episode: 1010.02\n",
      "Num timesteps: 167000\n",
      "Best mean reward: 1032.14 - Last mean reward per episode: 1035.38\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 168000\n",
      "Best mean reward: 1035.38 - Last mean reward per episode: 956.72\n",
      "Num timesteps: 169000\n",
      "Best mean reward: 1035.38 - Last mean reward per episode: 1034.70\n",
      "Num timesteps: 170000\n",
      "Best mean reward: 1035.38 - Last mean reward per episode: 1034.82\n",
      "Num timesteps: 171000\n",
      "Best mean reward: 1035.38 - Last mean reward per episode: 1025.72\n",
      "Num timesteps: 172000\n",
      "Best mean reward: 1035.38 - Last mean reward per episode: 1047.14\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 173000\n",
      "Best mean reward: 1047.14 - Last mean reward per episode: 1038.70\n",
      "Num timesteps: 174000\n",
      "Best mean reward: 1047.14 - Last mean reward per episode: 1049.50\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 175000\n",
      "Best mean reward: 1049.50 - Last mean reward per episode: 1030.16\n",
      "Num timesteps: 176000\n",
      "Best mean reward: 1049.50 - Last mean reward per episode: 1048.16\n",
      "Num timesteps: 177000\n",
      "Best mean reward: 1049.50 - Last mean reward per episode: 1049.30\n",
      "Num timesteps: 178000\n",
      "Best mean reward: 1049.50 - Last mean reward per episode: 1050.94\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 179000\n",
      "Best mean reward: 1050.94 - Last mean reward per episode: 1046.84\n",
      "Num timesteps: 180000\n",
      "Best mean reward: 1050.94 - Last mean reward per episode: 1041.04\n",
      "Num timesteps: 181000\n",
      "Best mean reward: 1050.94 - Last mean reward per episode: 1037.60\n",
      "Num timesteps: 182000\n",
      "Best mean reward: 1050.94 - Last mean reward per episode: 1031.14\n",
      "Num timesteps: 183000\n",
      "Best mean reward: 1050.94 - Last mean reward per episode: 1026.68\n",
      "Num timesteps: 184000\n",
      "Best mean reward: 1050.94 - Last mean reward per episode: 1042.60\n",
      "Num timesteps: 185000\n",
      "Best mean reward: 1050.94 - Last mean reward per episode: 1049.00\n",
      "Num timesteps: 186000\n",
      "Best mean reward: 1050.94 - Last mean reward per episode: 1039.66\n",
      "Num timesteps: 187000\n",
      "Best mean reward: 1050.94 - Last mean reward per episode: 1045.52\n",
      "Num timesteps: 188000\n",
      "Best mean reward: 1050.94 - Last mean reward per episode: 1056.00\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 189000\n",
      "Best mean reward: 1056.00 - Last mean reward per episode: 1050.32\n",
      "Num timesteps: 190000\n",
      "Best mean reward: 1056.00 - Last mean reward per episode: 1040.52\n",
      "Num timesteps: 191000\n",
      "Best mean reward: 1056.00 - Last mean reward per episode: 1048.24\n",
      "Num timesteps: 192000\n",
      "Best mean reward: 1056.00 - Last mean reward per episode: 1046.84\n",
      "Num timesteps: 193000\n",
      "Best mean reward: 1056.00 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 194000\n",
      "Best mean reward: 1056.00 - Last mean reward per episode: 1038.68\n",
      "Num timesteps: 195000\n",
      "Best mean reward: 1056.00 - Last mean reward per episode: 1049.24\n",
      "Num timesteps: 196000\n",
      "Best mean reward: 1056.00 - Last mean reward per episode: 1049.96\n",
      "Num timesteps: 197000\n",
      "Best mean reward: 1056.00 - Last mean reward per episode: 1039.82\n",
      "Num timesteps: 198000\n",
      "Best mean reward: 1056.00 - Last mean reward per episode: 1046.76\n",
      "Num timesteps: 199000\n",
      "Best mean reward: 1056.00 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 200000\n",
      "Best mean reward: 1056.00 - Last mean reward per episode: 1038.82\n",
      "Num timesteps: 201000\n",
      "Best mean reward: 1056.00 - Last mean reward per episode: 1048.50\n",
      "Num timesteps: 202000\n",
      "Best mean reward: 1056.00 - Last mean reward per episode: 1056.76\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 203000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1044.86\n",
      "Num timesteps: 204000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1047.10\n",
      "Num timesteps: 205000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1048.26\n",
      "Num timesteps: 206000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1044.20\n",
      "Num timesteps: 207000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 208000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1048.94\n",
      "Num timesteps: 209000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1049.06\n",
      "Num timesteps: 210000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1047.54\n",
      "Num timesteps: 211000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1049.24\n",
      "Num timesteps: 212000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 213000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 214000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1047.92\n",
      "Num timesteps: 215000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 216000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1028.94\n",
      "Num timesteps: 217000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 218000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1029.24\n",
      "Num timesteps: 219000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1040.60\n",
      "Num timesteps: 220000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1049.50\n",
      "Num timesteps: 221000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1050.40\n",
      "Num timesteps: 222000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1040.78\n",
      "Num timesteps: 223000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1046.34\n",
      "Num timesteps: 224000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1051.46\n",
      "Num timesteps: 225000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 226000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1046.84\n",
      "Num timesteps: 227000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 228000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1031.14\n",
      "Num timesteps: 229000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 230000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1056.54\n",
      "Num timesteps: 231000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1039.06\n",
      "Num timesteps: 232000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1038.82\n",
      "Num timesteps: 233000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1041.88\n",
      "Num timesteps: 234000\n",
      "Best mean reward: 1056.76 - Last mean reward per episode: 1057.76\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 235000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 236000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1046.00\n",
      "Num timesteps: 237000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 238000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 239000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 240000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 241000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 242000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 243000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1049.80\n",
      "Num timesteps: 244000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1046.76\n",
      "Num timesteps: 245000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1028.74\n",
      "Num timesteps: 246000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 247000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1047.26\n",
      "Num timesteps: 248000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 249000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 250000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.76\n",
      "Num timesteps: 251000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1044.46\n",
      "Num timesteps: 252000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1051.46\n",
      "Num timesteps: 253000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1049.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 254000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 255000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.76\n",
      "Num timesteps: 256000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1047.52\n",
      "Num timesteps: 257000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1047.52\n",
      "Num timesteps: 258000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1055.66\n",
      "Num timesteps: 259000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1041.80\n",
      "Num timesteps: 260000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 261000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 262000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1048.24\n",
      "Num timesteps: 263000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1057.00\n",
      "Num timesteps: 264000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1047.50\n",
      "Num timesteps: 265000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1026.44\n",
      "Num timesteps: 266000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1039.04\n",
      "Num timesteps: 267000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.76\n",
      "Num timesteps: 268000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1046.76\n",
      "Num timesteps: 269000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 270000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1051.70\n",
      "Num timesteps: 271000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 272000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 273000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 274000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1038.42\n",
      "Num timesteps: 275000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 276000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1039.06\n",
      "Num timesteps: 277000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 278000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 279000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1039.04\n",
      "Num timesteps: 280000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 281000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1049.06\n",
      "Num timesteps: 282000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1047.16\n",
      "Num timesteps: 283000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1039.08\n",
      "Num timesteps: 284000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1046.76\n",
      "Num timesteps: 285000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 286000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1036.26\n",
      "Num timesteps: 287000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1046.84\n",
      "Num timesteps: 288000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1047.50\n",
      "Num timesteps: 289000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 290000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 291000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1046.34\n",
      "Num timesteps: 292000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1054.36\n",
      "Num timesteps: 293000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1046.84\n",
      "Num timesteps: 294000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1036.88\n",
      "Num timesteps: 295000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1051.46\n",
      "Num timesteps: 296000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 297000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 298000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 299000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 300000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1046.54\n",
      "Num timesteps: 301000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 302000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 303000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 304000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1046.54\n",
      "Num timesteps: 305000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 306000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1046.26\n",
      "Num timesteps: 307000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1048.62\n",
      "Num timesteps: 308000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1047.50\n",
      "Num timesteps: 309000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1046.76\n",
      "Num timesteps: 310000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1046.76\n",
      "Num timesteps: 311000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1046.84\n",
      "Num timesteps: 312000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 313000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1046.34\n",
      "Num timesteps: 314000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1047.54\n",
      "Num timesteps: 315000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 316000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 317000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 318000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 319000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1052.00\n",
      "Num timesteps: 320000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 321000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 322000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 323000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 324000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 325000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1047.00\n",
      "Num timesteps: 326000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 327000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 328000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 943.98\n",
      "Num timesteps: 329000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 735.28\n",
      "Num timesteps: 330000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 829.32\n",
      "Num timesteps: 331000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 925.32\n",
      "Num timesteps: 332000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 980.92\n",
      "Num timesteps: 333000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1047.84\n",
      "Num timesteps: 334000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1037.60\n",
      "Num timesteps: 335000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 336000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1040.00\n",
      "Num timesteps: 337000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1046.84\n",
      "Num timesteps: 338000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1047.16\n",
      "Num timesteps: 339000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1051.46\n",
      "Num timesteps: 340000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1027.44\n",
      "Num timesteps: 341000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1031.84\n",
      "Num timesteps: 342000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1047.50\n",
      "Num timesteps: 343000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1030.62\n",
      "Num timesteps: 344000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1031.20\n",
      "Num timesteps: 345000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1046.34\n",
      "Num timesteps: 346000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 347000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1039.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 348000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1026.18\n",
      "Num timesteps: 349000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1045.76\n",
      "Num timesteps: 350000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 351000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 352000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1045.16\n",
      "Num timesteps: 353000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 354000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1046.54\n",
      "Num timesteps: 355000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1048.24\n",
      "Num timesteps: 356000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 357000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 358000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1046.34\n",
      "Num timesteps: 359000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1047.54\n",
      "Num timesteps: 360000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 361000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1034.50\n",
      "Num timesteps: 362000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1046.00\n",
      "Num timesteps: 363000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1046.76\n",
      "Num timesteps: 364000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1047.50\n",
      "Num timesteps: 365000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1050.94\n",
      "Num timesteps: 366000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1045.76\n",
      "Num timesteps: 367000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 368000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1046.00\n",
      "Num timesteps: 369000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1049.50\n",
      "Num timesteps: 370000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1046.00\n",
      "Num timesteps: 371000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1044.74\n",
      "Num timesteps: 372000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 373000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 374000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 375000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 376000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1034.90\n",
      "Num timesteps: 377000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1035.56\n",
      "Num timesteps: 378000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 379000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 380000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 381000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1049.14\n",
      "Num timesteps: 382000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 383000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1037.22\n",
      "Num timesteps: 384000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1038.76\n",
      "Num timesteps: 385000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1016.10\n",
      "Num timesteps: 386000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1048.16\n",
      "Num timesteps: 387000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 388000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1047.54\n",
      "Num timesteps: 389000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1045.54\n",
      "Num timesteps: 390000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1030.80\n",
      "Num timesteps: 391000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1047.84\n",
      "Num timesteps: 392000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 393000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 394000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1041.04\n",
      "Num timesteps: 395000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1033.32\n",
      "Num timesteps: 396000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1037.18\n",
      "Num timesteps: 397000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1045.76\n",
      "Num timesteps: 398000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1047.54\n",
      "Num timesteps: 399000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1049.06\n",
      "Num timesteps: 400000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1045.16\n",
      "Num timesteps: 401000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 1056.00\n",
      "Num timesteps: 402000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 674.32\n",
      "Num timesteps: 403000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 135.28\n",
      "Num timesteps: 404000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 163.22\n",
      "Num timesteps: 405000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 181.00\n",
      "Num timesteps: 406000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 172.38\n",
      "Num timesteps: 407000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 206.62\n",
      "Num timesteps: 408000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 223.60\n",
      "Num timesteps: 409000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 235.72\n",
      "Num timesteps: 410000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 212.36\n",
      "Num timesteps: 411000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 283.04\n",
      "Num timesteps: 412000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 271.86\n",
      "Num timesteps: 413000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 243.58\n",
      "Num timesteps: 414000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 266.14\n",
      "Num timesteps: 415000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 306.38\n",
      "Num timesteps: 416000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 319.80\n",
      "Num timesteps: 417000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 325.82\n",
      "Num timesteps: 418000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 315.06\n",
      "Num timesteps: 419000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 314.58\n",
      "Num timesteps: 420000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 261.70\n",
      "Num timesteps: 421000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 266.32\n",
      "Num timesteps: 422000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 280.10\n",
      "Num timesteps: 423000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 322.58\n",
      "Num timesteps: 424000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 330.26\n",
      "Num timesteps: 425000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 309.22\n",
      "Num timesteps: 426000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 340.70\n",
      "Num timesteps: 427000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 341.00\n",
      "Num timesteps: 428000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 352.06\n",
      "Num timesteps: 429000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 340.30\n",
      "Num timesteps: 430000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 312.98\n",
      "Num timesteps: 431000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 348.22\n",
      "Num timesteps: 432000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 394.94\n",
      "Num timesteps: 433000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 399.12\n",
      "Num timesteps: 434000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 359.44\n",
      "Num timesteps: 435000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 352.96\n",
      "Num timesteps: 436000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 361.94\n",
      "Num timesteps: 437000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 341.92\n",
      "Num timesteps: 438000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 378.72\n",
      "Num timesteps: 439000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 388.96\n",
      "Num timesteps: 440000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 383.20\n",
      "Num timesteps: 441000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 369.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 442000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 356.64\n",
      "Num timesteps: 443000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 391.36\n",
      "Num timesteps: 444000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 393.96\n",
      "Num timesteps: 445000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 410.12\n",
      "Num timesteps: 446000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 416.96\n",
      "Num timesteps: 447000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 412.22\n",
      "Num timesteps: 448000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 388.82\n",
      "Num timesteps: 449000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 391.12\n",
      "Num timesteps: 450000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 387.34\n",
      "Num timesteps: 451000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 378.14\n",
      "Num timesteps: 452000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 401.58\n",
      "Num timesteps: 453000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 415.40\n",
      "Num timesteps: 454000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 385.92\n",
      "Num timesteps: 455000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 366.96\n",
      "Num timesteps: 456000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 394.44\n",
      "Num timesteps: 457000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 375.80\n",
      "Num timesteps: 458000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 372.46\n",
      "Num timesteps: 459000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 384.62\n",
      "Num timesteps: 460000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 412.80\n",
      "Num timesteps: 461000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 413.02\n",
      "Num timesteps: 462000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 411.98\n",
      "Num timesteps: 463000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 437.68\n",
      "Num timesteps: 464000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 437.76\n",
      "Num timesteps: 465000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 402.52\n",
      "Num timesteps: 466000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 401.54\n",
      "Num timesteps: 467000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 393.04\n",
      "Num timesteps: 468000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 427.90\n",
      "Num timesteps: 469000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 429.30\n",
      "Num timesteps: 470000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 442.60\n",
      "Num timesteps: 471000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 440.78\n",
      "Num timesteps: 472000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 416.68\n",
      "Num timesteps: 473000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 383.60\n",
      "Num timesteps: 474000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 391.40\n",
      "Num timesteps: 475000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 445.06\n",
      "Num timesteps: 476000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 478.86\n",
      "Num timesteps: 477000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 457.02\n",
      "Num timesteps: 478000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 420.26\n",
      "Num timesteps: 479000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 409.04\n",
      "Num timesteps: 480000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 443.34\n",
      "Num timesteps: 481000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 465.26\n",
      "Num timesteps: 482000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 447.62\n",
      "Num timesteps: 483000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 476.42\n",
      "Num timesteps: 484000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 462.14\n",
      "Num timesteps: 485000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 413.10\n",
      "Num timesteps: 486000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 432.24\n",
      "Num timesteps: 487000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 440.34\n",
      "Num timesteps: 488000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 450.76\n",
      "Num timesteps: 489000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 433.34\n",
      "Num timesteps: 490000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 467.74\n",
      "Num timesteps: 491000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 426.50\n",
      "Num timesteps: 492000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 389.50\n",
      "Num timesteps: 493000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 449.06\n",
      "Num timesteps: 494000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 490.32\n",
      "Num timesteps: 495000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 466.56\n",
      "Num timesteps: 496000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 398.54\n",
      "Num timesteps: 497000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 422.46\n",
      "Num timesteps: 498000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 433.68\n",
      "Num timesteps: 499000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 421.18\n",
      "Num timesteps: 500000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 442.90\n",
      "Num timesteps: 501000\n",
      "Best mean reward: 1057.76 - Last mean reward per episode: 468.76\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model = PPO(\"MlpPolicy\", env).learn(total_timesteps=ts, callback=callback)\n",
    "end=time.time()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 15.247849067052206 min\n"
     ]
    }
   ],
   "source": [
    "print('Total time taken: {} min'.format((end - start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design by AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "while i<100:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    if env.M.break_flag:\n",
    "        break\n",
    "    i+=1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:478: MatplotlibDeprecationWarning: Calling gca() with keyword arguments was deprecated in Matplotlib 3.4. Starting two minor releases later, gca() will take no keyword arguments. The gca() function should only be used to get the current axes, or if no axes exist, create new axes with default keyword arguments. To create a new axes with non-default arguments, use plt.axes() or plt.subplot().\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.render() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.970562748477146"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.M.length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.006417384114196716"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEA_output_arr=env.M.FEA()\n",
    "env.M.max_u(FEA_output_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAACICAYAAADqIJGqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAABQCklEQVR4nO29eZhU1Z34/fnW0tVdvUODNNAskkZEVNQ2mOiQzASFGE2YSUYSnXFJ+GWyzTCTeUkwCWaUTHTim/xCXp2JjjHRRBLNMi6E4JJJYnDpAAFkUUBQNhuhofe1uuq8f9x7bt+6fWvp7uqF9nyep5+uustZ7r11zvd+tyNKKQwGg8FgMBjGEoGRboDBYDAYDAZDrjECjsFgMBgMhjGHEXAMBoPBYDCMOYyAYzAYDAaDYcxhBByDwWAwGAxjDiPgGAwGg8FgGHMYAcdgMJwxiMg0EWkVkWAOytotIu8ffKsMBsNoxAg4BoMhLSLypoh02ILF2yLyIxEpsvf9XkQ67X31IvIrEal0nfteEflfEWkRkSYReUpE5qap62YRidvltYrIQRH5rN6vlDqslCpSSsUH2y+l1HlKqd9nOk5ElIi0udrUKiJfGmz9BoNhaDECjsFgyIZrlVJFwMVADfA1174v2PtmA2XA/wUQkfcAzwBPAJOBmcAO4AUROTtNXS/ZQkwR8FHgWyJyUY77018u1G2y/741wu0xGAwZMAKOwWDIGqXUMeA3wDyffaeBX7r2fQt4WCm1VinVopQ6rZT6GvAy8G9Z1rcNeBU4F0BEZtgalZD9fbKIPCkip0XkdRH5P/pcEfk3EXlMRB62NUi7RaTGtf9NEVlkfw6KyFdE5IB97FYRqcrUvnR1iMiXReQXnuPXisj3sum7wWAYHEbAMRgMWWNP+lcD23z2VWBpXLaJSBR4L/Bzn2IeA67Msr5LsTRDW1Ic8jPgKJaG6GPAN0Xkr1z7P2wfUwY8CdyTopwvAp/A6lsJ8EmgPZs2pqnjZ8DVIlJs9yUIXAesy7Jcg8EwCIyAYzAYsuFxEWkENgF/AL7p2vc9e98OoA5LWBiHNb7U+ZRVB1SkqesyEWkUkRbgT8CPgf3eg2xh63Lgy0qpTqXUduAB4EbXYZuUUhtsn50fAxemqHM58DWl1F5lsUMpdcq1/892m/Tf4kx1KKUOAX8G/to+7q+AdqXUy2n6bjAYcoQRcAwGQzYsVUqVKaWmK6U+p5TqcO37J3vfFKXUDUqpk0ADkAAqfcqqBOrT1PWyXV4xMAk4j2SBSjMZOK2UanFtOwRMcX0/7vrcDuRr85aHKuBAmjZdbLdJ/z2dZR3rsDRDANdjtDcGw7BhBByDwZBzlFJtwEvA3/rsvg74bZblvI3l13Otz+63gHHaBGQzDTjWv9YCcASYNYDzMvFz4P0iMhVLk2MEHINhmDACjsFgGCpWATeJyD+JSLGIlIvIN4D3ALdnU4CIjMcSDHZ79ymljgAvAneKSL6IXAB8CvjJANr6ALBGRKrF4gK77kFha7N+D/wQeEMp9epgyzQYDNlhBByDwTAkKKU2AYuBv8HyuzkEXARcoZTq41Pj4j063wxWBNVJ4B9THPsJYAaWNud/gK8rpZ4bQHO/g+X8/AzQDPwAKHDt3+HJg/PdfpS9DliE0d4YDMOKKKVGug0Gg8FgMBgMOcVocAwGg8FgMIw5+iXgiEhAREqGqjEGg8FgMBgMuSCjgCMi60SkREQKgV3AHhFZOfRNMxgMBoPBYBgY2Whw5iqlmoGlWCnaZwJ/P5SNMhgMBoPBYBgMfgmvvIRFJIwl4NyjlIqJyKj2TK6oqFAzZswY6WYYDAaDwWAYQrZu3VqvlJrgty8bAec+4E2sNOzPi8h0rDDKUcuMGTPYsiXV0jUGg8FgMBjGAiJyKNW+jAKOUup7gHv120Mi8pe5aJjBYDAYDAbDUJBSwBGRL2Y49zvpdtoL4T0MnAUo4H6l1FoRGQc8ipWc603gOqVUg4gIsBZrNd924Gal1J/tsm4CvmYX/Q2l1EMZ2mYwGAwGg+EMYOuhBtY+t48Vi2ZzyfTynJWbzsm42P6rAT6LtYDdFOAzwMVZlN0D/KtSai5wGfB5EZmLlb79t0qpaqz1aFbZx38QqLb/Pg38F4AtEH0dWAC8G/i6iOTuChgMBoPBYBgx1j63j+f317P2uX05LTelBkcpdTuAiDyPtZJui/3934BfZypYKVWHlZ4dpVSLiLyKJSB9BHi/fdhDWOu0fNne/rCyUiu/LCJlIlJpH/usUuq0Xf+zwBLgp/3rqsFgMBgMhtHGikWzk/7nimycjM8Cul3fu+1tWSMiM7DWoKkFzrKFH4DjrrKmYK3oqzlKr9bIb7u3jk9jaX6YNm1af5pnMBgMBoNhhLhkejkPf2pBzsvNRsB5GPiTiPyP/X0p8KNsKxCRIuCXwD8rpZotVxsLpZTKVci5Uup+4H6AmpqaUR3GbjAYDAaDYWhJm+jPdvx9GLgFaLD/blFK3ZlN4Xb+nF8CjyilfmVvfts2PWH/P2FvPwZUuU6fam9Ltd1gMBgMBoPBl7QCju0Ps0Ep9Wel1Fr7b1s2BdvC0Q+AV5VS7oirJ4Gb7M83AU+4tt8oFpcBTbYp62ngKhEpt52Lr7K3GQwGg8FgGGNsPdTAjT+oZeuhhkGVk42J6s8icqlSanM/y74ca0mHnSKy3d72FeAu4DER+RRwCLjO3rcBK0T8daww8VsAlFKnRWQNoOu/QzscGwwGg8FgGFvoqCpgUL452Qg4C4Ab7GyBbYBgKXcuSHeSUmqTfawfH/A5XgGfT1HWg8CDWbR1zNLfPAFDlVfAYDAYDH0xY27uyFVUVTYCzuJB1WDICf2VaHMlARsMBoMhM2bMzR25iqrKZqmGQwAiMhHIH3SNhgHRX4l2qPIKGAwGg6EvZswdfYhlGUpzgMiHgW8Dk7EinqZjOQ6fN/TNGxg1NTXKLLZpYdSmBoPBcOYzlsfywfRNRLYqpWr89qWNorJZg7XUwj6l1Ews/5mX+9UCw4gxVCmwDQaDwTB8jOWxfNiXanARU0qdEpGAiASUUr8Tke/mtBWGIWO41KZj+e3CYDAYRpqxbAIbqr5lo8FptLMRPw88IiJrsaKpRj3uWPp0cfW5irkfjWhnrUuml+ekn6nKGMtvFwaDwXAmcqbMbe55Kpdko8H5CNAB/AtwA1AK3JHTVgwRbq92IKWH+zvF+z0X/UxVxlh+uzAYDIaRZiDj9ztlbktFNhqcjwOzlFI9SqmHlFLfU0qdGuqG5YIVi2azsLqCFYtmJ31Od5ybTNKvd3+q40dKivbWm+4apDvPTbZlGAwGwzuB4RrfBzL2Dma8PlO0P+nIJorqduAvgJnAFixT1R+VUtuHvHUDJFdRVDf+oJbn99ezsLrCV/r17k91fKZyhoqB1juQ80aqjwaDwTCSjNWx70zpV7ooqmzy4HzdLqQA+D/ASuC7QDCHbRyVZDK7ePenOn6kzDcDrXcg5xkTlcFgeCcyVse+FYtmU9fYwbbDjayrPcz1C6aNdJP6TTYanK9hrStVBGwDNmFpcOqGvnkDYyzkwRmpqKRM9fanXaO1D4axi7n3hrFALp7jXJRx0R3P0NAeozgS5KJp5TldKihXv9XB5sH5G2A88BzwK+CJ0SzcjBVGKiopU739addo7YNh7GLuvWEskIvnOBdlrFw8h/JomEmlBf0qK5u6h+O3mo2J6mIRKcHS4lwJ3C8iJ5RSVwxZqwxDrvb0Ss/6+5J5lWnr9bYrnRR+ppnmDGc+5t4bRgOD1U7k4jnORRnXL5jG9QumJfUnV3UPx281GxPVPCwn4/cBNcARLBPVbUPWqkHSXxPVO1Gtna2DdH/L8eOdeH0N2WOeD8NY40xx0PXiFWSG6ne59VADa57aDSKsvmbukJmossmDcxfwR+B7wGalVGzALRmlvBNzBWTrIN3fcvx4J15fQ/aY58Mw1kg1Lo52YT7b3HFe+tuvNU/tZvvRJqfOofrdZ/TBUUpdgyXcnBqLwg3kLrfLcOcNWFd7mIvueIZ1tYczHuttmzdzZDaZJP36d8l0y/Fs7XP7UvbbfX1TZZfu77UbCzkaDBYmt5JhrJFqPM2130mux0G/3HFL5lWy9J5NLL33hZT16H4tf2hzyrxw+vu62sMcONkKQHEk1Gde8Otjqvq3HmogNG5Kdar+ZBRwRORaYDuw0f4+X0SezHTemUSu0kQPt4Pj3U+/RkN7jDs37Mn4kA+l01qmst3X131sqs+DaYvhzGOo0rQbDMNBf4SMXAvz/RkHs2mn+7eoP2/cVcf2o01sP9LovMh6y1mxaDbl0TAN7TGnLd626e+3PbGLlq445dEwP/rku/vMC359TFX/2uf2EcgrKEnVn2yiqP4NeDfQCGAn+JuZxXmjguF80x9oRuRMpDp/5eI5FEeCxBP0eTi82pG6xg6KIyGWzKsccAbmVJqY/vxo3W8FzR0x5leVpc00naptqdrS3+toNEEGg2Ew9EfIyKUwv/VQA80dMaonFtHc2cO62sNpx7Js2plKeKmeUEhxJMS4wjyW3fdSn3IumV7OAzddmjSGe8f0FYtmUxwJ0pNQFEdCPHDTpc51yLTSwPyppU4/16zf49S/YtFsEt0dzan6k+1q4k0i4t6W3jN5FDGc9n398Oa6DanOv37BNDbuquP5/fWUR8NJD4fXlrr/pLU+6sZddc45urxU5Xu3u/unnej0vmz7pcu48Qe1bD/axMLqCuch78+1S9eWVPiVZfw/DAbDYBipyD2t2SiPhtl/opVDp9poaLe8SPzGsoH6S14yvZzKsgL2n2xj/St19CQUoYD0Kcc7//l9nzWhiO1Hm5g1oTBJyEs1d+p9j3/hCmecnz+11BGGLpleTs/pY/tT9ScbAWe3iFwPBEWkGvgn4MUszhsVrFg0m+aOGM2dPY5U6hcePVSe4qlCr/tTb7oHU29bMq8yqTzvOXWNHRxv7mLJvErOmVTsW25/MjAP9ked7fm5DDf0O86EFRsMhsHgnZz7O6cMdA5yj/0bd9U5/1ONZemECG+ZqeaCuZUlPLrlCCsXzxlQotfV157nbOtvv91t03P3jT+oRfIKClOdk02YeBT4KnAVIFi+OGuUUl0ZWzRCeMPE3SF7wKDCozOF0bn3a2nYr+xch1enK2/roQaWP7TZyUg5a2KxE5qXbeZi94/H77h1tYe5c8MeSqN5VBRFuGzmOOeHcM6k4qRrpsMDr6upYuOuOuZWlvBI7SEU0NoVZ2F1Bc0dMerbujnd2s2U8gLu+ugFAKz46Z851thJKCjMGF/ILZfPTGrXutrDfGP9bkC46ryz+MO+kyyrqWL9K2/xVmMnpdEQXTFFYX6Qts4444p62/tI7SEmlRbwgTkT+eELb9AdV0woziMvFHTaoetbMq+SxzYfTmpjZWk+z++vpzgSpL07TlzB/KmlvFrXTHdcMbW8gM++/138cNNBjjd3cevV5wI41w1I6q/3Om891MCqX+xwzvXmp9D9v3PDHiaV9paR6pn1fl7z1G6nP+OK8igIW6uxFEZCSaGcujw92J0/pZQXD5zimgsqOd3WnfZZcbdl7/EW7n76NVYungPgfNYp4fubNVs/V/rZW1ZTxcsHT9HWHe/Th1Tlr6s97LTD+9wOd/SLX/tGexTOmU6q/GD9ud5L732B7UcamV9Vxupr5mZ8htyaiZKCcNq6cp1JfrDCWLp8atpHNNNajuXRcJK5Ktv69fx67L//oTl26mip3/HZJPprxxJwvgogIucA92CtS3VGkM2bu5/U6zcxNHf2sP1Io3NMOpOHu2zvg9AfdeHOY00ZH4B05a19bh8N7TFCAaGlK872I42s+sUOKssKkvrj9xDqcL5thxto6Yo7x3n7c/fTr9HSFaelq4OjDR1OmV/5n51EggG64gm2vHkaENpjVjk7jzYSVySZ0tzXVLP/RCs3P1gLCC1dPQDE4or9J1qdH1FdYwf1bd20d8fp6kkA8Pj2twD47z8eJG7L8Q3t1vm6De0NVnt1W1pOtPJGfRs9CeuEEy3dSe24c8MeWrriSddD79t/wooMcG/XoZAARxo6nPaCNaHr41u6OpLK8gudXPPUbsfUePfTr3H9gml9VMrOfXCVkSr00/vZ3db2ht72QHIopy7vxQOn6Ekopwx9vXcea0qpKtfnNnfE2PVWMz0J5VyHhvaY0y/3sX7leNHqeoBdx5roSSge2PSGcx+9fUhVvr4/dz/9GudPKR1QyGx/SDfBDLdJ1QhPfa/vgK63VhoolTSGTx9f6DvW6jG7ubMnY139/U34Heu+z2vW72H7kUaaO3t4/POXZ+yaLvOF1+s5e0IRhZFQUp/c/W1ojyW5TnhflpfMq3SOcwst6V6M3POVLvcnraffStXelAKOiFwA/L/AZOBx4F4swWYB8O1MF0JEHgSuAU4opebZ28YBjwIzgDeB65RSDWI5+KwFrgbagZuVUn+2z7kJ+Jpd7DeUUg9lqtuLVzWXzk7oRk/uzR0xSgrCfex/Gh1Gp7US0FfQ8BNWlsyrZPlDm5PeWt2sWDTbeQBW/WIH9W3dSW+W3jflVP2YW1nivGH/9tW3aemKc7y5i/0n26ieUEh5NMzcyhKW3rMpSSNxy+Uz2fd2CwCl0TxmTchznNn0RLDtcAOzJhbzvtkTeGrHW4wvyqOjO+EIIgBdcUvgaI8lktoVT6E8DAAJzza30OBGoQgAr59sS3IME6AsGqKhvYfighBtnT3EEtZ293HhgBAQoSdh1RgMQFCEBIoEycdPLI4QCQVo6eqgvTuOAIEAhCTAtPFRCvOCbD/aRDQcoKsnQVxBcSRIZyxBT6KvBmfl4jkcPtXGf//xIOOLkjVFvoKv7QcXFByth1ewXVZTxf3PHyQcCvQxjfoJ+O7Pbq2ZV4PjHqS0c/iM8VHWv1LHvMkl7Hqr2VeD48U9mGtbvu7LnRv2UFEUYeuhhqxfAtzlNnfEaOuO0xGL09Qe44YF05I0ONmYYFcunpOkwUl33XJBuglLTwD6PqZq83C05UxmMO4Ama53JjMM9Ar8FYXWhO++n9A7B2XKFuznVOw1TWXKSu/W5m873IA9NLPveDNL732B62qqeGzz4ZQJ+FYsmu282Ow/0UpxJEj1xCLqmjqd8yHZlAWWtkYLJ+4XoAduurSPRkbPt25BSM83Xv+bhz+1gB8v72hLdT9TmqhEpBb4L+Al4IPArcBDwG1Kqc5UBbrOXwi0Ag+7BJxvAaeVUneJyCqgXCn1ZRG5GvhHLAFnAbBWKbXAFoi2YGVQVsBW4BKlVNqQl4Eutul9WFOpGr033e3kqiOE7tywh+4eRVc8wfyppay+9jznwdIqO72QWXk0zLbbrkqrkt52uJGWrh6CAtG8EC1dPU5YXibz2vlff5qWrh6KIyF+9Ml3Ow//Y5sPs7uumVhcEQpAj0eq0OWDZWpBhO1HGgmKJZyEApL0hgxQEA7QEfOKJwaDwTC2+OVn35vWDUGjXwgriiLsP9E64OzGS+/ZxPajTRRHgrR0xZ3x1zsP+LkreDUg7rE71edU7dRm8HjC0oT7nePnFqJNcKlM2NrMvO/tVtpjcWee0f3zM2VtPdTAZRefPyATVUQp9SP7814R+Sel1Jcy3wYLpdTzIjLDs/kjwPvtzw8Bvwe+bG9/WFnS1ssiUiYilfaxzyqlTgOIyLPAEuCn2bajP/QxLynlCDduDYnbTn/9gmnO2yNieZYvf2hzssZBxAmjc0voy2qqeGDTGyyrqUqSrKGvGWjv8RZue2IXPQnlCDrLaqrYU9fcRwXofXAmlURoOdnDpJJIUj827qojZqtRtHAjQEE46LzBVxRFAGjrjnPMNlto4Wb5FTOTzD+AEW4MBsM7AvdYnk6jtnFXHQ3tMXriCSctRirS+bTo5HiTSgsItXY5k/77Zk9g/SuWL6M3JYguU88t0XCA4kjI0W5qq8Njmw9z4GSb/SIcZFJpgROYo33l3PPNztuXsPVQAzc/WOvMdcEA1DV1OqlDdHCP26qh56VUVoiSgrDjPqDnGe1LR3Ikt9OvdHlw0gk4+SJyEdacB9Dl/q5NSP3kLNdK5MeBs+zPU7DWuNIctbel2j4kuB9WbdN3hzF7tSnaX0CHsWlWLp7D6sd32iYKy7kR+prD9tRZPgh76prZU9fcx2bpVRmfM6nYeaDiyjo/k08BwF0fu9BX9bli0WwOnmylrqmTy99Vwc5jTY7Q5tZeleSHnHKLI0G0L8yeumbWLD3f6avBYDC8U8jkGuA+TptlDp1KaU0BUvsA7TzW5CTH0wEXeky/+cFaehKKR2oPsaeu2fHTe2zz4STBJSjaTSDBnrrmpDnLHbCgTULaF1C33e3TpufISaUFtNi+h/GEx3/Q1vgDPP75y52oJ7c5SpetXTdWLJpNXWMHxxotI1F7LM6eumbHRcTrC9jQHkMlEr3+EB7SCTh1wHdc34+7vivgr9LeqQwopZSI5GxaFJFPA58GmDatrz8LpLfFZuMErG+KloJXLp7jW+b1C6YlSaheiXzVL3ZwrLGTcUV5SRK91gJp3FKwDnGfNaGoj0+BVu21dced8vzsuXuPtzjH6aiSTas+kFTGY1uOcM6kYupbrAfsWEM7lBVQPdFyKHPbaHXZ508pdWy22m8pGg4we1IJM8ZHeWrHW0wuK+C7H78IwOm/N1rI3Y9Cl2DojfTRKkytXXOf09bVw/4TrcyfWpr0A3Zfj4unlRFPKNb96TDLLq1i7/EWPnzPCxmfsYFyzlnFPP0vC5mx6tcA3PSe6fzLlbOZf8ezAHxpyTl87v3v6nNePKEIBqTP9lyRKZLPHZk02AXxsmWoHF2NA21qdhxp5CP3vsA3//p8rl8wjV3Hmrjm/9vEhy+czPc+cdGAymzpjLFpfz2Lz5tEYAif4WyIJxQnW7qYVJrP8aZO8sMByuyoxf6yYWcdn3vkz2z8579gzqRkxUG6Z2z6uCg98TbHoTaVUOQe89fVHu7j77aspqpP3jAtZASDwguv1zOxOI/SgjxHsAHbr9GebbV2R/uN6t+2O0eZjrht7uxxNETvnTXe8QnTc+H8qaVMLct3IlR74jiaoza77n3HW5xr4w3AcfvXLH9oMw/cdKmTcycARIIBXjnWSFN7D5FQwPEV1b52xZEQ8dbTx1Ldr5QCjlLqL1PtGwRvi0ilUqrONkGdsLcfA6pcx021tx2j16Slt//er2Cl1P3A/WD54Pgdk86JzrvPTzJ3S+ILq8dx/YJpKZPMuR8Wr0SuJez2hg7OruhNeOSVUrXKTm9r7uxh+9EmqicWUZLfe+vcESTzq8q4ZHp5Ur1AkrTsPs8tDesylj+0mS7b1HS6rZuTrd1JtlW3Zsv7vaQgbPUtlqAkP8R3P34Rp9u6kzJf6v7XtyZnGnC3QX/XbdfX7/EvXNFnICkpCDtt8IZleu+vN0Lrtid2kyuumDWee264mHAwQFNHjMrSfFq7eijOt67Jm3d9iPrWLsfsp1l07ll+xQ2pcAOZHSi9msnhIJs34tFU7lhg9lmWM/W+t1vYdriBv/5PK83ZxdPKBlxmcX6YD55fmfnAYSAYECaV5gM4/wdKYcQad9u6kpUGfi4GGj2uucPA/dDjmtZ86MR9C6sr+MO+k0laGrcQdddHL2Dtc/t44fV64gpOtXYTi1uuDO6AB7ACFGZNLOKxLUf6LHbpdU7W/jqHTllRpXr+cKd2cLdDzzkbd9Vx/YJpFOZZQQrtsThrntrN6mvPSzpHX6NzJhU7107X//z+ehJYASpd7dZc1NWT4JHaQ30CTgL5hSnfWLJJ9JdLngRuwlqh/CbgCdf2L4jIz7CcjJtsIehp4JsiojtwFZaz84BIN6BnY0v186PJdJ7fcVoFN6W8gCXzKrnxB7UsmVfZx3bqPW/NU9ZkfPBkb0jyw59awIpFs52w5QMnWpJsoHVNliZmflWZo31583Qbje09vGZL1mAlAozaUTMN7TGqJxRyvLmT0qjli3Owvo3zv/40N9jRXvoNoL6tm+JIiLmVJU4/3P5IftegrrGDg/VtfaLD3JEwQB/7rfs++IVZun84bnWo1sh5hZtsePOuDwGglKKpI8bWQw38RfUEzr1tIxdPK2PZpdMoioT48Utv8i9XneO8GeqBUAs3Gh0ltPa5fTyyfAGTywqYWWHlqRpM3hOvX1g2nGmT/ljRwoy2fhTkBQkGhObOmCPcAJxbmdK1ARh9/RgO9KTd5plk16zfY2s9kqP13FGHXo209zde19TJ/hOtTC0vcHxr/rDvJHMrS6hr7KArlnCW5YFeP02tZb32wsmsf6WOytJ8jjR0UB4NO6Hp0XCAYCDApNJ8th9pdKKfwBpn79rwapIJasm8Sr6xfjfRcNDxvZlbWcIDm95whJwHbroUwBlndbTu3MoSZ0HNcFCIxRWv1rWwZv0eVl8zl73HW1j+0GanfysXz+GBmy5l1S92sO1wIwfre8144aBQGAnSaKf3sPKFdZOfF6TZ3iaBUEp1XMZEfwNFRH6KpX2pAN4Gvo4Vbv4YMA04hBUmftoOE78Hy4G4HbhFKbXFLueTwFfsYv9dKfXDTHX3J4oqFz/SgZThF9evSWcy0JKu16Pc7fDljdrylvmur2xI8nqH3h9N9YRC6tu6WVZTxaNbjjh16XJ0ve6y9fZsIrogOcFTj/2moSPJvMcMNOLgg999nlePtzDnrCIO1Lfx3WUX8fl1yW5jxfkhWjp738T0D06z47arKI0mCyi5IFXf/LZnex28EXljkcE+E6OF0diP677/Eu2xHnYds5b1Wbd8Ae99V0Xac0ZjPzIx2PF+z1vNXP29P/L9v7s46UVURzh5TePea+R3zfS50XCA9ljCiZLSY6r+XhzpjZ5duXgOP9x0kAP1bY7pSZfpFnq0aWv5FTNZdfW5SXOIe+zXkVChgPDoP7wnaXzXwpmeX3R008LqCmf+ml9V5micrHE9QUtXvE/Kj4XVFX0sCXrM0mOYLl9THAly69Vz2birzqkvIL0mt7qH/pmuuv2+6u4h0+AopT6RYtcHfI5VwOdTlPMg8OBg2uL1R3H7FXhj77N98P0yKmpHKUi/HIT7IZs/tTQpzK56YpGjHdF+Je6swCsXz3Hyhbi5ZHo5syYWs/1II23d8SSv+WAgkORRXx4Nc7K1m3BQWDKvksOn2njh9XrGF+VxvLmLlq4eR7gJBYTzp5SyaX89waBQGAlx84N/ctSfIE5W4KnlBUmaHneElzt7sdYWFUZCdPXEae+2hAvdZ40330Ome6N9XCaXFfBWoxXx9drblqbLLdxcMLUUEeEJ2/Ft7XP7+Pv3zOA9s8azaX89n/nJVr79txcOiXADfX2rdJ/8tIGpjvXizt8yVhnKHDDDyWjsx9RxBdQePO18zyTcwOjsRyYGm+unyNbMtnbF+5h0tCDgFjC0BtqtqQfPNXP8LoXqiUV0xOLEE92EgkJxJERpNExLVweTSiJcVFbmzDna1A+WEOCu48DJVlq64rxypJEEVrLTaeML2birjpWL5zjj6d7jLew81tQ7xgeEvcdbWFZTxfefPwhYOXKsMd8am8cV5dETV06qEYBX32oCxNH4PPzSmwCEgwEKIgEa2y3BrK6pE2VnFqueUMjB+jbCQeHK7/zBmg9iCfLCQlN7j5N/rKUrzm1P7OKaCyrZdrjBSRzrkEZLk1HAsbUrNwBnK6XuEJFpwCSl1J8ynTta8GZy1bZHK+19p6WqE0n74HuFFK/DlDsjIySrEb3ZIrX3dyggXHfpNK4DJwS8sjTfWZ5eozOzrn58p50DJzlTrUYLbM2dlqOtW0Wp7aJrn9vHyVYrO28srti4q85K+GTbbnXeAT1hNrTHePHAKRJAQMFRO1TcXXZ33Epk193U6Qhq3iyy3iyzkJwt9w/7TiZJ9YDz5nLgRKvjLHf/jTUA5NvmNIDOWJyfb+kNttPCjR/vnlHOY595r/Ndm6/WPrePcYV5TCiOsLC6ghkVKZc38SXbJS/cfkPeqAA/k1GqY71cv2Ba1qapM5UzzaSWitHYj2hekGNpfjd+jMZ+ZGIgifvcRCO2X0l3T59IIK9fIkBJfsjXF9PN6mvmOi+k9Xb4N/RmW581oZCzKwqTomu1S4M7WMMbcQUQDvX64OjxXL/M7z3ewjfW76Y9lqD24GnL56UnwW1P7GLeFCutTCggTuSV1qx0dsdp6Yo7L1SHTuvs7IriUJA9dc20xxKEApLkQ9PQHksa4w/WtxFXVrZ4nTHebS1w05NQTqb0oEtXExDAsxK4m2w0OP+JpWX6K+AOoAX4JXBpFueOCrwPtfbzOHiylbiCjvo27vjIPFCqj8ZAPwSdPQlHJab9XnSZ2jdH2xC1n4qOZjpwwsoGjFJOnoIA1k372v/sZNbEIq65oNLJZTBtfCHbDjc4/i+NHd2cbLGED52nYNbEYuZWlnD+1zc66w5pLps5jkOn2pz8AW5NyMGTrQQgyeO9rqmTlhOtVJYVUFGYByL86Y1TNNoPWnF+kK6YYlyRZeo82dxFVyzBjPFRDpxocdaQ0oJLQKxcAuGAUHvwNCXREOGAEEsoehJW5uHJ5QVOWWD5rFQUReiIxXm7qZNYQjk/0gSK8YVWFuU5qzcC1uAaDQdIALUHT/dJOJiKL3/w3JT5haDXsa65s8eJ0HJHGvilG3erTvXz4cX75tift1/3se9EvwfD0POTlw87n6+a6+/0fqbh91vJJJRl0vAU5llT5oOb3mDJeZPYeazJGWfd2ms9x/hl+J27+jfE4oqACCXREJ3dCW5YMM1ZL688GqK9yxIKJpREQMSJXNLjzLbDDUwqLWD2pGKuq6li7XPW2nDbDjcQCgqRYACAkoIQze09BF35ZHQU6rbDDU52ebdGpCeh2Hm0keqJRdxy+Uwn8/pF08p48cApPnDuWfz21bdpaI/xjfW7mVIedZbI6bSFpUgwYLWd3pdi6M1SL/SaobR0EgoKCpWUIR6xhK4AUBK1+hLNC9LWFafCjhY7GuvqfyZj5wCRPyulLhaRbUqpi+xtO5RSF6Y9cQTJxgfHG2Xktke6/Unc9kJtn0w1sfj5QXgXFPNqkzTurJR6DRyvf4x3oUxdX6r2p/rvrk+nvfaa27ykK2MgePuWalt/WfOR81htR0bde/3FfH7dn5lQHOFkSxe3fnAO//C+WUl2cH1/9b3VYe464sGdodr9Jua9HpkWysuVYHIm+j0YRj/avAtw4JtXD3kE33AwkN9Kpt+pUoqZt24Akv0OoXfc0uO/N8O9O3u9l/JomBZ7+RLvdu84c+h0u6//Yyrth6aPv+XEIo41tPdZQsddrrsf7jnKva4W4Nsu8J8j3L6XqdDn9fofBftET+lj0i22GUhZQy8xEQliL8kjIhPou1TQGceKRbOpnlBIJBggGrbslysWzWZhdQXLaqqcNUNWLp5D2L5K8yaXcPODtVzxH//L0ntf4K4Nr3LRHc+wrtZ6+1m5eA7FEcu3RO/fdriB6olFLKupYvlDm5lbWeLUG7Q1HROL85hUmk80HCQUFLa82cDUciuT5JJ5lUwtyyco8IFzz6IkP8Te4y0svWcToaAg4OQH0N76KxfPsXx77P2tXT1UTyxi5eI5TlnvnTWeaDhA7cHTrPjZNpo7e9h7vIW6xg7CKQY3nQtHqwgLwgGnH26K7EgDvTkcFMqivcrCaDjAC6/XM64wj6ll+c6xcytLWDKvkmg4QDgghANCf4fZXcd6TXtf/sUOvvnX5/P9v7uEv3jXeGpmjHPuvRbsltVUERQYVxhmzfo9XHfpNCvc/NrzWLFoNvOnliblFnL7D+lrrY/Xof1etE2+uTP1Dzpb3G0fSnQUmo6yG8u8k/qaigds0y9kn55gtF+3gfxWtIZHa2u9/RMRonlBppTl877ZEwgFhHGFeTR3xJhQnEcAy09nXe1hZ6yYWl5AXVMnV37nD47/iUZf6ob2GDPHRwkFhIXVFUTtsbUwEmJqeQHttnPvjqNNgCIStPZPLS+gMBJy5o5oOMiE4jwiwYAzdkbD1hjtDiQJB4T9J1rp7Ekg9jFgjcPzp5YSAJo7Y5zz1d/wyjFreZ6Z46NOW1852pjUh1frWiiMhJxxXtftJ9wURkK0d/cdC8ttTb9gjcfzq8q4YcF0y/na5ZupLQQFdpsD4dSZjLPR4NwALAMuxlpe4WPA15RSP0974giSbRSVVwOSKnIlldTtlmi9GhvvfsCRsr2rFGfSsLi1DO61R9ykWo/Evd8tjacrIxWZ9qfD79xQQCjODyVFaOlrc3ZFIVPLC7LS5kwsjnCipcsp0/2j8mrEvG9yfpq8VG97/Tk21XlniublnaQpeif1NRXuZ1SnRsj2nLF63VL177Jv/paFsyt4ds/bKcflVJ8z8eZdH+ozzqTTlA9kHF9YXdFnXE21HlV/Gcy53nZnq9FPF0WVUYOjlHoE+BJwJ1Z246WjWbhJh5bI19Ue5sYf1LKspsqRlA/Wt3Hld/7A0ntfYG5libPC9pXf/r2V1wVL8g2KpXGpnljEpNJ8wgGhsT3Gu//9WedcnWNg+RUzHY2O1qY0d8aoPXiaCcV5jnQfCgoBLE2HfgsosKXucYV5VBTmURwJsfyKmSysrmDl4jlUTygkGg5SFg0RALpivRLu0ns28erxZsuWad/29u44d2141ZG8dfihYK0hEgkGiMWTFXORoCRpc5o7Y4QD4mhstCStJW8vwYDlh6PfRLR0r7U211xQybKaKqfvhZEQO20NzMH6tpQPdkGot115QeG9s8Y7bZo3uSRJM7espsrRbOkoA/1G5l6dd2p5QVI+H/cx+rvW6FRPLHL8mvyO8+LVBJ0JDJemaDQwlvuarZZlxaLZXD5rPD+6JXvXytF+3bLtu/s47+/dr3/RSJCjDR19xuXp46LOMU32nKHHtSLbOTlA8qRbHg07+wD++WfbOGivOQVQVV7AvMl9FRRa4zJ9XJTiSDCpjNaunqTvkaA43yNBYdP+emfs92pDwMr87CYYoI+WPhX9FW5c1dLcGUvS2L92vMWxYHjbEw5IVuandKuJj0t3ol4AczRSU1Oj7vvls31sqV7tRVAgIJbzqx/eGH79PRoOEE8kO2Zp3DH8S+dP9o0OAlJqY3QuBC966QOdLOrj973IaFnXUuck8OYvyER/j0+FQJLid35VmWMfjthRBG4bcSgg3PGReWzcVcfz9o9dr6/lvh/BgLW+CljC7bP/+n6njiu//Xv2n2wjEhTOnWyZf7cfafT1xfFb8sDPWVkvgTFcyyK4OROWSBhsWaPRQXuo25RrLctQtjfXZWfbd/dxzR0x33w2bj7w7d/z5qk24glrXACc9Br9JShWBJvXv2Q4cI9vZzID1eBsBbbY/08C+4D99uetuW5krtHOsjpsG3rfOFYunkMoIMQVScJNgF6NB/R1NAqHrMvVHks4wk3Ylm7DtojrnrAf3/6Wr3ATEJK0MQHXdlJ4nbTHEmw/0sja5/ax9rl9ORFuJhQNbD0WL+FAgPJoOKnvS+dPzij1D0a40ffJtw6X0N7Vk7BMhCLOvehJKO5++jVWLJptOfd1xZMSaDntc13j483Jy0ro711xZQlTyjIn6nQD7udOp2rX909ve35/PXc//Zrz33vMcOL3exlt5Q62rKHq42AY6jblWssylO3NddnZ9j3pOB1xnDrymPrWbmdsON7cxX57zadwFioFb6mTywpGRLiBsSHcZCLlLVFKzVRKnQ08B1yrlKpQSo0HrgGeGa4GDhS/h1vnPHls82HKC8MEsMwr0XCQaDhIgt4JM2J/CAagLBpCgIRSTC0vYGp5gaUiE/jQBZM5u6KQcCDg69zl/g6WyvHnn3kvV543ibrGDm5/cjci1oR9wZRSOmLpH/bn99fz2vHmJJXi/KmljlkmWwJi5b3RaIdn8RyTDd3xhBOaGA0HWTp/Ms/sPs5gk2RnU71bSNKmtrbueJJas6kjRkl+yHFe1iazVb98hYqiCNUTi5hfVcYDN11qraQ+fzJg9V8fXxoNs/TeFxw19qSSiPOsTCyOsPra86wEWydaqJ5Y1CdZnzZRaTOZNoMuq6lKMjt6l+rYeqiBpfdscurOlv46gLp/L7l0Hs3lBDvYsvp7fi6vQ6qyhtrM43aazZZM5tahaq+37MFe/2z77j5u9TVzWVhdwXU1VSnrrizNtx1zg1w0rYwA1tgTEGsOKI6EHP8RNwGgwnZBAGt8O2KHULvHOj2uuLfpeaogGykKbK1033IGSyRbW9UoIRsn451KqfMzbRtNpHMy1mmxNfoHteoXO5zEQ17cjlPFkRAXTStLCgl0TBpZmFy0s6+3HYMhm7C7gTIQM5LXtDfa8DrCLayucFa11Vk+/dKr62O1Gttd3qP/8B6W3feSY3Z0r1nmHmD9wsz1khvplmrQdWdrZhiMaaI/545Gs0+uyKV550xyyB0tbR3JdqSre/7tz9DYkd5peGpZPkcbO4eyiQabQTkZA2+JyNdEZIb991Xgrdw2cWjwOhVvPdSQpHqsKi9ISnudaiIvzu+Vhtu7rdDtsgLLYXb6uKjzOZoX9D0feqXyhvYY53ztNxw6nTI3Ub+IBAM0tMfSCjeDkblz4SMzHHgd0bxoR+iAWM7N+k0kErKSaK1+fCcN7TFuf3K3E9rvTrCnNTBzK0t4xXaELouGCAWE5VfMZO1z+5y1XHRKgOf317P8oc1Jb4FuM6kWcrRK3u/t2C9U3c8h0stg3rT7c66fWaG/b96jNdx4NGmfhpOBtHUo7uFIXrN0dWcTRu8n3ORC91EQyl5Lb8hOgzMOa6HMhfam54HbR7uT8ZYtW/okKNIP7HXff9GZuHW2xtuf3EVXPNkfJ5UWos9aGINkYXUFf9xfT39liQAgY8RRbDhZWF3BtsONtHT1OGY57yWMhgP8ePllrHlqt7Mm2HU1VUlZj4sjQX70yQVJTsPuZIla+6UdFv3WJHM7GmejBVl67wtsP2JlGdVp3VMlGUxVXy61LX5l9vfNO1dv6mNZmzQcDOb6eROa9nc9v1zcr+G4/1sPNXDTg7W0+vjN5HpeMGTHoBbbtAWZFSJSbH1VrZnOGS2414gqj/ZOAGdPKGL/Casb+0+0cvfTrxEMBCDe+9Cme0xz/RBvefN0v4UbsNtofk8p8ct+GQ0HaO7s4YYF05z1svyufUcs4SwqqjlwosVZaiIcDNDSFWfNU7uToi32Hm9xwtLbuqw1wbTW0L2wa1tXD8ebu7j16nMdkxiQlFXad5C2X0iON3U4GT4P2E6OkJxe3pt2frALDfrhTX2vQ+/7ExLfn2Ur0uFd822gvFMFpcE8H+6xds1Tu7NauDjXz+NQPN9+dfgJN5D7ecEweDLqu0TkfBHZBuwCdovIVhGZN/RNyw3Tx0WpnljE9PGFrHlqNzNW/ZrDp5LNQw3tMfJCI+c8lSpVtiE9ASwN3NL5k/uofwWYNaEo+XjpjUZ7+KU3SdiamKDA1PKCpGzLCmvdrwA4zuN6YJs1saj3efEILzoaaufRRho7uomGg9S3diXlV6pv63YiL76xfk+SQKDL8Zq2NKuvPY+F1RXcerXlDDlrYrETAeYVELyOw/0VPDTpHJ29JrPlD21m+9EmSvJDWQsHA3GE9UVrowfp3T4aoq28Jp9sTZODYcm8SieDe3/R6/GliiT0I9dO3wM1afXneg7k2hhGjmwMevcBX1RKTVdKTQf+Fbh/aJuVG9as38P2o00cb+pk+5FGxzG0y8expLEj9w66hqElARw42cpTO97qo4VR0MeJ253uqD2WcJRfcQUVhXm0uJ4B7TOVABraYnTFEyjXdq09WX3NXO7a8CovvF5PeTREVyzhLCR3sqWb9licow0dbD/SyCO1h2hoj3HaFb0GKkkg0KHrbt8cN1oYOGdSMQDX2VFYbrOAHrABR3DQoeooxdrn9vXLP0YLLX4h7G5hYM36PXYCyVBWk0yuJ2ot/K2+9rxBlZNpovRrd6774hWy3N+HSgDbuKuOhvYYdz/9Wsp+pOunfjZ1JFKmZ6C/gm2mfmcqL1XbM5XrPm/jrrqs2moYHWSzmnihUup3+otS6vciUjiEbRo07d1xbvxBLfWtVq6S0mh4SCKMDCPPALOC98ErDI0vykNhCSnuiKu4ssxDYAk5H/uvFx3hqqE9+RkTYEp5Aadbu5lir57ecqKVcYVhaFPEE1CYHyIYSDhvhvpNWK9dtfVQQ58BWwsdOnrPayLy26cnm+bOHl81fiqzzNrn9jlZr2dNLO4zac2tLOHFA6eYW1nCywdPATBrQmFWk1auTQqZVopOhbfvA1lxOtd98Zrt/Mx4uXa+dZuZ1j63z7cffv30S2SZ0dQ6wPa5/3tZV3vYiYS8fsG0pH3pfjOZynX3ecm8ykEtBmwYXrIRcA6KyGrgx/b3vwMODl2TBs/bzZ2uDLXW27l7yXaDIRMnWroJpYiWcPv1uOWrYABK8sO8b/YE/rDvpBNyfrShg8rSfGfQb+7scaIsTrZY2pyNu+qcQfmS6eXOSuZ+E40WOrRZyuvg7N6n0ZO2+1jondybO3uc7M/uyck9+PtNVI9uOUJPQvHoliNJofGpcNefK9+bwdJf4US3V+c0Goq+eIUs7/eh8DHRwnW6e+jXT0c7aH8eLn8vL3c//ZqjgfIKOKl+F9mU6+7zaEoSachMNlFU5cDtgPak1FFUoyum08XcCy5SNSu+z9zKEh6pPcSk0gJQiv0n2yjKC9LaPTKZIw1nDpGgUBAJ0tieWfMnWLlwwsEAV513Fr/ZWUd33EoKecn0cta/Usc1F1Ryuq3bWZKhrTtOY0c3p1os7c5n3/8uNu6qY25lCY9uOcKymipLI+KzvIN+rkujeRSEgxxv6qSlq8cxC3gjs4CUb9M6+sUdiaUnp3RRTe62PLrliO9bsx/DkdvEK8T59T2bYzLh15ex6KCcqU9eDQ7QR6OTTVmDuXZbDzWw6hc7HMd997Po177B1PPR/3qxX+cYhpbBRlE1AP8EICJBLJNVc26bmFuaO2K8eOAU2w43WGn4T/QGfrUb4caQgUgwQDAgWQk3YGlxFNAei/P49t4UUUcaOpxMpXq7fqstjgTp7LH8gE60dDmRVC8eOEVPQvFI7SFCdo4jPRhrFbsuo6WrVyupnUO9oerNHTFnDS7oa5bSjsfuicitoVh6zybficr9hr7ttquyvbS+GoD+TmyZjne3TWumvJFV+pidx5qSlufoT92ptBlDGcnjZw7KBV7zjvv7Y5sPs/1oE80dMd/1mS6ZXp60/cYf1LL9aJO1RIqHdNdnMNdO5zKbP7WUjbvqOGdSsXNttIZpYXVFUkJNXU9/nr9LppcTDgqxMyVB2DucbKKo1olIie13sxPYIyIrh75pA6e+tYuehPJd48PEKxky0RVP0G4vmREAssmt1ZNQhF2r9maipSvuDJKxnoTj2Lr8ipmUR8NMKi1IUqlrFbvG/bpSHAnxwE2XOkKSHqznTy3lwMm2lKp5PfB7I560yn7jrjrHuXjVL19JctDU7dVC0NJ7X0hOqJmBvcdbnGO9Tp6ZHHYzOYUmOQmniKxyO3Pf9sQunt9fz7L7XuKuDa+mXRrDXbefU6ufg3Kq/gzEMVnfs+1HGlmzfs+gy9Pc/tQuK9HlU7uAXnPPnRv2cMBe3brN9m10l7+u9jAX3fEM62oPO9vSOcqnuz7jCvMIBYS5lSX+SVrToMv1i+DSvwXt0+b+fteGV1l230spnz/vNb1rw6tGuDmDyMYHZ65SqllEbgB+A6zCWmzz7iFt2SAI2es/mcfQMBgEKyQccPImpSMWV8TiqTWEbvOoXoFdV7T3eIsjyDxw06VAshrd7QAaCgjBgNDVkyAocOvV5zoJCXUYuPbj0SHk3uRrqcLGvRoKrQU93tThXAP32/VjW444/heHTrX5aorcuDUn+livJsT9Ju/nrOrnA+OOIHMfv/ra83x9SrS/idvxtCehnNxIuh1eR1r3NfaSzlHbTzMxEI2F+554hTa31i6bPDRuuntU0v+Vi+dw99OvUVEUYf+JVsqjYQojoSSt18ZddU7CTLffSzpfHj9/F91unZD10S1H2FPX3Oc5SXeNUvmY6X2AJRQ+tZvV157naDV3HWtyMpBrfzb3MwEk3aP7/ziq3U8NHrIRcMIiEgaWAvcopWIiMqplh5gd0mswDAaFJdhEw4Gs1uRyZ7+eWJxHdzxBY7slYEwfX9ib+A+YVVHI/pNWPqaEgtWP7ySaF3Ki/ZbMq2TnsSb2Hm9xBujp46JUFFlZletbuzja0MHZE4ocTQvgqOHdZaxcPCdJAFjz1G4nOeD8qaVJk7J30v3RJxek9OnRvjvVEwo53tzFspoq9tQ1p538dbSY9kVq7uxh7/GWpGO9Tp2pzAnZRjM1d8RYs35PkklHl7Ny8Rzu3LCHlq44QYHlV8x0fJ9SOdL6mV7cdW873MCsicWsvmYue4+39Fn6Q9c/kLxEl0wv50efXOAIW1d+5w8URkKsvmZuxkg5N+7r+OOX3nTGSwXMWPVrwBLIUYriSMgx4+2yBQ6d0bt6QiGhNmt5Erewufd4S9Lzm0r4c1+Hy2aOS/I/m19VxnU1VUnPXTbXx7fPrlXC3Q7Hy2qqHB8ybb5qaI8RFKhr6uSWy2cCvc9kOCC+aUYMo5NsBJz7gDeBHcDzIjIdGNU+OGfWeqeG0U62iRgVvQt5zplUAlhvfw3tMc6f0vtTi4aDIJY5SycPjCsrsaDOtHznhleT3oz1BFseDbP/RCvzp5ZydkVh76TWEXMmZXdkVEN7LClCq89Ebav0dx5r4oGbLu2jTXFPGG7HTfdxa57aTUtXGy8fPOXro6Hr9U662qTm1fy438a9QoCf74xf6LT7eG+Ej7c9syYWs/1II+dPLWPV1ef6tn/Fotk0d8Qck59byPLTsm0/0sjyhzbbi+DGCbV29dHquP1C3GRaYkNr59ypDXTfUmkx0t2PVGHPrd1xRwjX+V+0tkMvKnvXxy5M8mvRz5E3oimVdsl9HVZdfS6rrj7X8eFZWF3B9Qum9XEYzsZfxu2rdF1NFSjl+Jq57+kl08uT7rn7Hu4/0crGXXVJAtPXPzyPr/zPzpT1GtIzUMtKJBigewCKi2ycjL8HfM+16ZCI/GU/6xk0IrIEWAsEgQeUUnelOtbI14ahRGtzvOuVBcSaAKLhoKOhmFpeQJM9eb9ypJFIKMCU8oIkk1f1hEKONXbYgpSw/UgjU8vy6YjFWVZTlTTRX1dTxWObDzvCjB7kvU6eWrvit3inFob0YK9V8mue2m0dJJlfEdyCT5ttdmtzOfB7nWH9zEpaw+TW/Hg1NF4hQJtoGtpjPLb5cB+ByvsG7+5vurDuTAKBdqRNp0Fy5zDa93YrDe0xppblEwqKI4x5NVl+dXrL9RMOdL/q27ppao8xt7IkSYOSyeTl7n9zR6xPHiiwJqMLp5b20WbpKL6KogjP7j7O8oc2s6ymyrkva9bvcUxcXiHUq13Sz4A7Q7DftlTXRuNd1625s8fp04ETLbR0xameWOTcu1TXx30P27rjfXJRXb9g2hkr4IQDQqwficPCAQgHgxTmB+nsTlAaDfumWymPhmjtjNOTUJRFQzS29zhzcMR2yNbj5LsmFHKwPnlh62g4QHss4Thve524gwJf//B5/HDTQfafbHMWVnYWNE3TpZQCjoj8nVLqJyLyxRSHfCd1sbnFjt66F7gSOApsFpEnlVJ70p9pMCQTCQ5exRxXlvZD+ye4t4OVD0ebjfTbrh6Uu3oSFOYFmT+1lH1vt9Iei1OYH2b2WSG2H21iSlk+lWUFTq6clw+e4tEtR2hojzlvtG5nYr+B2j15ed9yvREvgOMv4c6Fk6psPwojoaT/+nyv5uThTy1IimABa5mUPXXNTl3u/X5RSpdML3c0LtkKYrq/3ugZv7wymTQE7iR2fkKKrk8vitrU0cOPPvnupIieTOYjb79TCQePf+EKp0/6GUlXrrcf+rhDp9t9j3nXxKI+z4q+hzo69Y36NifqT9PWaQmf08cXOhm3U/nI6OzJWsu49VCDo/1xax7daQn8hB+vb9f8qaXMt4Wz+tYuWro6ONbQzv4TrY6WKZUGSN9DfW29vlhnItUTiziQhR+hm/ywZTKPhAP86JPvBnAWqtZxFwmsBKc6VcLWQw1Ji1kHAwG64nHLzB8IcMsVZ/PDF95IGjeDAWFhdYWTcqK9K45baokr6zkpzLdMw3VNnU75oYDQ01Lf+/B5SKfB0dmKi/txTYaKdwOvK6UOAojIz4CPAEbAMThvAJkIB4Vp4wuTf1y2NmZicYS/uWgK9z9/kAS92pkA1jwaVxAJBbjlvTN4+eApR1vh1qrUt3VzurWbSaUFzluodgp+76zxvPB6PZPLClh97Xl9TA+Ar1lCm5ncUVB+E78bvwUwM03aenDSWpf++IWsvmZuHw2IW3OSzoTU3BFLekv2Cmd+k7VffZnI1uclGwEkm2NWXzPX0Yy5J8hM9w5SJ/nzMz25NWP98VVxs6ymiu8/39d5tiPm7zC/YtFs6ho7ONbY6bzdTyrNZ/+JVis5pogj3C5/aHOSMOGnYXP/9ybk8yai1L8pt/CT6jroOpfe+wJHGzqYUh6lvrWrj4nR75ythxqoa+ygOBJiybxKpx0H65PXMRwtBANQEA76LgRaPcGayvUIWR4NOVnXrZQY/mb4VtsfsKE9xs0P1jKptIBxRXmcbu3m2gsn88zu4855Lx08xaX//iynWrtJKGtc1YtaF0eCxBOWCf6Hmw5SGAlRPbGIjlic063dlEbzaO7s4eU3TtPQHiMaDlAcCnHDgmn89tW3OdbY6fhDaXO2FphuvfpcbrizMWVq6YyJ/kYDIvIxYIlSarn9/e+BBUqpL/gdH6msVpU3fXcYW2jQBID8cNAJsx4OrHDoVictgBZayqNhzp9S6kxGOqz57qdfoygS4khDB/OrypLyo+iBTEeHlEfDffK8uDUB3iRvelLTpiH391wsIzAQhiO53kAZrrZlW08217s/fiCZEuSNhqSAXq2apjgSZOftSzKe433WiyNBSqN51DV2EFckJaDs7zXzJqL0E0YykepFQguqOqTd/Wx4+6fNeEnRj6OI4kgo7XJEUdeY7O7DN//6/D4alWzQ1ywV1RMKLY2urWXVmmFdt77W7uvsHsf1uOy9D0vmVXLnhj3EE1besfJomB3/vvS1RFe7r/NcRh8cETkby/flMiy90UvAv2htymhBRD4NfBog76x3jXBrziz8NCABQAIQz6AY0TZXTQLrbSIdxZEg3T2KYEDIC/cm1AsIlBZYPxy9jlNBOEhHLM7JZmtdsQklEd5u6kyyJbd1x7n16rncuWEPpdE8AJraY6xcPIdzJhUnaRL0G+L0cVFmupx0NXrQ09EhfkngUmknvG+f2aS+z8RA11fyttf9fzQxXG3Ltp5srne292Qga1qNBCsWzab24Kkk060At149N+057t+AN+w+FOxxXjJSRbz5kU7D4/aDSYWf0JhqmYt02i9v/3TOocllBUO67I/Xry+dSV2/yOlUEX9645STUNQb9TmlLN/xfZlcmk9bd9xJ6vjY5sNJ50TDAeIJKx8Y9HUM1lF1OvP6wy+9STwBCRSxuOWDWJgfZvuRRke4vflBy6yZcD0T0NcncM1Tu51Fgb379XOkX2RDAbEi3orGTU51PbNZquFlLP+Xn9qbPg78o1Jq2H6RIvIe4N+UUovt77cCKKXu9Dt+LGlwvA+8F202ue/5g318rYIeASUaDjKuKHldrkhQWHD2eA7Wt3G0oYOpZfk0dcR8kyRq3M5g500udUI53csQgOWTcV1NlROGq0mX2t5vgFp6zya2H21y3uLcOTPAkvy9fhapcr9kygI72HTxo+GN3HBmMJqeF3eeHx12PpA2eZ190/2uB9NWb1leU5bfGONeuueuj17Qr3a4y/Az52VLeTRETxw+cO5Efvvq2yigtStOOCh86vKZji+VXtxWp4PQWEELlhn8lstn8sNNBzne3MUNC6axp665zxgMOPcT/Jeo0Fmr3zd7As/sPg4IX7tmLudMKmbNU7sdR3Zdh59J3X2NUpncUy2Xkeqz33OzrvYwd27Y4/R/4646fvK5v0ypwclGwHlFKXWBZ9sOpdSFGe5lzhCRELAP+ABwDNgMXK+U2u13/GgXcLRE7JWy/ULotGoxAEy2V6TWql/3JK6FgHTMrypzVIXe8t1rEa1Zv4ftRxoddWJQ4PJ3VfDigVNUluZzpKHDye3iHUxSrc8zmMFTO2zqME89gOpoIm9+k1RmodFsqjEYDNnh9zv2W1NNjwl6n/ulaKBjwEV3PJPWNOPFXSf0mna8/xdWVwA4L2d6bC2OBJ0IsMrS/D4CnG6PrsddTrZ9dL8U6r5p03yqMXOwY6n7/HTt9dbjV6+IbFVK1fjVk00enN+IyCrgZ1jz7zJgg4iMA1BKne537/qJUqpHRL4API0VJv5gKuFmKHALHl4hJC8odPuoEIsjQdq64pS6JHa9wjRYqdC9CyoCzoJxbonczwHOKz2vvvY8R9o+3drNuKI8KooizBgfZf0rdSy/YiZXnjcpq/LdQkS6uqFveG2qyJdU+VGywe1Umiovi7uuVGah0WyqMRgM2eH3O04XOaj3uTU4Ax0DVi6ew+1P7nIW051ZUeiY3rT/4Y3vme6M65fNHOcsjFtRFEnSdrv/+/UlkyZMt0fPJd4km9n20W2q0xohb3h/rsfSdPcw3XH9rTcbDc4baXYrpdTZWdU0jNTU1KgtW7aMdDMMBoPBYDAMIYPS4CilZua+SQaDwWAwGAxDR0oNjoh8SSn1Lfvz3yqlfu7a902l1FeGqY39RkRagL0j3Q5DHyqAlDkLDCOGuS+jD3NPRifmvow+piulJvjtSCfg/FkpdbH3s9/30YaIbEmlsjKMHOa+jE7MfRl9mHsyOjH35cwiXcYSSfHZ77vBYDAYDAbDqCGdgKNSfPb7bjAYDAaDwTBqSOdkfKGINGNpawrsz9jf84e8ZYPj/pFugMEXc19GJ+a+jD7MPRmdmPtyBnFGrEVlMBgMBoPB0B8yrBpkMBgMBoPBcOZhBByDwWAwGAxjjjEn4IjIEhHZKyKv20tMGAaJiDwoIidEZJdr2zgReVZE9tv/y+3tIiLfs6//KyLiTi9wk338fhG5ybX9EhHZaZ/zPRGRdHUYLESkSkR+JyJ7RGS3iKywt5t7M0KISL6I/ElEdtj35HZ7+0wRqbWv46Mikmdvj9jfX7f3z3CVdau9fa+ILHZt9x3jUtVh6EVEgiKyTUTW29/NfRnLKKXGzB/WOlUHgLOBPGAHMHek23Wm/wELgYuBXa5t3wJW2Z9XAf9hf74a+A2WM/plQK29fRxw0P5fbn8ut/f9yT5W7HM/mK4O8+fcg0rgYvtzMdaCtHPNvRnReyJAkf05DNTa1+8x4OP29u8Dn7U/fw74vv3548Cj9ue59vgVAWba41ow3RiXqg7zl3R/vgisA9anu2bmvoyNvxFvQE47A+8BnnZ9vxW4daTbNRb+gBkkCzh7gUr7cyWw1/58H/AJ73HAJ4D7XNvvs7dVAq+5tjvHparD/KW8R08AV5p7Mzr+gCjwZ2ABVvbbkL3dGaewFhB+j/05ZB8n3rFLH5dqjLPP8a3D/DnXairwW+CvgPXprpm5L2Pjb6yZqKYAR1zfj9rbDLnnLKVUnf35OHCW/TnVPUi3/ajP9nR1GDzYKvSLsDQG5t6MILYZZDtwAngW682+USnVYx/ivo7Otbf3NwHj6f+9Gp+mDoPFd4EvAQn7e7prZu7LGGCsCTiGEUBZryZDmm9gOOo4UxGRIuCXwD8rpZrd+8y9GX6UUnGl1HwsjcG7gTkj2yKDiFwDnFBKbR3pthiGj7Em4BwDqlzfp9rbDLnnbRGpBLD/n7C3p7oH6bZP9dmerg6DjYiEsYSbR5RSv7I3m3szClBKNQK/wzJLlImITqzqvo7Otbf3lwKn6P+9OpWmDgNcDnxYRN4EfoZlplqLuS9jmrEm4GwGqm2v9Tws57AnR7hNY5UnAR1tcxOW/4fefqMdsXMZ0GSbMp4GrhKRcjvi5iosW3Qd0Cwil9kROjd6yvKrw4AVFQX8AHhVKfUd1y5zb0YIEZkgImX25wIsn6hXsQSdj9mHee+Jvo4fA/7X1og9CXzcjuaZCVRjOXz7jnH2OanqeMejlLpVKTVVKTUD65r9r1LqBsx9GduMtBNQrv+wIkX2Ydm9vzrS7RkLf8BPgToghmVD/hSWbfm3wH7gOWCcfawA99rXfydQ4yrnk8Dr9t8tru01wC77nHvozbDtW4f5c67bFVimoVeA7fbf1ebejOg9uQDYZt+TXcBt9vazsSbC14GfAxF7e779/XV7/9musr5qX/e92NFr9nbfMS5VHeavzz16P71RVOa+jOE/s1SDwWAwGAyGMcdYM1EZDAaDwWAwGAHHYDAYDAbD2MMIOAaDwWAwGMYcRsAxGAwGg8Ew5jACjsFgMBgMhjGHEXAMBsOwICJlIvI5+/NkEfnFENY1X0SuHqryDQbD6McIOAaDYbgow1qlGaXUW0qpj6U/fFDMx8pLYjAY3qEYAcdgMAwXdwGzRGS7iPxcRHYBiMjNIvK4iDwrIm+KyBdE5Isisk1EXhaRcfZxs0Rko4hsFZE/isgce/vfisguEdkhIs/bmWTvAJbZdS0TkUIReVBE/mSX+xFX3U+IyO9FZL+IfN3eXigiv7bL3CUiy0bkihkMhgETynyIwWAw5IRVwDyl1Hx79fP1rn3zsFZDz8fK+PplpdRFIvJ/sZaI+C5wP/AZpdR+EVkA/CfWmkK3AYuVUsdEpEwp1S0it2Flav4CgIh8Eyvd/iftpRT+JCLP2XW/266/HdgsIr8GpgNvKaU+ZJ9fOkTXxGAwDBFGwDEYDKOB3ymlWoAWEWkCnrK37wQusFdMfy/wc2tZLAAi9v8XgB+JyGPAr/DnKqzFFv8f+3s+MM3+/KxS6hSAiPwKawmMDcC3ReQ/sNL6/zEXnTQYDMOHEXAMBsNooMv1OeH6nsAapwJAo1JqvvdEpdRnbI3Oh4CtInKJT/kCfFQptTdpo3Wed70apZTaJyIXY/nxfENEfquUumMA/TIYDCOE8cExGAzDRQtQPJATlVLNwBsi8rdgraQuIhfan2cppWqVUrcBJ4Eqn7qeBv7RXhUdEbnIte9KERlnr/69FHhBRCYD7UqpnwB3AxcPpN0Gg2HkMAKOwWAYFmwz0Au2c/HdAyjiBuBTIrID2A18xN5+t4jstMt9EdgB/A6Yq52MgTVAGHhFRHbb3zV/An6JtQL4L5VSW4Dzsfx0tgNfB74xgPYaDIYRxKwmbjAY3rGIyM24nJENBsPYwWhwDAaDwWAwjDmMBsdgMBgMBsOYw2hwDAaDwWAwjDmMgGMwGAwGg2HMYQQcg8FgMBgMYw4j4BgMBoPBYBhzGAHHYDAYDAbDmOP/Bwf/mkEip5TOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_plotter.plot_results([log_dir], ts, results_plotter.X_TIMESTEPS, \"PPO BionicEnv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
