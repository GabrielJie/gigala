{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import gym\n",
    "from gym import spaces\n",
    "import time\n",
    "import random\n",
    "from stable_baselines3 import A2C, SAC,PPO\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from stable_baselines3.common import results_plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Element Model of the Space Frame Element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlaneTrussElementLength(x1,y1,z1,x2,y2,z2):\n",
    "    return math.sqrt((x2-x1)*(x2-x1) + (y2-y1)*(y2-y1)+(z2-z1)*(z2-z1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpaceFrameElementStiffness(E,G,A,Iy,Iz,J,x1,y1,z1,x2,y2,z2):\n",
    "    L = PlaneTrussElementLength(x1,y1,z1,x2,y2,z2)\n",
    "    w1 = E*A/L\n",
    "    w2 = 12*E*Iz/(L*L*L)\n",
    "    w3 = 6*E*Iz/(L*L)\n",
    "    w4 = 4*E*Iz/L\n",
    "    w5 = 2*E*Iz/L\n",
    "    w6 = 12*E*Iy/(L*L*L)\n",
    "    w7 = 6*E*Iy/(L*L)\n",
    "    w8 = 4*E*Iy/L\n",
    "    w9 = 2*E*Iy/L\n",
    "    w10 = G*J/L\n",
    "    \n",
    "    kprime = np.array([[w1, 0, 0, 0, 0, 0, -w1, 0, 0, 0, 0, 0],\n",
    "                        [0, w2, 0, 0, 0, w3, 0, -w2, 0, 0, 0, w3], \n",
    "                        [0, 0, w6, 0, -w7, 0, 0, 0, -w6, 0, -w7, 0],\n",
    "                        [0, 0, 0, w10, 0, 0, 0, 0, 0, -w10, 0, 0],\n",
    "                        [0, 0, -w7, 0, w8, 0, 0, 0, w7, 0, w9, 0],\n",
    "                        [0, w3, 0, 0, 0, w4, 0, -w3, 0, 0, 0, w5],\n",
    "                        [-w1, 0, 0, 0, 0, 0, w1, 0, 0, 0, 0, 0],\n",
    "                        [0, -w2, 0, 0, 0, -w3, 0, w2, 0, 0, 0, -w3],\n",
    "                        [0, 0, -w6, 0, w7, 0, 0, 0, w6, 0, w7, 0],\n",
    "                        [0, 0, 0, -w10, 0, 0, 0, 0, 0, w10, 0, 0],\n",
    "                        [0, 0, -w7, 0, w9, 0, 0, 0, w7, 0, w8, 0],\n",
    "                        [0, w3, 0, 0, 0, w5, 0, -w3, 0, 0, 0, w4]])  \n",
    "    \n",
    "    \n",
    "    if x1 == x2 and y1 == y2:\n",
    "        if z2 > z1:\n",
    "            Lambda = np.array([[0, 0, 1], [0, 1, 0], [-1, 0, 0]])\n",
    "        else:\n",
    "            Lambda = np.array([[0, 0, -1], [0, 1, 0], [1, 0, 0]])\n",
    "    else:\n",
    "        CXx = (x2-x1)/L\n",
    "        CYx = (y2-y1)/L\n",
    "        CZx = (z2-z1)/L\n",
    "        D = math.sqrt(CXx*CXx + CYx*CYx)\n",
    "        CXy = -CYx/D\n",
    "        CYy = CXx/D\n",
    "        CZy = 0\n",
    "        CXz = -CXx*CZx/D\n",
    "        CYz = -CYx*CZx/D\n",
    "        CZz = D\n",
    "        Lambda = np.array([[CXx, CYx, CZx], [CXy, CYy, CZy], [CXz, CYz, CZz]])\n",
    "        \n",
    "        \n",
    "    R = np.array([np.concatenate((np.concatenate((Lambda,np.zeros((3,3)),np.zeros((3,3)),np.zeros((3,3))),axis=1),\n",
    "        np.concatenate((np.zeros((3,3)), Lambda, np.zeros((3,3)), np.zeros((3,3))),axis=1) ,\n",
    "        np.concatenate((np.zeros((3,3)), np.zeros((3,3)), Lambda, np.zeros((3,3))),axis=1), \n",
    "        np.concatenate((np.zeros((3,3)), np.zeros((3,3)), np.zeros((3,3)), Lambda),axis=1)))])[0]\n",
    "    return np.dot(np.dot(R.T,kprime),R)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpaceFrameAssemble(K,k,i,j):\n",
    "    K[6*i,6*i] = K[6*i,6*i] + k[0,0]\n",
    "    K[6*i,6*i+1] = K[6*i,6*i+1] + k[0,1]\n",
    "    K[6*i,6*i+2] = K[6*i,6*i+2] + k[0,2]\n",
    "    K[6*i,6*i+3] = K[6*i,6*i+3] + k[0,3]\n",
    "    K[6*i,6*i+4] = K[6*i,6*i+4] + k[0,4]\n",
    "    K[6*i,6*i+5] = K[6*i,6*i+5] + k[0,5]\n",
    "    K[6*i,6*j] = K[6*i,6*j] + k[0,6]\n",
    "    K[6*i,6*j+1] = K[6*i,6*j+1] + k[0,7]\n",
    "    K[6*i,6*j+2] = K[6*i,6*j+2] + k[0,8]\n",
    "    K[6*i,6*j+3] = K[6*i,6*j+3] + k[0,9]\n",
    "    K[6*i,6*j+4] = K[6*i,6*j+4] + k[0,10]\n",
    "    K[6*i,6*j+5] = K[6*i,6*j+5] + k[0,11]\n",
    "    K[6*i+1,6*i] = K[6*i+1,6*i] + k[1,0]\n",
    "    K[6*i+1,6*i+1] = K[6*i+1,6*i+1] + k[1,1]\n",
    "    K[6*i+1,6*i+2] = K[6*i+1,6*i+2] + k[1,2]\n",
    "    K[6*i+1,6*i+3] = K[6*i+1,6*i+3] + k[1,3]\n",
    "    K[6*i+1,6*i+4] = K[6*i+1,6*i+4] + k[1,4]\n",
    "    K[6*i+1,6*i+5] = K[6*i+1,6*i+5] + k[1,5]\n",
    "    K[6*i+1,6*j] = K[6*i+1,6*j] + k[1,6]\n",
    "    K[6*i+1,6*j+1] = K[6*i+1,6*j+1] + k[1,7]\n",
    "    K[6*i+1,6*j+2] = K[6*i+1,6*j+2] + k[1,8]\n",
    "    K[6*i+1,6*j+3] = K[6*i+1,6*j+3] + k[1,9]\n",
    "    K[6*i+1,6*j+4] = K[6*i+1,6*j+4] + k[1,10]\n",
    "    K[6*i+1,6*j+5] = K[6*i+1,6*j+5] + k[1,11]\n",
    "    K[6*i+2,6*i]   = K[6*i+2,6*i] + k[2,0]\n",
    "    K[6*i+2,6*i+1] = K[6*i+2,6*i+1] + k[2,1]\n",
    "    K[6*i+2,6*i+2] = K[6*i+2,6*i+2] + k[2,2]\n",
    "    K[6*i+2,6*i+3] = K[6*i+2,6*i+3] + k[2,3]\n",
    "    K[6*i+2,6*i+4] = K[6*i+2,6*i+4] + k[2,4]\n",
    "    K[6*i+2,6*i+5] = K[6*i+2,6*i+5] + k[2,5]\n",
    "    K[6*i+2,6*j]   = K[6*i+2,6*j] + k[2,6]\n",
    "    K[6*i+2,6*j+1] = K[6*i+2,6*j+1] + k[2,7]\n",
    "    K[6*i+2,6*j+2] = K[6*i+2,6*j+2] + k[2,8]\n",
    "    K[6*i+2,6*j+3] = K[6*i+2,6*j+3] + k[2,9]\n",
    "    K[6*i+2,6*j+4] = K[6*i+2,6*j+4] + k[2,10]\n",
    "    K[6*i+2,6*j+5] = K[6*i+2,6*j+5] + k[2,11]\n",
    "    K[6*i+3,6*i] = K[6*i+3,6*i] + k[3,0]\n",
    "    K[6*i+3,6*i+1] = K[6*i+3,6*i+1] + k[3,1]\n",
    "    K[6*i+3,6*i+2] = K[6*i+3,6*i+2] + k[3,2]\n",
    "    K[6*i+3,6*i+3] = K[6*i+3,6*i+3] + k[3,3]\n",
    "    K[6*i+3,6*i+4] = K[6*i+3,6*i+4] + k[3,4]\n",
    "    K[6*i+3,6*i+5] = K[6*i+3,6*i+5] + k[3,5]\n",
    "    K[6*i+3,6*j] = K[6*i+3,6*j] + k[3,6]\n",
    "    K[6*i+3,6*j+1] = K[6*i+3,6*j+1] + k[3,7]\n",
    "    K[6*i+3,6*j+2] = K[6*i+3,6*j+2] + k[3,8]    \n",
    "    K[6*i+3,6*j+3] = K[6*i+3,6*j+3] + k[3,9]\n",
    "    K[6*i+3,6*j+4] = K[6*i+3,6*j+4] + k[3,10]\n",
    "    K[6*i+3,6*j+5] = K[6*i+3,6*j+5] + k[3,11]\n",
    "    K[6*i+4,6*i] = K[6*i+4,6*i] + k[4,0]\n",
    "    K[6*i+4,6*i+1] = K[6*i+4,6*i+1] + k[4,1]\n",
    "    K[6*i+4,6*i+2] = K[6*i+4,6*i+2] + k[4,2]\n",
    "    K[6*i+4,6*i+3] = K[6*i+4,6*i+3] + k[4,3]\n",
    "    K[6*i+4,6*i+4] = K[6*i+4,6*i+4] + k[4,4]\n",
    "    K[6*i+4,6*i+5] = K[6*i+4,6*i+5] + k[4,5]\n",
    "    K[6*i+4,6*j] = K[6*i+4,6*j] + k[4,6]\n",
    "    K[6*i+4,6*j+1] = K[6*i+4,6*j+1] + k[4,7]\n",
    "    K[6*i+4,6*j+2] = K[6*i+4,6*j+2] + k[4,8]\n",
    "    K[6*i+4,6*j+3] = K[6*i+4,6*j+3] + k[4,9]\n",
    "    K[6*i+4,6*j+4] = K[6*i+4,6*j+4] + k[4,10]\n",
    "    K[6*i+4,6*j+5] = K[6*i+4,6*j+5] + k[4,11]\n",
    "    K[6*i+5,6*i] = K[6*i+5,6*i] + k[5,0]\n",
    "    K[6*i+5,6*i+1] = K[6*i+5,6*i+1] + k[5,1]\n",
    "    K[6*i+5,6*i+2] = K[6*i+5,6*i+2] + k[5,2]\n",
    "    K[6*i+5,6*i+3] = K[6*i+5,6*i+3] + k[5,3]\n",
    "    K[6*i+5,6*i+4] = K[6*i+5,6*i+4] + k[5,4]\n",
    "    K[6*i+5,6*i+5] = K[6*i+5,6*i+5] + k[5,5]\n",
    "    K[6*i+5,6*j] = K[6*i+5,6*j] + k[5,6]\n",
    "    K[6*i+5,6*j+1] = K[6*i+5,6*j+1] + k[5,7]\n",
    "    K[6*i+5,6*j+2] = K[6*i+5,6*j+2] + k[5,8]\n",
    "    K[6*i+5,6*j+3] = K[6*i+5,6*j+3] + k[5,9]\n",
    "    K[6*i+5,6*j+4] = K[6*i+5,6*j+4] + k[5,10]\n",
    "    K[6*i+5,6*j+5] = K[6*i+5,6*j+5] + k[5,11]\n",
    "    K[6*j,6*i] = K[6*j,6*i] + k[6,0]\n",
    "    K[6*j,6*i+1] = K[6*j,6*i+1] + k[6,1]\n",
    "    K[6*j,6*i+2] = K[6*j,6*i+2] + k[6,2]\n",
    "    K[6*j,6*i+3] = K[6*j,6*i+3] + k[6,3]\n",
    "    K[6*j,6*i+4] = K[6*j,6*i+4] + k[6,4]\n",
    "    K[6*j,6*i+5] = K[6*j,6*i+5] + k[6,5]\n",
    "    K[6*j,6*j] = K[6*j,6*j] + k[6,6]\n",
    "    K[6*j,6*j+1] = K[6*j,6*j+1] + k[6,7]\n",
    "    K[6*j,6*j+2] = K[6*j,6*j+2] + k[6,8]\n",
    "    K[6*j,6*j+3] = K[6*j,6*j+3] + k[6,9]\n",
    "    K[6*j,6*j+4] = K[6*j,6*j+4] + k[6,10]\n",
    "    K[6*j,6*j+5] = K[6*j,6*j+5] + k[6,11]\n",
    "    K[6*j+1,6*i] = K[6*j+1,6*i] + k[7,0]\n",
    "    K[6*j+1,6*i+1] = K[6*j+1,6*i+1] + k[7,1]\n",
    "    K[6*j+1,6*i+2] = K[6*j+1,6*i+2] + k[7,2]\n",
    "    K[6*j+1,6*i+3] = K[6*j+1,6*i+3] + k[7,3]\n",
    "    K[6*j+1,6*i+4] = K[6*j+1,6*i+4] + k[7,4]\n",
    "    K[6*j+1,6*i+5] = K[6*j+1,6*i+5] + k[7,5]\n",
    "    K[6*j+1,6*j] = K[6*j+1,6*j] + k[7,6]\n",
    "    K[6*j+1,6*j+1] = K[6*j+1,6*j+1] + k[7,7]\n",
    "    K[6*j+1,6*j+2] = K[6*j+1,6*j+2] + k[7,8]\n",
    "    K[6*j+1,6*j+3] = K[6*j+1,6*j+3] + k[7,9]\n",
    "    K[6*j+1,6*j+4] = K[6*j+1,6*j+4] + k[7,10]\n",
    "    K[6*j+1,6*j+5] = K[6*j+1,6*j+5] + k[7,11]\n",
    "    K[6*j+2,6*i] = K[6*j+2,6*i] + k[8,0]\n",
    "    K[6*j+2,6*i+1] = K[6*j+2,6*i+1] + k[8,1]\n",
    "    K[6*j+2,6*i+2] = K[6*j+2,6*i+2] + k[8,2]\n",
    "    K[6*j+2,6*i+3] = K[6*j+2,6*i+3] + k[8,3]\n",
    "    K[6*j+2,6*i+4] = K[6*j+2,6*i+4] + k[8,4]\n",
    "    K[6*j+2,6*i+5] = K[6*j+2,6*i+5] + k[8,5]\n",
    "    K[6*j+2,6*j] = K[6*j+2,6*j] + k[8,6]\n",
    "    K[6*j+2,6*j+1] = K[6*j+2,6*j+1] + k[8,7]\n",
    "    K[6*j+2,6*j+2] = K[6*j+2,6*j+2] + k[8,8]\n",
    "    K[6*j+2,6*j+3] = K[6*j+2,6*j+3] + k[8,9]\n",
    "    K[6*j+2,6*j+4] = K[6*j+2,6*j+4] + k[8,10]\n",
    "    K[6*j+2,6*j+5] = K[6*j+2,6*j+5] + k[8,11]\n",
    "    K[6*j+3,6*i] = K[6*j+3,6*i] + k[9,0]\n",
    "    K[6*j+3,6*i+1] = K[6*j+3,6*i+1] + k[9,1]\n",
    "    K[6*j+3,6*i+2] = K[6*j+3,6*i+2] + k[9,2]\n",
    "    K[6*j+3,6*i+3] = K[6*j+3,6*i+3] + k[9,3]\n",
    "    K[6*j+3,6*i+4] = K[6*j+3,6*i+4] + k[9,4]\n",
    "    K[6*j+3,6*i+5] = K[6*j+3,6*i+5] + k[9,5]\n",
    "    K[6*j+3,6*j] = K[6*j+3,6*j] + k[9,6]\n",
    "    K[6*j+3,6*j+1] = K[6*j+3,6*j+1] + k[9,7]\n",
    "    K[6*j+3,6*j+2] = K[6*j+3,6*j+2] + k[9,8]\n",
    "    K[6*j+3,6*j+3] = K[6*j+3,6*j+3] + k[9,9]\n",
    "    K[6*j+3,6*j+4] = K[6*j+3,6*j+4] + k[9,10]\n",
    "    K[6*j+3,6*j+5] = K[6*j+3,6*j+5] + k[9,11]\n",
    "    K[6*j+4,6*i] = K[6*j+4,6*i] + k[10,0]\n",
    "    K[6*j+4,6*i+1] = K[6*j+4,6*i+1] + k[10,1]\n",
    "    K[6*j+4,6*i+2] = K[6*j+4,6*i+2] + k[10,2]\n",
    "    K[6*j+4,6*i+3] = K[6*j+4,6*i+3] + k[10,3]\n",
    "    K[6*j+4,6*i+4] = K[6*j+4,6*i+4] + k[10,4]\n",
    "    K[6*j+4,6*i+5] = K[6*j+4,6*i+5] + k[10,5]\n",
    "    K[6*j+4,6*j] = K[6*j+4,6*j] + k[10,6]\n",
    "    K[6*j+4,6*j+1] = K[6*j+4,6*j+1] + k[10,7]\n",
    "    K[6*j+4,6*j+2] = K[6*j+4,6*j+2] + k[10,8]\n",
    "    K[6*j+4,6*j+3] = K[6*j+4,6*j+3] + k[10,9]\n",
    "    K[6*j+4,6*j+4] = K[6*j+4,6*j+4] + k[10,10]\n",
    "    K[6*j+4,6*j+5] = K[6*j+4,6*j+5] + k[10,11]\n",
    "    K[6*j+5,6*i] = K[6*j+5,6*i] + k[11,0]\n",
    "    K[6*j+5,6*i+1] = K[6*j+5,6*i+1] + k[11,1]\n",
    "    K[6*j+5,6*i+2] = K[6*j+5,6*i+2] + k[11,2]\n",
    "    K[6*j+5,6*i+3] = K[6*j+5,6*i+3] + k[11,3]\n",
    "    K[6*j+5,6*i+4] = K[6*j+5,6*i+4] + k[11,4]\n",
    "    K[6*j+5,6*i+5] = K[6*j+5,6*i+5] + k[11,5]\n",
    "    K[6*j+5,6*j] = K[6*j+5,6*j] + k[11,6]\n",
    "    K[6*j+5,6*j+1] = K[6*j+5,6*j+1] + k[11,7]\n",
    "    K[6*j+5,6*j+2] = K[6*j+5,6*j+2] + k[11,8]\n",
    "    K[6*j+5,6*j+3] = K[6*j+5,6*j+3] + k[11,9]\n",
    "    K[6*j+5,6*j+4] = K[6*j+5,6*j+4] + k[11,10]\n",
    "    K[6*j+5,6*j+5] = K[6*j+5,6*j+5] + k[11,11]\n",
    "    \n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FEA_u(coord, elcon, bc_node, bc_val, global_force, \n",
    "          E=210e6, G=84e6, A=2e-2, Iy=10e-5, Iz=20e-5, J=5e-5):\n",
    "    coord=np.array(coord)\n",
    "    elcon=np.array(elcon)\n",
    "    \n",
    "    K=np.zeros(shape=(6*(np.max(elcon)+1),6*(np.max(elcon)+1)))\n",
    "    for el in elcon:\n",
    "        k=SpaceFrameElementStiffness(E,G,A,Iy,Iz,J,\n",
    "                                     coord[el[0]][0],coord[el[0]][1],coord[el[0]][2],\\\n",
    "                                     coord[el[1]][0],coord[el[1]][1],coord[el[1]][2])\n",
    "        K=SpaceFrameAssemble(K,k,el[0],el[1])\n",
    "        \n",
    "    F = np.array(global_force)\n",
    "    \n",
    "    \n",
    "    # https://github.com/CALFEM/calfem-matlab/blob/master/fem/solveq.m\n",
    "    \n",
    "    bc=np.array([bc_node, \n",
    "                bc_val]).T\n",
    "    nd, nd=K.shape\n",
    "    fdof=np.array([i for i in range(nd)]).T\n",
    "    d=np.zeros(shape=(len(fdof),))\n",
    "    Q=np.zeros(shape=(len(fdof),))\n",
    "\n",
    "    pdof=bc[:,0].astype(int)\n",
    "    dp=bc[:,1]\n",
    "    fdof=np.delete(fdof, pdof, 0)\n",
    "    s=np.linalg.lstsq(K[fdof,:][:,fdof], (F[fdof].T-np.dot(K[fdof,:][:,pdof],dp.T)).T, rcond=None)[0] \n",
    "    d[pdof]=dp\n",
    "    d[fdof]=s.reshape(-1,)\n",
    "    \n",
    "#     Q=np.dot(K,d).T-F \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 The Space Frame Element - verification\n",
    "d=FEA_u(np.array([0,0,0,\n",
    "                  3,0,0,\n",
    "                  0,0,-3,\n",
    "                  0,-4,0]).reshape(4,3),\n",
    "        elcon=np.array([[0, 1],\n",
    "                      [0, 2],\n",
    "                      [0, 3]]),\n",
    "        bc_node=list(range(6,24)), \n",
    "        bc_val=[0]*18,\n",
    "        global_force=[-10,0,20,0,0,0,\n",
    "                                0,0,0,0,0,0,\n",
    "                                0,0,0,0,0,0,\n",
    "                                0,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.05147750e-06, -6.65367100e-08,  1.41769582e-05,  1.44778793e-06,\n",
       "        1.74858422e-06,  1.13605431e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_length(coord,elcon):\n",
    "    coord=np.array(coord)\n",
    "    elcon=np.array(elcon)\n",
    "    t_length=0\n",
    "    for i in range(len(elcon)):\n",
    "        l=PlaneTrussElementLength(coord[elcon[i][0]][0],\\\n",
    "                                    coord[elcon[i][0]][1],\\\n",
    "                                    coord[elcon[i][0]][2],\\\n",
    "                                    coord[elcon[i][1]][0],\\\n",
    "                                    coord[elcon[i][1]][1],\\\n",
    "                                    coord[elcon[i][1]][2])\n",
    "        t_length+=l        \n",
    "    return t_length    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possible_lines_dic(n,m,dx,dy):\n",
    "    A=[(-dx,0),(-dx,dy),(0,dy),(dx,dy),(dx,0),(dx,-dy),(0,-dy),(-dx,-dy)]\n",
    "    dic={}\n",
    "    t=0\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            for item in A:\n",
    "                x,y=j*dx,i*dy\n",
    "                x1,y1=x+item[0],y+item[1]\n",
    "                if (x1>=0 and x1<=(m-1)*dx and \n",
    "                    y1>=0 and y1<=(n-1)*dy and \n",
    "                    (x1,y1,x,y) not in dic):\n",
    "                    dic[(x,y,x1,y1)]=t\n",
    "                    t+=1\n",
    "    return dic                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = len(possible_lines_dic(n=5,m=5,dx=1,dy=1)) + 3 # +2 for x and y +1 for action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,n=5,m=5,dx=1,dy=1, force=-500,\n",
    "                 E=210e6, G=84e6, A=2e-2, Iy=10e-5, Iz=20e-5, J=5e-5, break_flag=False):\n",
    "        # n,m,dx,dy - grid parameters    \n",
    "        self.E=E\n",
    "        self.G=G\n",
    "        self.A=A\n",
    "        self.Iy=Iy\n",
    "        self.Iz=Iz\n",
    "        self.J=J\n",
    "        self.n=n\n",
    "        self.m=m\n",
    "        self.dx=dx\n",
    "        self.dy=dy\n",
    "        self.dic_lines=possible_lines_dic(self.n,self.m,self.dx,self.dy)\n",
    "        self.line_list=len(self.dic_lines)*[0]\n",
    "        self.break_flag=break_flag\n",
    "        self.coord=[[2,2,0]]    \n",
    "        self.elcon=[]  \n",
    "        self.el_dic={(2,2):0}\n",
    "        self.max_el=0\n",
    "        self.force=force\n",
    "        self.bc_node = []\n",
    "        self.bc_val = []\n",
    "        self.global_force = [0, self.force, 0, 0, 0, 0]\n",
    "        self.old_weight=float(\"inf\")\n",
    "        self.old_strength=-float(\"inf\")\n",
    "        self.visit_list = [0,0,0,0] # number of checkpoints is 4\n",
    "    \n",
    "    def reset(self,break_flag,force):\n",
    "        self.dic_lines=possible_lines_dic(self.n, self.m, self.dx, self.dy)\n",
    "        self.line_list=len(self.dic_lines)*[0]\n",
    "        self.break_flag=break_flag\n",
    "        self.coord=[[2,2,0]]    \n",
    "        self.elcon=[]  \n",
    "        self.el_dic={(2,2):0}\n",
    "        self.max_el=0\n",
    "        self.force=force\n",
    "        self.bc_node = []\n",
    "        self.bc_val = []\n",
    "        self.global_force = [0, self.force, 0, 0, 0, 0]\n",
    "        self.visit_list = [0,0,0,0] # number of checkpoints is 4\n",
    "    \n",
    "    def FEA(self):\n",
    "        return FEA_u(self.coord, \n",
    "                     self.elcon, \n",
    "                     self.bc_node, \n",
    "                     self.bc_val, \n",
    "                     self.global_force, )\n",
    "        \n",
    "    def max_u(self, FEA_output_arr):\n",
    "        t=1\n",
    "        A=[]\n",
    "        while t<len(FEA_output_arr):\n",
    "            A.append(FEA_output_arr[t])\n",
    "            t+=6            \n",
    "        return min(A)    \n",
    "            \n",
    "    \n",
    "    def length(self):\n",
    "        return total_length(self.coord,self.elcon)\n",
    "    \n",
    "    \n",
    "    def move_w(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x-self.dx\n",
    "        y_new=y\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "            \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "            \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1\n",
    "            \n",
    "        return x_new, y_new\n",
    "            \n",
    "    def move_nw(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x-self.dx\n",
    "        y_new=y+self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "            \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]]) \n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1    \n",
    "                \n",
    "        return x_new, y_new\n",
    "    \n",
    "    def move_n(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x\n",
    "        y_new=y+self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "            \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                \n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "                  \n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1     \n",
    "        \n",
    "        return x_new, y_new\n",
    "    \n",
    "    \n",
    "    def move_ne(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x+self.dx\n",
    "        y_new=y+self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "                        \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1     \n",
    "                  \n",
    "        return x_new, y_new\n",
    "    \n",
    "    def move_e(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x+self.dx\n",
    "        y_new=y\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "                       \n",
    "                  \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])   \n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1      \n",
    "        return x_new, y_new\n",
    "    \n",
    "    def move_se(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x+self.dx\n",
    "        y_new=y-self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "            \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1      \n",
    "        \n",
    "        return x_new, y_new\n",
    "    \n",
    "    def move_s(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x\n",
    "        y_new=y-self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True \n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "            \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1      \n",
    "        \n",
    "        return x_new, y_new\n",
    "    \n",
    "    def move_sw(self,x,y):\n",
    "        # x,y - current location\n",
    "        x_new=x-self.dx\n",
    "        y_new=y-self.dy\n",
    "        if x_new<0 or x_new>(self.m-1)*self.dx or y_new<0 or y_new>(self.n-1)*self.dy \\\n",
    "        or 3 in self.line_list:\n",
    "            self.break_flag=True\n",
    "        else:\n",
    "            try:\n",
    "                self.line_list[self.dic_lines[(x,y,x_new,y_new)]]+=1\n",
    "            except KeyError:\n",
    "                self.line_list[self.dic_lines[(x_new,y_new, x,y)]]+=1\n",
    "                        \n",
    "            if (x_new,y_new) not in self.el_dic:\n",
    "                self.max_el+=1\n",
    "                self.el_dic[(x_new,y_new)]=self.max_el\n",
    "                self.coord.append([x_new,y_new,0])\n",
    "                if (x_new,y_new)!=(1,1) and (x_new,y_new)!=(self.m-2,1) and \\\n",
    "                    (x_new,y_new)!=(self.m-2,self.n-2) and \\\n",
    "                    (x_new,y_new)!=(1,self.n-2):\n",
    "                    self.global_force += [0,self.force,0,0,0,0]\n",
    "                elif (x_new,y_new)==(1,1) or (x_new,y_new)==(self.m-2,1) or \\\n",
    "                    (x_new,y_new)==(self.m-2,self.n-2) or \\\n",
    "                    (x_new,y_new)==(1,self.n-2):\n",
    "                    range_ = list(range(self.el_dic[(x_new,y_new)]*6-6,self.el_dic[(x_new,y_new)]*6))\n",
    "                    self.bc_node += range_\n",
    "                    self.bc_val += [0]*len(range_)\n",
    "                    self.global_force += [0,0,0,0,0,0]\n",
    "                        \n",
    "            if  (self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]) not in self.elcon and \\\n",
    "                 (self.el_dic[(x_new,y_new)],self.el_dic[(x,y)]) not in self.elcon:\n",
    "                self.elcon.append([self.el_dic[(x,y)], self.el_dic[(x_new,y_new)]])\n",
    "                  \n",
    "                  \n",
    "            if (x_new,y_new) in self.el_dic:\n",
    "                if (x_new,y_new)==(1,1):\n",
    "                    self.visit_list[0]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,1):\n",
    "                    self.visit_list[1]+=1\n",
    "                elif (x_new,y_new)==(self.m-2,self.n-2):\n",
    "                    self.visit_list[2]+=1\n",
    "                elif (x_new,y_new)==(1,self.n-2):\n",
    "                    self.visit_list[3]+=1    \n",
    "                                  \n",
    "        return x_new, y_new\n",
    "    \n",
    "        \n",
    "    def action_space(self,action,x0,y0):\n",
    "        if action==0:\n",
    "            return self.move_w(x0,y0)\n",
    "        elif action==1:    \n",
    "            return self.move_nw(x0,y0)\n",
    "        elif action==2:  \n",
    "            return self.move_n(x0,y0)\n",
    "        elif action==3:\n",
    "            return self.move_ne(x0,y0)\n",
    "        elif action==4:\n",
    "            return self.move_e(x0,y0)\n",
    "        elif action==5:\n",
    "            return self.move_se(x0,y0)\n",
    "        elif action==6:\n",
    "            return self.move_s(x0,y0)\n",
    "        elif action==7:\n",
    "            return self.move_sw(x0,y0)\n",
    "                        \n",
    "    \n",
    "    def nn_input(self,x,y,action):  \n",
    "        return self.line_list+[x,y]+[action]        \n",
    "    \n",
    "    def reward_(self,x_new,y_new,n_steps):\n",
    "        reward=2*n_steps\n",
    "        if all([x>=1 for x in self.visit_list]):\n",
    "            reward+=100\n",
    "            weight=self.length()\n",
    "        \n",
    "            FEA_output_arr=self.FEA()\n",
    "            max_=self.max_u(FEA_output_arr)\n",
    "            strength=max_\n",
    "            if weight<=self.old_weight:\n",
    "                reward+=500\n",
    "                self.old_weight=weight\n",
    "            if strength>=self.old_strength: \n",
    "                reward+=1000\n",
    "                self.old_strength=strength        \n",
    "            self.break_flag=True     \n",
    "            return reward \n",
    "        return reward     \n",
    "                                   \n",
    "    def draw(self,color):\n",
    "        c=self.coord\n",
    "        e=self.elcon\n",
    "        c=np.array(c)\n",
    "        e=np.array(e)\n",
    "        coord=c.reshape(np.max(e)+1,3)\n",
    "        fig=plt.figure(figsize=(13,5))\n",
    "        for item in e:\n",
    "            ax = fig.gca(projection='3d') \n",
    "            ax.plot([coord[item[0]][0],coord[item[1]][0]],\\\n",
    "                     [coord[item[0]][1],coord[item[1]][1]],\\\n",
    "                     [coord[item[0]][2],coord[item[1]][2]],\n",
    "                     color=color) \n",
    "        ax.view_init(-90,90)\n",
    "        ax.set_xlim([0, 5])\n",
    "        ax.set_ylim([0, 5])\n",
    "        plt.show()             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DISCRETE_ACTIONS=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BionicEnv(gym.Env):\n",
    "    \n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n",
    "        self.M=Model()\n",
    "        self.M.reset(False,-500)\n",
    "        self.x0=2\n",
    "        self.y0=2\n",
    "        self.obs=self.M.nn_input(self.x0,self.y0, 0)\n",
    "        self.observation_space = spaces.Box(low=np.array([-1e10 for x in range(DIM)]),\n",
    "                                            high=np.array([1e10 for y in range(DIM)]),\n",
    "                                            shape=(DIM,),\n",
    "                                           dtype=np.int64)\n",
    "        self.step_=0\n",
    "        self.needs_reset = True\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        x_new, y_new = self.x0, self.y0 \n",
    "        x_new, y_new = self.M.action_space(action, x_new, y_new)\n",
    "        self.obs=self.M.nn_input(x_new,y_new,action)\n",
    "                \n",
    "        self.step_+=1           \n",
    "        reward=self.M.reward_(x_new,y_new,self.step_)\n",
    "        self.x0,self.y0 = x_new,y_new\n",
    "        \n",
    "        done=False\n",
    "        if self.M.break_flag:\n",
    "            reward-=100\n",
    "            done=True\n",
    "        \n",
    "        if self.needs_reset:\n",
    "            raise RuntimeError(\"Tried to step environment that needs reset\")\n",
    "            \n",
    "        if done:\n",
    "            self.needs_reset = True\n",
    "      \n",
    "        return np.array(self.obs), reward, done, dict()\n",
    "\n",
    "    def reset(self):\n",
    "        self.M.reset(False,-500)\n",
    "        self.x0=2\n",
    "        self.y0=2\n",
    "        self.obs=self.M.nn_input(self.x0,self.y0,0)\n",
    "        self.step_=0\n",
    "        self.needs_reset = False\n",
    "        return np.array(self.obs)  \n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        self.M.draw('blue')    \n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(\n",
    "                        f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\"\n",
    "                    )\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model to {self.save_path}.zip\")\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log dir\n",
    "log_dir = \"/tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = BionicEnv()\n",
    "# Logs will be saved in log_dir/monitor.csv\n",
    "env = Monitor(env, log_dir)\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=5e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: -30.68\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -30.68 - Last mean reward per episode: -38.40\n",
      "Num timesteps: 3000\n",
      "Best mean reward: -30.68 - Last mean reward per episode: -6.12\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 4000\n",
      "Best mean reward: -6.12 - Last mean reward per episode: -0.44\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 5000\n",
      "Best mean reward: -0.44 - Last mean reward per episode: 39.54\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 6000\n",
      "Best mean reward: 39.54 - Last mean reward per episode: 17.04\n",
      "Num timesteps: 7000\n",
      "Best mean reward: 39.54 - Last mean reward per episode: 39.82\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 8000\n",
      "Best mean reward: 39.82 - Last mean reward per episode: 38.12\n",
      "Num timesteps: 9000\n",
      "Best mean reward: 39.82 - Last mean reward per episode: 78.62\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 10000\n",
      "Best mean reward: 78.62 - Last mean reward per episode: 127.26\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 11000\n",
      "Best mean reward: 127.26 - Last mean reward per episode: 93.70\n",
      "Num timesteps: 12000\n",
      "Best mean reward: 127.26 - Last mean reward per episode: 115.60\n",
      "Num timesteps: 13000\n",
      "Best mean reward: 127.26 - Last mean reward per episode: 154.22\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 14000\n",
      "Best mean reward: 154.22 - Last mean reward per episode: 142.08\n",
      "Num timesteps: 15000\n",
      "Best mean reward: 154.22 - Last mean reward per episode: 136.62\n",
      "Num timesteps: 16000\n",
      "Best mean reward: 154.22 - Last mean reward per episode: 159.46\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 17000\n",
      "Best mean reward: 159.46 - Last mean reward per episode: 153.70\n",
      "Num timesteps: 18000\n",
      "Best mean reward: 159.46 - Last mean reward per episode: 134.52\n",
      "Num timesteps: 19000\n",
      "Best mean reward: 159.46 - Last mean reward per episode: 115.64\n",
      "Num timesteps: 20000\n",
      "Best mean reward: 159.46 - Last mean reward per episode: 132.70\n",
      "Num timesteps: 21000\n",
      "Best mean reward: 159.46 - Last mean reward per episode: 155.68\n",
      "Num timesteps: 22000\n",
      "Best mean reward: 159.46 - Last mean reward per episode: 142.32\n",
      "Num timesteps: 23000\n",
      "Best mean reward: 159.46 - Last mean reward per episode: 158.82\n",
      "Num timesteps: 24000\n",
      "Best mean reward: 159.46 - Last mean reward per episode: 184.60\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 25000\n",
      "Best mean reward: 184.60 - Last mean reward per episode: 214.66\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 26000\n",
      "Best mean reward: 214.66 - Last mean reward per episode: 201.16\n",
      "Num timesteps: 27000\n",
      "Best mean reward: 214.66 - Last mean reward per episode: 230.84\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 28000\n",
      "Best mean reward: 230.84 - Last mean reward per episode: 233.54\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 29000\n",
      "Best mean reward: 233.54 - Last mean reward per episode: 245.56\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 30000\n",
      "Best mean reward: 245.56 - Last mean reward per episode: 241.28\n",
      "Num timesteps: 31000\n",
      "Best mean reward: 245.56 - Last mean reward per episode: 214.42\n",
      "Num timesteps: 32000\n",
      "Best mean reward: 245.56 - Last mean reward per episode: 241.02\n",
      "Num timesteps: 33000\n",
      "Best mean reward: 245.56 - Last mean reward per episode: 196.42\n",
      "Num timesteps: 34000\n",
      "Best mean reward: 245.56 - Last mean reward per episode: 194.32\n",
      "Num timesteps: 35000\n",
      "Best mean reward: 245.56 - Last mean reward per episode: 221.88\n",
      "Num timesteps: 36000\n",
      "Best mean reward: 245.56 - Last mean reward per episode: 243.46\n",
      "Num timesteps: 37000\n",
      "Best mean reward: 245.56 - Last mean reward per episode: 258.06\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 38000\n",
      "Best mean reward: 258.06 - Last mean reward per episode: 190.64\n",
      "Num timesteps: 39000\n",
      "Best mean reward: 258.06 - Last mean reward per episode: 250.34\n",
      "Num timesteps: 40000\n",
      "Best mean reward: 258.06 - Last mean reward per episode: 273.02\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 41000\n",
      "Best mean reward: 273.02 - Last mean reward per episode: 250.24\n",
      "Num timesteps: 42000\n",
      "Best mean reward: 273.02 - Last mean reward per episode: 225.52\n",
      "Num timesteps: 43000\n",
      "Best mean reward: 273.02 - Last mean reward per episode: 273.24\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 44000\n",
      "Best mean reward: 273.24 - Last mean reward per episode: 249.92\n",
      "Num timesteps: 45000\n",
      "Best mean reward: 273.24 - Last mean reward per episode: 196.64\n",
      "Num timesteps: 46000\n",
      "Best mean reward: 273.24 - Last mean reward per episode: 255.48\n",
      "Num timesteps: 47000\n",
      "Best mean reward: 273.24 - Last mean reward per episode: 232.74\n",
      "Num timesteps: 48000\n",
      "Best mean reward: 273.24 - Last mean reward per episode: 256.60\n",
      "Num timesteps: 49000\n",
      "Best mean reward: 273.24 - Last mean reward per episode: 231.82\n",
      "Num timesteps: 50000\n",
      "Best mean reward: 273.24 - Last mean reward per episode: 215.34\n",
      "Num timesteps: 51000\n",
      "Best mean reward: 273.24 - Last mean reward per episode: 225.92\n",
      "Num timesteps: 52000\n",
      "Best mean reward: 273.24 - Last mean reward per episode: 268.06\n",
      "Num timesteps: 53000\n",
      "Best mean reward: 273.24 - Last mean reward per episode: 285.22\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 54000\n",
      "Best mean reward: 285.22 - Last mean reward per episode: 295.48\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 55000\n",
      "Best mean reward: 295.48 - Last mean reward per episode: 236.20\n",
      "Num timesteps: 56000\n",
      "Best mean reward: 295.48 - Last mean reward per episode: 288.44\n",
      "Num timesteps: 57000\n",
      "Best mean reward: 295.48 - Last mean reward per episode: 302.00\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 58000\n",
      "Best mean reward: 302.00 - Last mean reward per episode: 301.26\n",
      "Num timesteps: 59000\n",
      "Best mean reward: 302.00 - Last mean reward per episode: 293.60\n",
      "Num timesteps: 60000\n",
      "Best mean reward: 302.00 - Last mean reward per episode: 274.00\n",
      "Num timesteps: 61000\n",
      "Best mean reward: 302.00 - Last mean reward per episode: 295.80\n",
      "Num timesteps: 62000\n",
      "Best mean reward: 302.00 - Last mean reward per episode: 268.68\n",
      "Num timesteps: 63000\n",
      "Best mean reward: 302.00 - Last mean reward per episode: 306.46\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 64000\n",
      "Best mean reward: 306.46 - Last mean reward per episode: 302.44\n",
      "Num timesteps: 65000\n",
      "Best mean reward: 306.46 - Last mean reward per episode: 304.26\n",
      "Num timesteps: 66000\n",
      "Best mean reward: 306.46 - Last mean reward per episode: 284.96\n",
      "Num timesteps: 67000\n",
      "Best mean reward: 306.46 - Last mean reward per episode: 311.36\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 68000\n",
      "Best mean reward: 311.36 - Last mean reward per episode: 291.54\n",
      "Num timesteps: 69000\n",
      "Best mean reward: 311.36 - Last mean reward per episode: 297.16\n",
      "Num timesteps: 70000\n",
      "Best mean reward: 311.36 - Last mean reward per episode: 281.42\n",
      "Num timesteps: 71000\n",
      "Best mean reward: 311.36 - Last mean reward per episode: 315.46\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 72000\n",
      "Best mean reward: 315.46 - Last mean reward per episode: 355.04\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 73000\n",
      "Best mean reward: 355.04 - Last mean reward per episode: 352.12\n",
      "Num timesteps: 74000\n",
      "Best mean reward: 355.04 - Last mean reward per episode: 301.12\n",
      "Num timesteps: 75000\n",
      "Best mean reward: 355.04 - Last mean reward per episode: 284.76\n",
      "Num timesteps: 76000\n",
      "Best mean reward: 355.04 - Last mean reward per episode: 291.64\n",
      "Num timesteps: 77000\n",
      "Best mean reward: 355.04 - Last mean reward per episode: 278.40\n",
      "Num timesteps: 78000\n",
      "Best mean reward: 355.04 - Last mean reward per episode: 304.28\n",
      "Num timesteps: 79000\n",
      "Best mean reward: 355.04 - Last mean reward per episode: 340.90\n",
      "Num timesteps: 80000\n",
      "Best mean reward: 355.04 - Last mean reward per episode: 324.64\n",
      "Num timesteps: 81000\n",
      "Best mean reward: 355.04 - Last mean reward per episode: 313.90\n",
      "Num timesteps: 82000\n",
      "Best mean reward: 355.04 - Last mean reward per episode: 317.02\n",
      "Num timesteps: 83000\n",
      "Best mean reward: 355.04 - Last mean reward per episode: 328.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 84000\n",
      "Best mean reward: 355.04 - Last mean reward per episode: 285.28\n",
      "Num timesteps: 85000\n",
      "Best mean reward: 355.04 - Last mean reward per episode: 291.26\n",
      "Num timesteps: 86000\n",
      "Best mean reward: 355.04 - Last mean reward per episode: 300.34\n",
      "Num timesteps: 87000\n",
      "Best mean reward: 355.04 - Last mean reward per episode: 347.40\n",
      "Num timesteps: 88000\n",
      "Best mean reward: 355.04 - Last mean reward per episode: 336.98\n",
      "Num timesteps: 89000\n",
      "Best mean reward: 355.04 - Last mean reward per episode: 358.92\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 90000\n",
      "Best mean reward: 358.92 - Last mean reward per episode: 332.38\n",
      "Num timesteps: 91000\n",
      "Best mean reward: 358.92 - Last mean reward per episode: 286.06\n",
      "Num timesteps: 92000\n",
      "Best mean reward: 358.92 - Last mean reward per episode: 338.38\n",
      "Num timesteps: 93000\n",
      "Best mean reward: 358.92 - Last mean reward per episode: 377.64\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 94000\n",
      "Best mean reward: 377.64 - Last mean reward per episode: 350.92\n",
      "Num timesteps: 95000\n",
      "Best mean reward: 377.64 - Last mean reward per episode: 368.00\n",
      "Num timesteps: 96000\n",
      "Best mean reward: 377.64 - Last mean reward per episode: 391.90\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 97000\n",
      "Best mean reward: 391.90 - Last mean reward per episode: 385.44\n",
      "Num timesteps: 98000\n",
      "Best mean reward: 391.90 - Last mean reward per episode: 324.08\n",
      "Num timesteps: 99000\n",
      "Best mean reward: 391.90 - Last mean reward per episode: 322.66\n",
      "Num timesteps: 100000\n",
      "Best mean reward: 391.90 - Last mean reward per episode: 337.40\n",
      "Num timesteps: 101000\n",
      "Best mean reward: 391.90 - Last mean reward per episode: 332.02\n",
      "Num timesteps: 102000\n",
      "Best mean reward: 391.90 - Last mean reward per episode: 333.52\n",
      "Num timesteps: 103000\n",
      "Best mean reward: 391.90 - Last mean reward per episode: 342.08\n",
      "Num timesteps: 104000\n",
      "Best mean reward: 391.90 - Last mean reward per episode: 396.02\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 105000\n",
      "Best mean reward: 396.02 - Last mean reward per episode: 376.08\n",
      "Num timesteps: 106000\n",
      "Best mean reward: 396.02 - Last mean reward per episode: 342.22\n",
      "Num timesteps: 107000\n",
      "Best mean reward: 396.02 - Last mean reward per episode: 329.48\n",
      "Num timesteps: 108000\n",
      "Best mean reward: 396.02 - Last mean reward per episode: 352.32\n",
      "Num timesteps: 109000\n",
      "Best mean reward: 396.02 - Last mean reward per episode: 361.18\n",
      "Num timesteps: 110000\n",
      "Best mean reward: 396.02 - Last mean reward per episode: 334.70\n",
      "Num timesteps: 111000\n",
      "Best mean reward: 396.02 - Last mean reward per episode: 303.06\n",
      "Num timesteps: 112000\n",
      "Best mean reward: 396.02 - Last mean reward per episode: 346.22\n",
      "Num timesteps: 113000\n",
      "Best mean reward: 396.02 - Last mean reward per episode: 375.00\n",
      "Num timesteps: 114000\n",
      "Best mean reward: 396.02 - Last mean reward per episode: 384.74\n",
      "Num timesteps: 115000\n",
      "Best mean reward: 396.02 - Last mean reward per episode: 343.74\n",
      "Num timesteps: 116000\n",
      "Best mean reward: 396.02 - Last mean reward per episode: 350.58\n",
      "Num timesteps: 117000\n",
      "Best mean reward: 396.02 - Last mean reward per episode: 377.64\n",
      "Num timesteps: 118000\n",
      "Best mean reward: 396.02 - Last mean reward per episode: 379.44\n",
      "Num timesteps: 119000\n",
      "Best mean reward: 396.02 - Last mean reward per episode: 361.46\n",
      "Num timesteps: 120000\n",
      "Best mean reward: 396.02 - Last mean reward per episode: 391.98\n",
      "Num timesteps: 121000\n",
      "Best mean reward: 396.02 - Last mean reward per episode: 432.68\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 122000\n",
      "Best mean reward: 432.68 - Last mean reward per episode: 408.08\n",
      "Num timesteps: 123000\n",
      "Best mean reward: 432.68 - Last mean reward per episode: 388.50\n",
      "Num timesteps: 124000\n",
      "Best mean reward: 432.68 - Last mean reward per episode: 397.40\n",
      "Num timesteps: 125000\n",
      "Best mean reward: 432.68 - Last mean reward per episode: 431.68\n",
      "Num timesteps: 126000\n",
      "Best mean reward: 432.68 - Last mean reward per episode: 455.54\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 127000\n",
      "Best mean reward: 455.54 - Last mean reward per episode: 390.10\n",
      "Num timesteps: 128000\n",
      "Best mean reward: 455.54 - Last mean reward per episode: 400.82\n",
      "Num timesteps: 129000\n",
      "Best mean reward: 455.54 - Last mean reward per episode: 432.30\n",
      "Num timesteps: 130000\n",
      "Best mean reward: 455.54 - Last mean reward per episode: 432.48\n",
      "Num timesteps: 131000\n",
      "Best mean reward: 455.54 - Last mean reward per episode: 437.62\n",
      "Num timesteps: 132000\n",
      "Best mean reward: 455.54 - Last mean reward per episode: 411.78\n",
      "Num timesteps: 133000\n",
      "Best mean reward: 455.54 - Last mean reward per episode: 411.16\n",
      "Num timesteps: 134000\n",
      "Best mean reward: 455.54 - Last mean reward per episode: 431.46\n",
      "Num timesteps: 135000\n",
      "Best mean reward: 455.54 - Last mean reward per episode: 445.48\n",
      "Num timesteps: 136000\n",
      "Best mean reward: 455.54 - Last mean reward per episode: 432.94\n",
      "Num timesteps: 137000\n",
      "Best mean reward: 455.54 - Last mean reward per episode: 450.24\n",
      "Num timesteps: 138000\n",
      "Best mean reward: 455.54 - Last mean reward per episode: 556.54\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 139000\n",
      "Best mean reward: 556.54 - Last mean reward per episode: 549.60\n",
      "Num timesteps: 140000\n",
      "Best mean reward: 556.54 - Last mean reward per episode: 535.24\n",
      "Num timesteps: 141000\n",
      "Best mean reward: 556.54 - Last mean reward per episode: 482.16\n",
      "Num timesteps: 142000\n",
      "Best mean reward: 556.54 - Last mean reward per episode: 480.54\n",
      "Num timesteps: 143000\n",
      "Best mean reward: 556.54 - Last mean reward per episode: 410.80\n",
      "Num timesteps: 144000\n",
      "Best mean reward: 556.54 - Last mean reward per episode: 435.92\n",
      "Num timesteps: 145000\n",
      "Best mean reward: 556.54 - Last mean reward per episode: 530.52\n",
      "Num timesteps: 146000\n",
      "Best mean reward: 556.54 - Last mean reward per episode: 622.78\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 147000\n",
      "Best mean reward: 622.78 - Last mean reward per episode: 618.12\n",
      "Num timesteps: 148000\n",
      "Best mean reward: 622.78 - Last mean reward per episode: 566.10\n",
      "Num timesteps: 149000\n",
      "Best mean reward: 622.78 - Last mean reward per episode: 547.80\n",
      "Num timesteps: 150000\n",
      "Best mean reward: 622.78 - Last mean reward per episode: 518.36\n",
      "Num timesteps: 151000\n",
      "Best mean reward: 622.78 - Last mean reward per episode: 542.72\n",
      "Num timesteps: 152000\n",
      "Best mean reward: 622.78 - Last mean reward per episode: 575.46\n",
      "Num timesteps: 153000\n",
      "Best mean reward: 622.78 - Last mean reward per episode: 582.54\n",
      "Num timesteps: 154000\n",
      "Best mean reward: 622.78 - Last mean reward per episode: 581.44\n",
      "Num timesteps: 155000\n",
      "Best mean reward: 622.78 - Last mean reward per episode: 635.50\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 156000\n",
      "Best mean reward: 635.50 - Last mean reward per episode: 595.80\n",
      "Num timesteps: 157000\n",
      "Best mean reward: 635.50 - Last mean reward per episode: 584.44\n",
      "Num timesteps: 158000\n",
      "Best mean reward: 635.50 - Last mean reward per episode: 554.78\n",
      "Num timesteps: 159000\n",
      "Best mean reward: 635.50 - Last mean reward per episode: 531.02\n",
      "Num timesteps: 160000\n",
      "Best mean reward: 635.50 - Last mean reward per episode: 549.22\n",
      "Num timesteps: 161000\n",
      "Best mean reward: 635.50 - Last mean reward per episode: 562.02\n",
      "Num timesteps: 162000\n",
      "Best mean reward: 635.50 - Last mean reward per episode: 609.64\n",
      "Num timesteps: 163000\n",
      "Best mean reward: 635.50 - Last mean reward per episode: 588.28\n",
      "Num timesteps: 164000\n",
      "Best mean reward: 635.50 - Last mean reward per episode: 587.52\n",
      "Num timesteps: 165000\n",
      "Best mean reward: 635.50 - Last mean reward per episode: 607.34\n",
      "Num timesteps: 166000\n",
      "Best mean reward: 635.50 - Last mean reward per episode: 626.52\n",
      "Num timesteps: 167000\n",
      "Best mean reward: 635.50 - Last mean reward per episode: 596.72\n",
      "Num timesteps: 168000\n",
      "Best mean reward: 635.50 - Last mean reward per episode: 591.90\n",
      "Num timesteps: 169000\n",
      "Best mean reward: 635.50 - Last mean reward per episode: 597.02\n",
      "Num timesteps: 170000\n",
      "Best mean reward: 635.50 - Last mean reward per episode: 580.76\n",
      "Num timesteps: 171000\n",
      "Best mean reward: 635.50 - Last mean reward per episode: 548.88\n",
      "Num timesteps: 172000\n",
      "Best mean reward: 635.50 - Last mean reward per episode: 585.74\n",
      "Num timesteps: 173000\n",
      "Best mean reward: 635.50 - Last mean reward per episode: 643.84\n",
      "Saving new best model to /tmp/gym/best_model.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 174000\n",
      "Best mean reward: 643.84 - Last mean reward per episode: 659.04\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 175000\n",
      "Best mean reward: 659.04 - Last mean reward per episode: 588.90\n",
      "Num timesteps: 176000\n",
      "Best mean reward: 659.04 - Last mean reward per episode: 549.90\n",
      "Num timesteps: 177000\n",
      "Best mean reward: 659.04 - Last mean reward per episode: 561.88\n",
      "Num timesteps: 178000\n",
      "Best mean reward: 659.04 - Last mean reward per episode: 603.94\n",
      "Num timesteps: 179000\n",
      "Best mean reward: 659.04 - Last mean reward per episode: 612.42\n",
      "Num timesteps: 180000\n",
      "Best mean reward: 659.04 - Last mean reward per episode: 594.76\n",
      "Num timesteps: 181000\n",
      "Best mean reward: 659.04 - Last mean reward per episode: 597.56\n",
      "Num timesteps: 182000\n",
      "Best mean reward: 659.04 - Last mean reward per episode: 630.18\n",
      "Num timesteps: 183000\n",
      "Best mean reward: 659.04 - Last mean reward per episode: 675.74\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 184000\n",
      "Best mean reward: 675.74 - Last mean reward per episode: 645.70\n",
      "Num timesteps: 185000\n",
      "Best mean reward: 675.74 - Last mean reward per episode: 635.68\n",
      "Num timesteps: 186000\n",
      "Best mean reward: 675.74 - Last mean reward per episode: 727.02\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 187000\n",
      "Best mean reward: 727.02 - Last mean reward per episode: 768.30\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 188000\n",
      "Best mean reward: 768.30 - Last mean reward per episode: 732.78\n",
      "Num timesteps: 189000\n",
      "Best mean reward: 768.30 - Last mean reward per episode: 668.08\n",
      "Num timesteps: 190000\n",
      "Best mean reward: 768.30 - Last mean reward per episode: 640.78\n",
      "Num timesteps: 191000\n",
      "Best mean reward: 768.30 - Last mean reward per episode: 662.74\n",
      "Num timesteps: 192000\n",
      "Best mean reward: 768.30 - Last mean reward per episode: 703.76\n",
      "Num timesteps: 193000\n",
      "Best mean reward: 768.30 - Last mean reward per episode: 738.34\n",
      "Num timesteps: 194000\n",
      "Best mean reward: 768.30 - Last mean reward per episode: 705.82\n",
      "Num timesteps: 195000\n",
      "Best mean reward: 768.30 - Last mean reward per episode: 722.46\n",
      "Num timesteps: 196000\n",
      "Best mean reward: 768.30 - Last mean reward per episode: 709.76\n",
      "Num timesteps: 197000\n",
      "Best mean reward: 768.30 - Last mean reward per episode: 714.28\n",
      "Num timesteps: 198000\n",
      "Best mean reward: 768.30 - Last mean reward per episode: 769.70\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 199000\n",
      "Best mean reward: 769.70 - Last mean reward per episode: 790.58\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 200000\n",
      "Best mean reward: 790.58 - Last mean reward per episode: 794.92\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 201000\n",
      "Best mean reward: 794.92 - Last mean reward per episode: 759.16\n",
      "Num timesteps: 202000\n",
      "Best mean reward: 794.92 - Last mean reward per episode: 780.54\n",
      "Num timesteps: 203000\n",
      "Best mean reward: 794.92 - Last mean reward per episode: 748.38\n",
      "Num timesteps: 204000\n",
      "Best mean reward: 794.92 - Last mean reward per episode: 735.92\n",
      "Num timesteps: 205000\n",
      "Best mean reward: 794.92 - Last mean reward per episode: 793.58\n",
      "Num timesteps: 206000\n",
      "Best mean reward: 794.92 - Last mean reward per episode: 857.06\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 207000\n",
      "Best mean reward: 857.06 - Last mean reward per episode: 809.16\n",
      "Num timesteps: 208000\n",
      "Best mean reward: 857.06 - Last mean reward per episode: 786.52\n",
      "Num timesteps: 209000\n",
      "Best mean reward: 857.06 - Last mean reward per episode: 780.90\n",
      "Num timesteps: 210000\n",
      "Best mean reward: 857.06 - Last mean reward per episode: 799.64\n",
      "Num timesteps: 211000\n",
      "Best mean reward: 857.06 - Last mean reward per episode: 809.96\n",
      "Num timesteps: 212000\n",
      "Best mean reward: 857.06 - Last mean reward per episode: 778.08\n",
      "Num timesteps: 213000\n",
      "Best mean reward: 857.06 - Last mean reward per episode: 799.74\n",
      "Num timesteps: 214000\n",
      "Best mean reward: 857.06 - Last mean reward per episode: 800.48\n",
      "Num timesteps: 215000\n",
      "Best mean reward: 857.06 - Last mean reward per episode: 791.18\n",
      "Num timesteps: 216000\n",
      "Best mean reward: 857.06 - Last mean reward per episode: 783.42\n",
      "Num timesteps: 217000\n",
      "Best mean reward: 857.06 - Last mean reward per episode: 771.88\n",
      "Num timesteps: 218000\n",
      "Best mean reward: 857.06 - Last mean reward per episode: 790.86\n",
      "Num timesteps: 219000\n",
      "Best mean reward: 857.06 - Last mean reward per episode: 758.02\n",
      "Num timesteps: 220000\n",
      "Best mean reward: 857.06 - Last mean reward per episode: 731.60\n",
      "Num timesteps: 221000\n",
      "Best mean reward: 857.06 - Last mean reward per episode: 784.70\n",
      "Num timesteps: 222000\n",
      "Best mean reward: 857.06 - Last mean reward per episode: 869.64\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 223000\n",
      "Best mean reward: 869.64 - Last mean reward per episode: 859.36\n",
      "Num timesteps: 224000\n",
      "Best mean reward: 869.64 - Last mean reward per episode: 885.26\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 225000\n",
      "Best mean reward: 885.26 - Last mean reward per episode: 872.38\n",
      "Num timesteps: 226000\n",
      "Best mean reward: 885.26 - Last mean reward per episode: 909.40\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 227000\n",
      "Best mean reward: 909.40 - Last mean reward per episode: 891.14\n",
      "Num timesteps: 228000\n",
      "Best mean reward: 909.40 - Last mean reward per episode: 852.22\n",
      "Num timesteps: 229000\n",
      "Best mean reward: 909.40 - Last mean reward per episode: 773.08\n",
      "Num timesteps: 230000\n",
      "Best mean reward: 909.40 - Last mean reward per episode: 773.68\n",
      "Num timesteps: 231000\n",
      "Best mean reward: 909.40 - Last mean reward per episode: 813.98\n",
      "Num timesteps: 232000\n",
      "Best mean reward: 909.40 - Last mean reward per episode: 875.22\n",
      "Num timesteps: 233000\n",
      "Best mean reward: 909.40 - Last mean reward per episode: 887.70\n",
      "Num timesteps: 234000\n",
      "Best mean reward: 909.40 - Last mean reward per episode: 870.34\n",
      "Num timesteps: 235000\n",
      "Best mean reward: 909.40 - Last mean reward per episode: 838.52\n",
      "Num timesteps: 236000\n",
      "Best mean reward: 909.40 - Last mean reward per episode: 891.50\n",
      "Num timesteps: 237000\n",
      "Best mean reward: 909.40 - Last mean reward per episode: 855.04\n",
      "Num timesteps: 238000\n",
      "Best mean reward: 909.40 - Last mean reward per episode: 850.52\n",
      "Num timesteps: 239000\n",
      "Best mean reward: 909.40 - Last mean reward per episode: 856.48\n",
      "Num timesteps: 240000\n",
      "Best mean reward: 909.40 - Last mean reward per episode: 921.10\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 241000\n",
      "Best mean reward: 921.10 - Last mean reward per episode: 970.10\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 242000\n",
      "Best mean reward: 970.10 - Last mean reward per episode: 911.22\n",
      "Num timesteps: 243000\n",
      "Best mean reward: 970.10 - Last mean reward per episode: 922.40\n",
      "Num timesteps: 244000\n",
      "Best mean reward: 970.10 - Last mean reward per episode: 900.98\n",
      "Num timesteps: 245000\n",
      "Best mean reward: 970.10 - Last mean reward per episode: 960.90\n",
      "Num timesteps: 246000\n",
      "Best mean reward: 970.10 - Last mean reward per episode: 944.84\n",
      "Num timesteps: 247000\n",
      "Best mean reward: 970.10 - Last mean reward per episode: 939.28\n",
      "Num timesteps: 248000\n",
      "Best mean reward: 970.10 - Last mean reward per episode: 939.42\n",
      "Num timesteps: 249000\n",
      "Best mean reward: 970.10 - Last mean reward per episode: 943.52\n",
      "Num timesteps: 250000\n",
      "Best mean reward: 970.10 - Last mean reward per episode: 982.54\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 251000\n",
      "Best mean reward: 982.54 - Last mean reward per episode: 1016.88\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 252000\n",
      "Best mean reward: 1016.88 - Last mean reward per episode: 995.60\n",
      "Num timesteps: 253000\n",
      "Best mean reward: 1016.88 - Last mean reward per episode: 1018.30\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 254000\n",
      "Best mean reward: 1018.30 - Last mean reward per episode: 978.82\n",
      "Num timesteps: 255000\n",
      "Best mean reward: 1018.30 - Last mean reward per episode: 1036.22\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 256000\n",
      "Best mean reward: 1036.22 - Last mean reward per episode: 1115.84\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 257000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1101.44\n",
      "Num timesteps: 258000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1076.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 259000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1000.52\n",
      "Num timesteps: 260000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1030.84\n",
      "Num timesteps: 261000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 983.50\n",
      "Num timesteps: 262000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 911.20\n",
      "Num timesteps: 263000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 913.52\n",
      "Num timesteps: 264000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 910.52\n",
      "Num timesteps: 265000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 956.84\n",
      "Num timesteps: 266000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 984.20\n",
      "Num timesteps: 267000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1046.84\n",
      "Num timesteps: 268000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1030.22\n",
      "Num timesteps: 269000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1003.88\n",
      "Num timesteps: 270000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 977.10\n",
      "Num timesteps: 271000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 969.72\n",
      "Num timesteps: 272000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 993.30\n",
      "Num timesteps: 273000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1001.46\n",
      "Num timesteps: 274000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1064.82\n",
      "Num timesteps: 275000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1013.20\n",
      "Num timesteps: 276000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1033.12\n",
      "Num timesteps: 277000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1020.02\n",
      "Num timesteps: 278000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1072.58\n",
      "Num timesteps: 279000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1030.58\n",
      "Num timesteps: 280000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1028.78\n",
      "Num timesteps: 281000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1008.68\n",
      "Num timesteps: 282000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1010.66\n",
      "Num timesteps: 283000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1019.52\n",
      "Num timesteps: 284000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1028.20\n",
      "Num timesteps: 285000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1018.80\n",
      "Num timesteps: 286000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 976.92\n",
      "Num timesteps: 287000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1008.54\n",
      "Num timesteps: 288000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 960.58\n",
      "Num timesteps: 289000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 994.06\n",
      "Num timesteps: 290000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 971.58\n",
      "Num timesteps: 291000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1019.76\n",
      "Num timesteps: 292000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1027.06\n",
      "Num timesteps: 293000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 962.64\n",
      "Num timesteps: 294000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 920.52\n",
      "Num timesteps: 295000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 929.38\n",
      "Num timesteps: 296000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1001.68\n",
      "Num timesteps: 297000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1044.00\n",
      "Num timesteps: 298000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1096.46\n",
      "Num timesteps: 299000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1094.44\n",
      "Num timesteps: 300000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1026.40\n",
      "Num timesteps: 301000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 930.00\n",
      "Num timesteps: 302000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 898.40\n",
      "Num timesteps: 303000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1000.46\n",
      "Num timesteps: 304000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1033.34\n",
      "Num timesteps: 305000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1069.16\n",
      "Num timesteps: 306000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1102.18\n",
      "Num timesteps: 307000\n",
      "Best mean reward: 1115.84 - Last mean reward per episode: 1162.48\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 308000\n",
      "Best mean reward: 1162.48 - Last mean reward per episode: 1171.06\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 309000\n",
      "Best mean reward: 1171.06 - Last mean reward per episode: 1242.14\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 310000\n",
      "Best mean reward: 1242.14 - Last mean reward per episode: 1212.68\n",
      "Num timesteps: 311000\n",
      "Best mean reward: 1242.14 - Last mean reward per episode: 1219.12\n",
      "Num timesteps: 312000\n",
      "Best mean reward: 1242.14 - Last mean reward per episode: 1214.86\n",
      "Num timesteps: 313000\n",
      "Best mean reward: 1242.14 - Last mean reward per episode: 1282.64\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 314000\n",
      "Best mean reward: 1282.64 - Last mean reward per episode: 1274.34\n",
      "Num timesteps: 315000\n",
      "Best mean reward: 1282.64 - Last mean reward per episode: 1325.30\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 316000\n",
      "Best mean reward: 1325.30 - Last mean reward per episode: 1338.14\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 317000\n",
      "Best mean reward: 1338.14 - Last mean reward per episode: 1314.84\n",
      "Num timesteps: 318000\n",
      "Best mean reward: 1338.14 - Last mean reward per episode: 1305.60\n",
      "Num timesteps: 319000\n",
      "Best mean reward: 1338.14 - Last mean reward per episode: 1283.20\n",
      "Num timesteps: 320000\n",
      "Best mean reward: 1338.14 - Last mean reward per episode: 1249.80\n",
      "Num timesteps: 321000\n",
      "Best mean reward: 1338.14 - Last mean reward per episode: 1275.88\n",
      "Num timesteps: 322000\n",
      "Best mean reward: 1338.14 - Last mean reward per episode: 1198.16\n",
      "Num timesteps: 323000\n",
      "Best mean reward: 1338.14 - Last mean reward per episode: 1268.00\n",
      "Num timesteps: 324000\n",
      "Best mean reward: 1338.14 - Last mean reward per episode: 1304.90\n",
      "Num timesteps: 325000\n",
      "Best mean reward: 1338.14 - Last mean reward per episode: 1370.34\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 326000\n",
      "Best mean reward: 1370.34 - Last mean reward per episode: 1428.98\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 327000\n",
      "Best mean reward: 1428.98 - Last mean reward per episode: 1462.92\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 328000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1410.98\n",
      "Num timesteps: 329000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1364.82\n",
      "Num timesteps: 330000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1332.58\n",
      "Num timesteps: 331000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1297.00\n",
      "Num timesteps: 332000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1314.92\n",
      "Num timesteps: 333000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1341.76\n",
      "Num timesteps: 334000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1313.56\n",
      "Num timesteps: 335000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1247.52\n",
      "Num timesteps: 336000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1176.32\n",
      "Num timesteps: 337000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1155.54\n",
      "Num timesteps: 338000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1089.86\n",
      "Num timesteps: 339000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1162.04\n",
      "Num timesteps: 340000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1207.54\n",
      "Num timesteps: 341000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1085.88\n",
      "Num timesteps: 342000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 996.24\n",
      "Num timesteps: 343000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 917.26\n",
      "Num timesteps: 344000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 978.54\n",
      "Num timesteps: 345000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 960.54\n",
      "Num timesteps: 346000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 919.26\n",
      "Num timesteps: 347000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 943.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 348000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 973.58\n",
      "Num timesteps: 349000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1012.38\n",
      "Num timesteps: 350000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1004.24\n",
      "Num timesteps: 351000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1066.86\n",
      "Num timesteps: 352000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1079.98\n",
      "Num timesteps: 353000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1148.06\n",
      "Num timesteps: 354000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1096.20\n",
      "Num timesteps: 355000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1109.50\n",
      "Num timesteps: 356000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1110.52\n",
      "Num timesteps: 357000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1094.36\n",
      "Num timesteps: 358000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1192.62\n",
      "Num timesteps: 359000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1158.62\n",
      "Num timesteps: 360000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1182.44\n",
      "Num timesteps: 361000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1236.24\n",
      "Num timesteps: 362000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1303.62\n",
      "Num timesteps: 363000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1324.68\n",
      "Num timesteps: 364000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1352.80\n",
      "Num timesteps: 365000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1296.16\n",
      "Num timesteps: 366000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1259.08\n",
      "Num timesteps: 367000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1293.28\n",
      "Num timesteps: 368000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1275.40\n",
      "Num timesteps: 369000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1290.90\n",
      "Num timesteps: 370000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1305.96\n",
      "Num timesteps: 371000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1301.10\n",
      "Num timesteps: 372000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1364.80\n",
      "Num timesteps: 373000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1359.56\n",
      "Num timesteps: 374000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1337.02\n",
      "Num timesteps: 375000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1313.06\n",
      "Num timesteps: 376000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1296.14\n",
      "Num timesteps: 377000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1313.18\n",
      "Num timesteps: 378000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1345.56\n",
      "Num timesteps: 379000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1333.56\n",
      "Num timesteps: 380000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1360.44\n",
      "Num timesteps: 381000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1391.66\n",
      "Num timesteps: 382000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1451.36\n",
      "Num timesteps: 383000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1414.72\n",
      "Num timesteps: 384000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1461.40\n",
      "Num timesteps: 385000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1396.44\n",
      "Num timesteps: 386000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1439.72\n",
      "Num timesteps: 387000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1413.88\n",
      "Num timesteps: 388000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1395.06\n",
      "Num timesteps: 389000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1384.30\n",
      "Num timesteps: 390000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1434.20\n",
      "Num timesteps: 391000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1447.84\n",
      "Num timesteps: 392000\n",
      "Best mean reward: 1462.92 - Last mean reward per episode: 1504.38\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 393000\n",
      "Best mean reward: 1504.38 - Last mean reward per episode: 1556.06\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 394000\n",
      "Best mean reward: 1556.06 - Last mean reward per episode: 1510.22\n",
      "Num timesteps: 395000\n",
      "Best mean reward: 1556.06 - Last mean reward per episode: 1506.62\n",
      "Num timesteps: 396000\n",
      "Best mean reward: 1556.06 - Last mean reward per episode: 1526.34\n",
      "Num timesteps: 397000\n",
      "Best mean reward: 1556.06 - Last mean reward per episode: 1510.88\n",
      "Num timesteps: 398000\n",
      "Best mean reward: 1556.06 - Last mean reward per episode: 1518.62\n",
      "Num timesteps: 399000\n",
      "Best mean reward: 1556.06 - Last mean reward per episode: 1468.40\n",
      "Num timesteps: 400000\n",
      "Best mean reward: 1556.06 - Last mean reward per episode: 1452.48\n",
      "Num timesteps: 401000\n",
      "Best mean reward: 1556.06 - Last mean reward per episode: 1435.76\n",
      "Num timesteps: 402000\n",
      "Best mean reward: 1556.06 - Last mean reward per episode: 1422.88\n",
      "Num timesteps: 403000\n",
      "Best mean reward: 1556.06 - Last mean reward per episode: 1512.02\n",
      "Num timesteps: 404000\n",
      "Best mean reward: 1556.06 - Last mean reward per episode: 1495.56\n",
      "Num timesteps: 405000\n",
      "Best mean reward: 1556.06 - Last mean reward per episode: 1584.34\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 406000\n",
      "Best mean reward: 1584.34 - Last mean reward per episode: 1600.86\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 407000\n",
      "Best mean reward: 1600.86 - Last mean reward per episode: 1650.12\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 408000\n",
      "Best mean reward: 1650.12 - Last mean reward per episode: 1699.26\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 409000\n",
      "Best mean reward: 1699.26 - Last mean reward per episode: 1643.06\n",
      "Num timesteps: 410000\n",
      "Best mean reward: 1699.26 - Last mean reward per episode: 1587.56\n",
      "Num timesteps: 411000\n",
      "Best mean reward: 1699.26 - Last mean reward per episode: 1526.24\n",
      "Num timesteps: 412000\n",
      "Best mean reward: 1699.26 - Last mean reward per episode: 1462.40\n",
      "Num timesteps: 413000\n",
      "Best mean reward: 1699.26 - Last mean reward per episode: 1408.66\n",
      "Num timesteps: 414000\n",
      "Best mean reward: 1699.26 - Last mean reward per episode: 1407.60\n",
      "Num timesteps: 415000\n",
      "Best mean reward: 1699.26 - Last mean reward per episode: 1433.56\n",
      "Num timesteps: 416000\n",
      "Best mean reward: 1699.26 - Last mean reward per episode: 1465.56\n",
      "Num timesteps: 417000\n",
      "Best mean reward: 1699.26 - Last mean reward per episode: 1489.86\n",
      "Num timesteps: 418000\n",
      "Best mean reward: 1699.26 - Last mean reward per episode: 1495.78\n",
      "Num timesteps: 419000\n",
      "Best mean reward: 1699.26 - Last mean reward per episode: 1484.54\n",
      "Num timesteps: 420000\n",
      "Best mean reward: 1699.26 - Last mean reward per episode: 1538.08\n",
      "Num timesteps: 421000\n",
      "Best mean reward: 1699.26 - Last mean reward per episode: 1561.20\n",
      "Num timesteps: 422000\n",
      "Best mean reward: 1699.26 - Last mean reward per episode: 1642.02\n",
      "Num timesteps: 423000\n",
      "Best mean reward: 1699.26 - Last mean reward per episode: 1653.98\n",
      "Num timesteps: 424000\n",
      "Best mean reward: 1699.26 - Last mean reward per episode: 1711.50\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 425000\n",
      "Best mean reward: 1711.50 - Last mean reward per episode: 1752.14\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 426000\n",
      "Best mean reward: 1752.14 - Last mean reward per episode: 1730.92\n",
      "Num timesteps: 427000\n",
      "Best mean reward: 1752.14 - Last mean reward per episode: 1737.20\n",
      "Num timesteps: 428000\n",
      "Best mean reward: 1752.14 - Last mean reward per episode: 1759.58\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 429000\n",
      "Best mean reward: 1759.58 - Last mean reward per episode: 1791.18\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 430000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1760.36\n",
      "Num timesteps: 431000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1744.74\n",
      "Num timesteps: 432000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1736.34\n",
      "Num timesteps: 433000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1620.94\n",
      "Num timesteps: 434000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1660.32\n",
      "Num timesteps: 435000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1617.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 436000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1572.28\n",
      "Num timesteps: 437000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1589.48\n",
      "Num timesteps: 438000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1542.86\n",
      "Num timesteps: 439000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1569.92\n",
      "Num timesteps: 440000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1591.64\n",
      "Num timesteps: 441000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1608.10\n",
      "Num timesteps: 442000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1631.06\n",
      "Num timesteps: 443000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1632.08\n",
      "Num timesteps: 444000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1580.50\n",
      "Num timesteps: 445000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1580.22\n",
      "Num timesteps: 446000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1583.86\n",
      "Num timesteps: 447000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1562.46\n",
      "Num timesteps: 448000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1638.58\n",
      "Num timesteps: 449000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1719.14\n",
      "Num timesteps: 450000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1721.06\n",
      "Num timesteps: 451000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1732.22\n",
      "Num timesteps: 452000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1715.10\n",
      "Num timesteps: 453000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1706.74\n",
      "Num timesteps: 454000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1738.26\n",
      "Num timesteps: 455000\n",
      "Best mean reward: 1791.18 - Last mean reward per episode: 1802.36\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 456000\n",
      "Best mean reward: 1802.36 - Last mean reward per episode: 1777.62\n",
      "Num timesteps: 457000\n",
      "Best mean reward: 1802.36 - Last mean reward per episode: 1765.82\n",
      "Num timesteps: 458000\n",
      "Best mean reward: 1802.36 - Last mean reward per episode: 1740.64\n",
      "Num timesteps: 459000\n",
      "Best mean reward: 1802.36 - Last mean reward per episode: 1740.90\n",
      "Num timesteps: 460000\n",
      "Best mean reward: 1802.36 - Last mean reward per episode: 1801.06\n",
      "Num timesteps: 461000\n",
      "Best mean reward: 1802.36 - Last mean reward per episode: 1864.04\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 462000\n",
      "Best mean reward: 1864.04 - Last mean reward per episode: 1939.60\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 463000\n",
      "Best mean reward: 1939.60 - Last mean reward per episode: 2016.44\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 464000\n",
      "Best mean reward: 2016.44 - Last mean reward per episode: 1997.92\n",
      "Num timesteps: 465000\n",
      "Best mean reward: 2016.44 - Last mean reward per episode: 2012.90\n",
      "Num timesteps: 466000\n",
      "Best mean reward: 2016.44 - Last mean reward per episode: 2032.16\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 467000\n",
      "Best mean reward: 2032.16 - Last mean reward per episode: 2025.06\n",
      "Num timesteps: 468000\n",
      "Best mean reward: 2032.16 - Last mean reward per episode: 2083.60\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 469000\n",
      "Best mean reward: 2083.60 - Last mean reward per episode: 2127.30\n",
      "Saving new best model to /tmp/gym/best_model.zip\n",
      "Num timesteps: 470000\n",
      "Best mean reward: 2127.30 - Last mean reward per episode: 2068.92\n",
      "Num timesteps: 471000\n",
      "Best mean reward: 2127.30 - Last mean reward per episode: 2080.80\n",
      "Num timesteps: 472000\n",
      "Best mean reward: 2127.30 - Last mean reward per episode: 2052.18\n",
      "Num timesteps: 473000\n",
      "Best mean reward: 2127.30 - Last mean reward per episode: 1994.94\n",
      "Num timesteps: 474000\n",
      "Best mean reward: 2127.30 - Last mean reward per episode: 1951.46\n",
      "Num timesteps: 475000\n",
      "Best mean reward: 2127.30 - Last mean reward per episode: 1968.28\n",
      "Num timesteps: 476000\n",
      "Best mean reward: 2127.30 - Last mean reward per episode: 1980.06\n",
      "Num timesteps: 477000\n",
      "Best mean reward: 2127.30 - Last mean reward per episode: 2031.48\n",
      "Num timesteps: 478000\n",
      "Best mean reward: 2127.30 - Last mean reward per episode: 2070.82\n",
      "Num timesteps: 479000\n",
      "Best mean reward: 2127.30 - Last mean reward per episode: 2093.12\n",
      "Num timesteps: 480000\n",
      "Best mean reward: 2127.30 - Last mean reward per episode: 2095.50\n",
      "Num timesteps: 481000\n",
      "Best mean reward: 2127.30 - Last mean reward per episode: 2118.38\n",
      "Num timesteps: 482000\n",
      "Best mean reward: 2127.30 - Last mean reward per episode: 2090.42\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model = PPO(\"MlpPolicy\", env).learn(total_timesteps=ts, callback=callback)\n",
    "end=time.time()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total time taken: {} min'.format((end - start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design by AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "while i<100:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    if env.M.break_flag:\n",
    "        break\n",
    "    i+=1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.M.length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEA_output_arr=env.M.FEA()\n",
    "env.M.max_u(FEA_output_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_plotter.plot_results([log_dir], ts, results_plotter.X_TIMESTEPS, \"PPO BionicEnv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
